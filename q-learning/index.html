<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Martin Thoma" />
        <meta name="copyright" content="Martin Thoma" />
        <link title = "Martin Thoma"
              type  = "application/opensearchdescription+xml"
              rel   = "search"
              href  = "../opensearch.xml">

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="Machine learning, RL, Reinforcement Learning, Machine learning, " />

<meta property="og:title" content="Q-Learning "/>
<meta property="og:url" content="../q-learning/" />
<meta property="og:description" content="Reinforcement Learning (RL) is about finding optimal actions automatically. So you have an environment env which has env.reset() -&gt; None: Start a new episode. This could be a new game in the case of chess. env.step(action) -&gt; observation, reward, is_done, additional_information: Make a step in the environment. The is_done …" />
<meta property="og:site_name" content="Martin Thoma" />
<meta property="og:article:author" content="Martin Thoma" />
<meta property="og:article:published_time" content="2017-11-26T20:00:00+01:00" />
<meta name="twitter:title" content="Q-Learning ">
<meta name="twitter:description" content="Reinforcement Learning (RL) is about finding optimal actions automatically. So you have an environment env which has env.reset() -&gt; None: Start a new episode. This could be a new game in the case of chess. env.step(action) -&gt; observation, reward, is_done, additional_information: Make a step in the environment. The is_done …">
<meta property="og:image" content="logos/ml.png" />
<meta name="twitter:image" content="logos/ml.png" >

        <title>Q-Learning  · Martin Thoma
</title>
        <!-- Latest compiled and minified CSS -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="../theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../static/custom.css" media="screen">

        <!-- MathJax -->
<script type="text/x-mathjax-config">
<!--
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [ ['$$', '$$'], ['\\[','\\]']],
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    processEscapes: true
  }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
// -->
</script>
<script type="text/javascript" async
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

        <link href="https://martin-thoma.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Martin Thoma - Full Atom Feed" />
        <link href="https://martin-thoma.com/feeds/index.xml" type="application/rss+xml" rel="alternate" title="Martin Thoma - Full RSS Feed" />
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top navbar-default">
            <div class="container">
                <div class="container-fluid">
                    <div class="collapse navbar-collapse">
                        <ul class="nav pull-left top-menu navbar-nav">
                            <li><a href=".." style="font-family: 'Monaco', 'Inconsolata', 'Andale Mono', 'Lucida Console', 'Bitstream Vera Sans Mono', 'Courier New', Courier, Monospace;
                        font-size: 20px;" class="navbar-brand">Martin Thoma</a>
                            </li>
                        </ul>
                        <ul class="nav pull-right top-menu navbar-nav">
                            <li ><a href="..">Home</a></li>
                            <li ><a href="../categories.html">Categories</a></li>
                            <li ><a href="../tags.html">Tags</a></li>
                            <li ><a href="../archives.html">Archives</a></li>
                            <li><a href="../support-me/">Support me</a></li>
                            <li><form class="navbar-form" action="../search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="search" class="search-query form-control" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row">
                <div class="col-sm-1 col-md-1"></div>
                <div class="col-sm-10 col-md-10">
<article>
<div class="row">
    <header class="page-header col-sm-10 col-md-10 col-md-offset-2">
    <h1><a href="../q-learning/"> Q-Learning  </a></h1>
    </header>
</div>

<div class="row">
    <div class="col-sm-2 col-md-2 table-of-content">
        <nav>
        <h4>Contents</h4>
        <div id="toc"><ul><li><a class="toc-href" href="#the-idea-of-q-learning" title="The idea of Q-Learning">The idea of Q-Learning</a></li><li><a class="toc-href" href="#code" title="Code">Code</a></li><li><a class="toc-href" href="#results" title="Results">Results</a></li><li><a class="toc-href" href="#see-also" title="See also">See also</a></li></ul></div>
        </nav>
    </div>
    <div class="col-sm-8 col-md-8 article-content" id="contentAfterTitle">

            
            <p>Reinforcement Learning (RL) is about finding optimal actions automatically.
So you have an environment <code>env</code> which has</p>
<ul>
<li><code>env.reset() -&gt; None</code>: Start a new episode. This could be a new game in the
  case of chess.</li>
<li><code>env.step(action) -&gt; observation, reward, is_done, additional_information</code>:
  Make a step in the environment. The <code>is_done</code> says if the episode is over,
  e.g. if a game of chess is over. If it is over, then the environment needs
  a reset.</li>
</ul>
<p>and an <code>agent</code> which has</p>
<ul>
<li><code>agent.reset() -&gt; agent</code>: Reset internal variables</li>
<li><code>agent.act(observation, no_exploration) -&gt; action</code>: Let the agent take an action.
  If you want to evaluate the agent, set <code>no_exploration</code> to <code>True</code>.</li>
<li><code>agent.remember(prev_state, action, reward, state, is_done) -&gt; agent</code>: Store
  what is necessary - here the learning happens.</li>
<li><code>agent.save(path) -&gt; agent</code>: Serialize the agent to <code>path</code></li>
<li><code>agent.load(path) -&gt; agent</code>: De-serialize the agent from <code>path</code></li>
</ul>
<h2 id="the-idea-of-q-learning">The idea of Q-Learning</h2>
<p>The following is a mixed introduction to RL / Q-Learning. You might want to
have a look at my <a href="https://martin-thoma.com/reinforcement-learning/">Reinforcement Learning</a>
post as well.</p>
<p>If there is a limited set of observations <span class="math">\(\mathcal{S}\)</span> (states) and a limited
set of actions <span class="math">\(\mathcal{A}\)</span>, then you have <span class="math">\(|\mathcal{S}| \cdot |\mathcal{A}|\)</span>
possibilities to rate. For some of the observations you also receive a reward.
But rewards might be delayed:</p>
<div class="highlight"><pre><span></span>                      a0 --- s3, r=10
                    /
     a0-- s1, r= 10 - a1 --- s4, r= 0
    /
s0 -
    \a1 -- s2, r=-10 - a0 --- s5, r = 100
</pre></div>
<p>This shows that you start in state <code>s0</code> where you can execute actions <code>a0</code> and
<code>a1</code>. Action <code>a0</code> lives you a reward of 10, action <code>a1</code> a reward of <code>-10</code>. So
if you take the action greedy, you would take <code>a0</code> and end up in state <code>s1</code>.
But if you look one step ahead, you can see that <code>s2</code> ends up in state <code>s5</code>
with a reward of 100 whereas <code>s1</code> can only get a reward of 10 or 0.</p>
<p>In many cases, one does not want a greedy action. And one does not want to rely
completely on very high rewards in the very far future. Direct rewards are
prefered, but if it is really high we wait a bit longer. This thought leads to
the <strong>value</strong> of a state / action. The value of a state or a state/action pair
is its current reward plus its reward in future. As we want to prefer rewards
which come directly, we discount the future rewards with a factor <span class="math">\(\gamma \in [0, 1]\)</span>:</p>
<div class="math">$$V(s) = \max_{a \in \mathcal{A}} (R(s, a) + \gamma \sum_{s'} V(s'))$$</div>
<p>The <span class="math">\(\max_{a \in \mathcal{A}}\)</span> means we execute the optimal action all the
time.</p>
<p>Most of the time, the environments are not deterministic. Then you need to take
the transition probability from getting from state <span class="math">\(s\)</span> into state <span class="math">\(s'\)</span> when
you execute action <span class="math">\(a\)</span> into account:</p>
<div class="math">$$V(s) = \max_{a \in \mathcal{A}} (R(s, a) + \gamma \sum_{s' \in \mathcal{S}} T(s, a, s') V(s'))$$</div>
<p>Ok, awesome! But now comes the tricky part: We don't have the function <span class="math">\(V\)</span>.
If both, <span class="math">\(\mathcal{S}\)</span> and <span class="math">\(\mathcal{A}\)</span> are finite, we can simply:</p>
<ul>
<li>Initialize a table which has the columns (state, value of action 1, value of
  action 2, ..., value of action <span class="math">\(n\)</span>) and one row per state. You could
  initialize it to zero.</li>
<li>Run the agent. Update the <span class="math">\((state, action)\)</span> cell with a weighted average of
  what was in the table + what was observed. The weighting factor is
  <span class="math">\(\alpha \in (0, 1)\)</span>.</li>
</ul>
<p>That's it.</p>
<h2 id="code">Code</h2>
<p>You might want to read <a href="https://martin-thoma.com/ml-best-practice/">Best practice for Machine Learning Projects</a>
to understand why the following code was written as it is.</p>
<p>The latest code can be found on <a href="https://github.com/MartinThoma/algorithms/blob/master/ML/rl/q_table_agent.py">Github MartinThoma:algorithms/</a></p>
<p>First, the configuration file:</p>
<div class="highlight"><pre><span></span>model_name: 'qlearning'
problem:
  gamma: 0.99  # discounting factor
training:
  nb_epochs: 100000
  learning_rate: 0.7  # alpha
  print_score: 500  # each 500 episodes
  exploration:
    name: 'Boltzmann'
    clip: [-500, 500]
testing:
  nb_epochs: 10000
</pre></div>
<p>Now the code:</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
 28
 29
 30
 31
 32
 33
 34
 35
 36
 37
 38
 39
 40
 41
 42
 43
 44
 45
 46
 47
 48
 49
 50
 51
 52
 53
 54
 55
 56
 57
 58
 59
 60
 61
 62
 63
 64
 65
 66
 67
 68
 69
 70
 71
 72
 73
 74
 75
 76
 77
 78
 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>
<span class="c1"># -*- coding: utf-8 -*-</span>

<span class="sd">"""Q-Table Reinforcement Learning agent."""</span>

<span class="c1"># core modules</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">yaml</span>

<span class="c1"># 3rd party modules</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">280490</span><span class="p">)</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">format</span><span class="o">=</span><span class="s1">'</span><span class="si">%(asctime)s</span><span class="s1"> </span><span class="si">%(levelname)s</span><span class="s1"> </span><span class="si">%(message)s</span><span class="s1">'</span><span class="p">,</span>
                    <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">,</span>
                    <span class="n">stream</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">formatter</span><span class="o">=</span><span class="p">{</span><span class="s1">'float_kind'</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s2">"{:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="p">)})</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">environment_name</span><span class="p">,</span> <span class="n">agent_cfg_file</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Load, train and evaluate a Reinforcment Learning agent.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    environment_name : str</span>
<span class="sd">    agent_cfg_file : str</span>
<span class="sd">    """</span>
    <span class="n">cfg</span> <span class="o">=</span> <span class="n">load_cfg</span><span class="p">(</span><span class="n">agent_cfg_file</span><span class="p">)</span>

    <span class="c1"># Set up environment and agent</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">environment_name</span><span class="p">)</span>
    <span class="n">cfg</span><span class="p">[</span><span class="s1">'env'</span><span class="p">]</span> <span class="o">=</span> <span class="n">env</span>
    <span class="n">cfg</span><span class="p">[</span><span class="s1">'serialize_path'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="s1">'artifacts/{}-{}.pickle'</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s1">'model_name'</span><span class="p">],</span> <span class="n">environment_name</span><span class="p">))</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">load_agent</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">env</span><span class="p">)</span>

    <span class="n">agent</span> <span class="o">=</span> <span class="n">train_agent</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="n">test_agent</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"Average reward: {:5.3f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"Trained epochs: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">epoch</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">QTableAgent</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    Q-Table Agent.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    cfg : dict</span>
<span class="sd">    """</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">agent_cfg</span><span class="p">,</span> <span class="n">nb_observations</span><span class="p">,</span> <span class="n">nb_actions</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nb_obs</span> <span class="o">=</span> <span class="n">nb_observations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nb_act</span> <span class="o">=</span> <span class="n">nb_actions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">nb_observations</span><span class="p">,</span> <span class="n">nb_actions</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">agent_cfg</span><span class="p">[</span><span class="s1">'training'</span><span class="p">][</span><span class="s1">'learning_rate'</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">agent_cfg</span><span class="p">[</span><span class="s1">'problem'</span><span class="p">][</span><span class="s1">'gamma'</span><span class="p">]</span>  <span class="c1"># discount</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">exploration</span> <span class="o">=</span> <span class="n">agent_cfg</span><span class="p">[</span><span class="s1">'training'</span><span class="p">][</span><span class="s1">'exploration'</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""Reset the agent. Call this at the beginning of an episode."""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation</span><span class="p">,</span> <span class="n">no_exploration</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        Decide which action to execute.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        observation : int</span>
<span class="sd">        no_exploration : bool, optional (default: False)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        action : int</span>
<span class="sd">        """</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">"Reset before you run an episode."</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">observation</span><span class="p">,</span> <span class="p">:])</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">no_exploration</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">exploration</span><span class="p">[</span><span class="s1">'name'</span><span class="p">]</span> <span class="o">==</span> <span class="s1">'epsilon-greedy'</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">exploration</span><span class="p">[</span><span class="s1">'epsilon'</span><span class="p">]:</span>
                    <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nb_act</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">exploration</span><span class="p">[</span><span class="s1">'name'</span><span class="p">]</span> <span class="o">==</span> <span class="s1">'Boltzmann'</span><span class="p">:</span>
                <span class="n">T</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="n">clip</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">exploration</span><span class="p">[</span><span class="s1">'clip'</span><span class="p">]</span>
                <span class="n">q_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">observation</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">'float64'</span><span class="p">)</span>
                <span class="n">q_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">q_values</span> <span class="o">/</span> <span class="n">T</span><span class="p">,</span> <span class="n">clip</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">clip</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">exp_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">q_values</span><span class="p">)</span>
                <span class="n">probs</span> <span class="o">=</span> <span class="n">exp_values</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_values</span><span class="p">)</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nb_act</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="n">probs</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="bp">NotImplemented</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">exploration</span><span class="p">[</span><span class="s1">'name'</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">action</span>

    <span class="k">def</span> <span class="nf">remember</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prev_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">is_done</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        Store data in the Q-Table. Here, the learning happens.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        prev_state : int</span>
<span class="sd">        action : int</span>
<span class="sd">        reward : float</span>
<span class="sd">        state : int</span>
<span class="sd">        """</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">prev_state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_done</span><span class="p">:</span>
            <span class="n">delta</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">prev_state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">delta</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="sd">"""Serialize an agent."""</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Q'</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">,</span>
                <span class="s1">'epoch'</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">}</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">handle</span><span class="p">:</span>
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">handle</span><span class="p">,</span> <span class="n">protocol</span><span class="o">=</span><span class="n">pickle</span><span class="o">.</span><span class="n">HIGHEST_PROTOCOL</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="sd">"""Load an agent."""</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">handle</span><span class="p">:</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'Q'</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'epoch'</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span>


<span class="k">def</span> <span class="nf">load_agent</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">env</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Create (and load) a QTableAgent.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    cfg : dict</span>
<span class="sd">    env : OpenAI environment</span>
<span class="sd">    """</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">QTableAgent</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s1">'serialize_path'</span><span class="p">]):</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s1">'serialize_path'</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">agent</span>


<span class="c1"># General training and testing code</span>
<span class="k">def</span> <span class="nf">train_agent</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Train an agent in environment.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    cfg : dict</span>
<span class="sd">    env : OpenAI environment</span>
<span class="sd">    agent : object</span>

<span class="sd">    Return</span>
<span class="sd">    ------</span>
<span class="sd">    agent : object</span>
<span class="sd">    """</span>
    <span class="n">cum_reward</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s1">'training'</span><span class="p">][</span><span class="s1">'nb_epochs'</span><span class="p">]):</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">observation_previous</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">is_done</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="k">while</span> <span class="ow">not</span> <span class="n">is_done</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">observation_previous</span><span class="p">)</span>
            <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">is_done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">cum_reward</span> <span class="o">+=</span> <span class="n">reward</span>
            <span class="n">agent</span><span class="o">.</span><span class="n">remember</span><span class="p">(</span><span class="n">observation_previous</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">observation</span><span class="p">,</span>
                           <span class="n">is_done</span><span class="p">)</span>
            <span class="n">observation_previous</span> <span class="o">=</span> <span class="n">observation</span>
        <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">cfg</span><span class="p">[</span><span class="s1">'training'</span><span class="p">][</span><span class="s1">'print_score'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">episode</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">agent</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s1">'serialize_path'</span><span class="p">])</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">"Average score: {:&gt;5.2f}"</span>
                  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cum_reward</span> <span class="o">/</span> <span class="p">(</span><span class="n">episode</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="k">print</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">Q</span><span class="p">)</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s1">'serialize_path'</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">agent</span>


<span class="k">def</span> <span class="nf">test_agent</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">):</span>
    <span class="sd">"""Calculate average reward."""</span>
    <span class="n">cum_reward</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s1">'testing'</span><span class="p">][</span><span class="s1">'nb_epochs'</span><span class="p">]):</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">observation_previous</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">is_done</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="k">while</span> <span class="ow">not</span> <span class="n">is_done</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">observation_previous</span><span class="p">,</span> <span class="n">no_exploration</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">is_done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">cum_reward</span> <span class="o">+=</span> <span class="n">reward</span>
            <span class="n">observation_previous</span> <span class="o">=</span> <span class="n">observation</span>
    <span class="k">return</span> <span class="n">cum_reward</span> <span class="o">/</span> <span class="n">cfg</span><span class="p">[</span><span class="s1">'testing'</span><span class="p">][</span><span class="s1">'nb_epochs'</span><span class="p">]</span>


<span class="c1"># General code for loading ML configuration files</span>
<span class="k">def</span> <span class="nf">load_cfg</span><span class="p">(</span><span class="n">yaml_filepath</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Load a YAML configuration file.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    yaml_filepath : str</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    cfg : dict</span>
<span class="sd">    """</span>
    <span class="c1"># Read YAML experiment definition file</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">yaml_filepath</span><span class="p">,</span> <span class="s1">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">stream</span><span class="p">:</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">stream</span><span class="p">)</span>
    <span class="n">cfg</span> <span class="o">=</span> <span class="n">make_paths_absolute</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">yaml_filepath</span><span class="p">),</span> <span class="n">cfg</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cfg</span>


<span class="k">def</span> <span class="nf">make_paths_absolute</span><span class="p">(</span><span class="n">dir_</span><span class="p">,</span> <span class="n">cfg</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Make all values for keys ending with `_path` absolute to dir_.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    dir_ : str</span>
<span class="sd">    cfg : dict</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    cfg : dict</span>
<span class="sd">    """</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">key</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">"_path"</span><span class="p">):</span>
            <span class="n">cfg</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dir_</span><span class="p">,</span> <span class="n">cfg</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
            <span class="n">cfg</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="n">key</span><span class="p">]):</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">"</span><span class="si">%s</span><span class="s2"> does not exist."</span><span class="p">,</span> <span class="n">cfg</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="n">key</span><span class="p">])</span> <span class="ow">is</span> <span class="nb">dict</span><span class="p">:</span>
            <span class="n">cfg</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">make_paths_absolute</span><span class="p">(</span><span class="n">dir_</span><span class="p">,</span> <span class="n">cfg</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">cfg</span>


<span class="k">def</span> <span class="nf">get_parser</span><span class="p">():</span>
    <span class="sd">"""Get parser object."""</span>
    <span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">ArgumentParser</span><span class="p">,</span> <span class="n">ArgumentDefaultsHelpFormatter</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="vm">__doc__</span><span class="p">,</span>
                            <span class="n">formatter_class</span><span class="o">=</span><span class="n">ArgumentDefaultsHelpFormatter</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">"--env"</span><span class="p">,</span>
                        <span class="n">dest</span><span class="o">=</span><span class="s2">"environment_name"</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s2">"OpenAI Gym environment"</span><span class="p">,</span>
                        <span class="n">metavar</span><span class="o">=</span><span class="s2">"ENVIRONMENT"</span><span class="p">,</span>
                        <span class="n">default</span><span class="o">=</span><span class="s2">"FrozenLake-v0"</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">"--agent"</span><span class="p">,</span>
                        <span class="n">dest</span><span class="o">=</span><span class="s2">"agent_cfg_file"</span><span class="p">,</span>
                        <span class="n">required</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                        <span class="n">metavar</span><span class="o">=</span><span class="s2">"AGENT_YAML"</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s2">"Configuration file for the agent"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">parser</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">get_parser</span><span class="p">()</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
    <span class="n">main</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">environment_name</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">agent_cfg_file</span><span class="p">)</span>
</pre></div>
</td></tr></table>
<h2 id="results">Results</h2>
<table class="table">
<tr>
<th>Environment</th>
<th>Config File</th>
<th>Time</th>
<th>Score</th>
</tr>
<tr>
<td>FrozenLake-v0</td>
<td>qlearning.yaml</td>
<td>50s</td>
<td>0.166</td>
</tr>
<tr>
<td>FrozenLake-v0</td>
<td>q-lr10.yaml</td>
<td>48s</td>
<td>0.743</td>
</tr>
<tr>
<td>FrozenLake-v0</td>
<td>q-lr90.yaml</td>
<td>48s</td>
<td>0.156</td>
</tr>
<tr>
<td>CliffWalking-v0</td>
<td>q-lr10.yaml</td>
<td>128s</td>
<td>-13.000</td>
</tr>
<tr>
<td>FrozenLake8x8-v0</td>
<td>q-lr10.yaml</td>
<td>179s</td>
<td>0.569</td>
</tr>
<tr>
<td>NChain-v0</td>
<td>q-lr90.yaml</td>
<td>TODO</td>
<td>TODO</td>
</tr>
<tr>
<td>OneRoundDeterministicReward-v0</td>
<td>q-lr10.yaml</td>
<td>5s</td>
<td>1.00</td>
</tr>
<tr>
<td>OneRoundNondeterministicReward-v0</td>
<td>q-lr10.yaml</td>
<td>6s</td>
<td>2.475</td>
</tr>
<tr>
<td>Roulette-v0</td>
<td>q-lr10.yaml</td>
<td>533s</td>
<td>-2.764</td>
</tr>
<tr>
<td>Taxi-v2</td>
<td>q-lr10.yaml</td>
<td>68s</td>
<td>8.471</td>
</tr>
<tr>
<td>TwoRoundDeterministicReward-v0</td>
<td>q-lr10.yaml</td>
<td>10s</td>
<td>3.000</td>
</tr>
<tr>
<td>TwoRoundNondeterministicReward-v0</td>
<td>q-lr10.yaml</td>
<td>ERROR</td>
<td>ERROR</td>
</tr>
</table>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="https://github.com/dennybritz/reinforcement-learning">dennybritz/reinforcement-learning</a></li>
<li>Tijsma, Drugan, Wiering: <a href="http://www.ai.rug.nl/~mwiering/GROUP/ARTICLES/Exploration_QLearning.pdf">Comparing Exploration Strategies for Q-learning in Random Stochastic Mazes</a></li>
</ul>
            
            <div id="disqus_thread"></div>
<script>
    /*
    var disqus_config = function () {
        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW
        var d = document, s = d.createElement('script');

        s.src = '//martinthoma.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

            <hr/>
        </div>
        <section>
        <div class="col-sm-2 col-md-2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time pubdate="pubdate" datetime="2017-11-26T20:00:00+01:00">Nov 26, 2017</time>
            <br/>
            by <a rel="author" class="vcard author post-author" itemprop="author" href="../author/martin-thoma/"><span class="fn" itemscope="" itemtype="https://schema.org/Person"><span itemprop="name">Martin Thoma</span></span></a>
            <h4>Category</h4>
            <a class="category-link" href="../categories.html#machine-learning-ref">Machine learning</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="../tags.html#machine-learning-ref">Machine learning
                    <span>56</span>
</a></li>
                <li><a href="../tags.html#reinforcement-learning-ref">Reinforcement Learning
                    <span>2</span>
</a></li>
                <li><a href="../tags.html#rl-ref">RL
                    <span>6</span>
</a></li>
            </ul>
<h4>Contact</h4>
    <a href="https://twitter.com/themoosemind" title="My Twitter Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-twitter sidebar-social-links"></i></a>
    <a href="mailto:info@martin-thoma.de" title="My Email Address" class="sidebar-social-links" target="_blank">
    <i class="fa fa-envelope sidebar-social-links"></i></a>
    <a href="https://github.com/MartinThoma" title="My Github Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-github sidebar-social-links"></i></a>
    <a href="http://stackoverflow.com/users/562769/martin-thoma" title="My Stackoverflow Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-stackoverflow sidebar-social-links"></i></a>
        </div>
        </section>
</div>
</article>
                </div>
                <div class="col-sm-1 col-md-1"></div>
            </div>
        </div>
        <div id="push"></div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-subtitle"><span class="site-name">Martin Thoma</span> - A blog about Code, the Web and Cyberculture</li>
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>        <script src="//code.jquery.com/jquery.min.js"></script>
        <!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>
        <script>
(function(){
    'use strict';

    /*
    Create intra-page links
    Requires that your headings already have an `id` attribute set (because that's what jekyll does)
    For every heading in your page, this adds a little anchor link `#` that you can click to get a permalink to the heading.
    Ignores `h1`, because you should only have one per page.
    The text content of the tag is used to generate the link, so it will fail "gracefully-ish" if you have duplicate heading text.

    Credit: https://gist.github.com/SimplGy/a229d25cdb19d7f21231
     */

    var headingNodes = [], results, link,
        tags = ['h2', 'h3', 'h4', 'h5', 'h6'];

    tags.forEach(function(tag){
        var contentTag = document.getElementById('contentAfterTitle');
      results = contentTag.getElementsByTagName(tag);
      Array.prototype.push.apply(headingNodes, results);
    });

    headingNodes.forEach(function(node){
      link = document.createElement('a');
      link.className = 'deepLink';
      link.textContent = ' ¶';
      link.href = '#' + node.getAttribute('id');
      node.appendChild(link);
    });

  })();
</script>
    </body>
    <!-- Theme: Elegant built for Pelican
    License : http://oncrashreboot.com/pelican-elegant -->
</html>