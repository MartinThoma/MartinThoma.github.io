<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Martin Thoma" />
        <meta name="copyright" content="Martin Thoma" />
        <link title = "Martin Thoma"
              type  = "application/opensearchdescription+xml"
              rel   = "search"
              href  = "../opensearch.xml">

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="Klausur, German posts, " />

<meta property="og:title" content="Informationsfusion "/>
<meta property="og:url" content="../informationsfusion/" />
<meta property="og:description" content="Dieser Artikel beschäftigt sich mit der Vorlesung „Informationsfusion“ am KIT. Er dient als Prüfungsvorbereitung. Ich habe die Vorlesungen nicht gehört, aber die Folien von Herrn Prof. Dr.-Ing. Michael Heizmann aus dem Wintersemester 2015/2016 gelesen. In der Vorlesung &#39;Informationsfusion&#39; ist der Kalman-Filter ein zentraler Inhalt. Behandelter ..." />
<meta property="og:site_name" content="Martin Thoma" />
<meta property="og:article:author" content="Martin Thoma" />
<meta property="og:article:published_time" content="2016-05-23T20:00:00+02:00" />
<meta name="twitter:title" content="Informationsfusion ">
<meta name="twitter:description" content="Dieser Artikel beschäftigt sich mit der Vorlesung „Informationsfusion“ am KIT. Er dient als Prüfungsvorbereitung. Ich habe die Vorlesungen nicht gehört, aber die Folien von Herrn Prof. Dr.-Ing. Michael Heizmann aus dem Wintersemester 2015/2016 gelesen. In der Vorlesung &#39;Informationsfusion&#39; ist der Kalman-Filter ein zentraler Inhalt. Behandelter ...">
<meta property="og:image" content="logos/klausur.png" />
<meta name="twitter:image" content="logos/klausur.png" >

        <title>Informationsfusion  · Martin Thoma
</title>
        <!-- Latest compiled and minified CSS -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="../theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../static/custom.css" media="screen">

        <!-- MathJax -->
<script type="text/x-mathjax-config">
<!--
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [ ['$$', '$$'], ['\\[','\\]']],
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    processEscapes: true
  }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
// -->
</script>
<script type="text/javascript" async
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

        <link href="https://martin-thoma.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Martin Thoma - Full Atom Feed" />
        <link href="https://martin-thoma.com/feeds/index.xml" type="application/rss+xml" rel="alternate" title="Martin Thoma - Full RSS Feed" />
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top navbar-default">
            <div class="container">
                <div class="container-fluid">
                    <div class="collapse navbar-collapse">
                        <ul class="nav pull-left top-menu navbar-nav">
                            <li><a href=".." style="font-family: 'Monaco', 'Inconsolata', 'Andale Mono', 'Lucida Console', 'Bitstream Vera Sans Mono', 'Courier New', Courier, Monospace;
                        font-size: 20px;" class="navbar-brand">Martin Thoma</a>
                            </li>
                        </ul>
                        <ul class="nav pull-right top-menu navbar-nav">
                            <li ><a href="..">Home</a></li>
                            <li ><a href="../categories.html">Categories</a></li>
                            <li ><a href="../tags.html">Tags</a></li>
                            <li ><a href="../archives.html">Archives</a></li>
                            <li><form class="navbar-form" action="../search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="search" class="search-query form-control" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row">
                <div class="col-sm-1 col-md-1"></div>
                <div class="col-sm-10 col-md-10">
<article>
<div class="row">
    <header class="page-header col-sm-10 col-md-10 col-md-offset-2">
    <h1><a href="../informationsfusion/"> Informationsfusion  </a></h1>
    </header>
</div>

<div class="row">
    <div class="col-sm-2 col-md-2 table-of-content">
        <nav>
        <h4>Contents</h4>
        <div id="toc"><ul><li><a class="toc-href" href="#behandelter-stoff" title="Behandelter Stoff">Behandelter Stoff</a><ul><li><a class="toc-href" href="#grundlagen" title="Grundlagen">Grundlagen</a></li><li><a class="toc-href" href="#wt" title="WT">WT</a></li><li><a class="toc-href" href="#dempster-shafer-theorie" title="Dempster-Shafer-Theorie">Dempster-Shafer-Theorie</a></li><li><a class="toc-href" href="#fuzzy-systeme" title="Fuzzy-Systeme">Fuzzy-Systeme</a></li><li><a class="toc-href" href="#neuronale-netze" title="Neuronale Netze">Neuronale Netze</a></li><li><a class="toc-href" href="#registrierung" title="Registrierung">Registrierung</a></li><li><a class="toc-href" href="#energiefunktionale" title="Energie&shy;funktionale">Energie&shy;funktionale</a></li></ul></li><li><a class="toc-href" href="#abkurzungen_1" title="Abk&uuml;rzungen">Abk&uuml;rzungen</a></li><li><a class="toc-href" href="#meine-fragen" title="Meine Fragen">Meine Fragen</a></li><li><a class="toc-href" href="#ubungsaufgaben" title="&Uuml;bungsaufgaben">&Uuml;bungsaufgaben</a><ul><li><a class="toc-href" href="#ub-1" title="&Uuml;B 1">&Uuml;B 1</a></li><li><a class="toc-href" href="#ub-2" title="&Uuml;B 2">&Uuml;B 2</a></li><li><a class="toc-href" href="#ub-3" title="&Uuml;B 3">&Uuml;B 3</a></li></ul></li><li><a class="toc-href" href="#prufungsfragen_1" title="Pr&uuml;fungsfragen">Pr&uuml;fungsfragen</a><ul><li><a class="toc-href" href="#kalman-filter" title="Kalman-Filter">Kalman-Filter</a></li></ul></li><li><a class="toc-href" href="#absprachen_1" title="Absprachen">Absprachen</a></li><li><a class="toc-href" href="#material-und-links" title="Material und Links">Material und Links</a></li><li><a class="toc-href" href="#vorlesungsempfehlungen" title="Vorlesungs&shy;empfehlungen">Vorlesungs&shy;empfehlungen</a></li><li><a class="toc-href" href="#termine-und-klausurablauf" title="Termine und Klausurablauf">Termine und Klausurablauf</a></li></ul></div>
        </nav>
    </div>
    <div class="col-sm-8 col-md-8 article-content" id="contentAfterTitle">

            
            <div class="info">Dieser Artikel besch&auml;ftigt sich mit der Vorlesung &bdquo;Informationsfusion&ldquo; am KIT. Er dient als Pr&uuml;fungsvorbereitung. Ich habe die Vorlesungen nicht geh&ouml;rt, aber die Folien von <a href="http://ies.anthropomatik.kit.edu/mitarbeiter.php?person=heizmann">Herrn Prof. Dr.-Ing. Michael Heizmann</a> aus dem Wintersemester 2015/2016 gelesen.</div>
<p>In der Vorlesung 'Informationsfusion' ist der Kalman-Filter ein zentraler
Inhalt.</p>
<h2 id="behandelter-stoff">Behandelter Stoff</h2>
<h3 id="grundlagen">Grundlagen</h3>
<p>Slides: <code>IF-Kap1_151110.pdf</code></p>
<p>Es wurden Grundbegriffe wie Daten, Information, Merkmal, Informationsfusion,
Signal, usw. eingef&uuml;hrt.</p>
<h3 id="wt">WT</h3>
<p>Slides: <code>IF-Kap2_151215.pdf</code></p>
<ul>
<li>Wahrscheinlichkeitsraum, Zufallsvariable</li>
<li>Guide to the Expression of Uncertainty in Measurement (GUM)</li>
<li>Bayessche Methodik</li>
</ul>
<dl>
<dt><a href="https://en.wikipedia.org/wiki/Probability_axioms#Axioms"><dfn id="kolmogorov-axioms">Kolmogorov-Axiome</dfn></a></dt>
<dd>Siehe <a href="https://martin-thoma.com/probabilistische-planung#probability-measure">Probabilistische Planung</a></dd>
<dt><a href="https://de.wikipedia.org/wiki/Kovarianz_(Stochastik)"><dfn>Kovarianz</dfn></a> (<dfn id="covariance">Covariance</dfn>)</dt>
<dd>Es seien $X, Y$ Zufallsvariablen. Dann hei&szlig;t
        $$COV(X, Y) = \mathbb{E}((X - \mathbb{E}(X)) \cdot (Y - \mathbb{E}(Y)))$$
        die Kovarianz von $X$ und $Y$.
    </dd>
<dt><dfn id="statistisches-modell">Statistisches Modell</dfn></dt>
<dd>Es sei $X$ eine Zufallsvariable. Dann hei&szlig;t ein Tupel
        $(X, (P_\theta)_{\theta \in \Theta})$
        ein statistisches Modell, wenn $(P_\theta)_{\theta \in \Theta}$ eine
        Familie von Wahrscheinlichkeitsverteilungen ist.
    </dd>
<dt><a href="https://de.wikipedia.org/wiki/Sch%C3%A4tzfunktion"><dfn id="schaetzer">Sch&auml;tzer</dfn></a></dt>
<dd>
        Sei $\mathcal{X} = (X_1, \dots, X_n)$ eine Stichprobe und
        $\theta$ ein Parameter. Dann hei&szlig;t die Abbildung
        $$T: \mathcal{X} \rightarrow \tilde{\Thata}$$
        mit $\tilde{\Thata} \supseteq \Theta$ ein Sch&auml;tzer f&uuml;r $\theta$.
    </dd>
<dt><dfn>Asymptotisch Erwartungstreuer Sch&auml;tzer</dfn></dt>
<dd>

        Ein Sch&auml;tzer $\hat{\theta} = \hat{\theta}(X_1, \dots, X_n)$ hei&szlig;t
        asymptotisch erwartungstreu, wenn der Grenzwert der zu sch&auml;tzenden
        Folge unter Annahme von $\theta$ gleich $\theta$ ist:

        $$\lim_{n \rightarrow \infty} \mathbb{E}(\hat{\theta}) = \theta$$

    </dd>
<dt><a href="https://de.wikipedia.org/wiki/Kalman-Filter"><dfn id="kalman-filter">Kalman-Filter</dfn></a> (<dfn>KF</dfn>)</dt>
<dd>Siehe <a href="https://martin-thoma.com/kalman-filter/">Kalman-filter Artikel</a>.</dd>
<dt><dfn id="extended-kalman-filter">Extended Kalman Filter</dfn> (<dfn id="ekf">EKF</dfn>)</dt>
<dd>Siehe <a href="https://martin-thoma.com/kalman-filter/">Kalman-filter Artikel</a>.</dd><a href="https://de.wikipedia.org/wiki/GUM_(Norm)"><dfn id="gum">GUM</dfn></a> (<dfn>Guide to the Expression of Uncertainty in Measurement</dfn>)</dl>
<dd>GUM ist eine internationale Norm welche das Ziel hat, die
        Vergleichbarkeit zwischen Messergebnissen herzustellen. Dazu
        wurden in der Norm Grunds&auml;tze und Vorgehensweisen zur Bestimmung der
        Messunsicherheit festgelegt.<br/>
<br/>
        GUM ist auf metrische Merkmale beschr&auml;nkt.</dd>
<dt><dfn>Standardunsicherheit</dfn></dt>
<dd>Die Standardunsicherheit einer Messung ist

        $$u_i = s(\bar{x_i}) = \sqrt{\frac{s^2(x_i)}{n}}$$</dd>

<h3 id="dempster-shafer-theorie">Dempster-Shafer-Theorie</h3>
<p>Slides: <code>IF-Kap3_160125.pdf</code></p>
<p>For this chapter, I highly recommend reading <a href="https://statistik.econ.kit.edu/download/Artikel%20-%20Anwendung%20der%20Dempster-Shafer%20Evidenztheorie%20auf%20die%20Bonit%C3%A4tspr%C3%BCfung.pdf">Anwendung der Dempster-Shafer Evidenztheorie auf die Bonit&auml;tspr&uuml;fung</a>.</p>
<dl>
<dt><dfn id="frame-of-discernment">Frame of discernment</dfn> (<dfn>Wahrnehmungsrahmen</dfn>)</dt>
<dd>

        Der Wahrnehmungsrahmen ist eine Menge $\Omega$. Die Elemente dieser
        Mengen hei&szlig;en Alternativen oder Aussagen. Eine Hypothese ist eine
        Teilmenge $H \subseteq \Omega$ des Wahrnehmungsrahmens.

    </dd>
<dt><dfn>Basisma&szlig;</dfn> (<dfn id="basic-probability-mass">basic probability mass</dfn>)</dt>
<dd>

        Sei $\Omega$ ein Wahrnehmungsrahmen und

        $$m: \mathcal{P}(\Omega) \rightarrow [0, 1]$$

        eine Abbildung von der Potenzmenge von $\Omega$ in das
        Einheitsintervall. $m$ hei&szlig;t <i>Basisma&szlig;</i>, wenn gilt:

        <ul>
<li>$m(\emptyset) = 0$</li>
<li>$\sum_{X \subseteq \Omega} m(X) = 1$</li>
</ul>
</dd>
<dt><dfn id="belief-function">Belief function</dfn> (<dfn>Glaubensfunktion</dfn>)</dt>
<dd>

        Sei $\Omega$ ein Wahrnehmungsrahmen, $m$ ein Basisma&szlig; und
        $$Bel: \mathcal{P}(\Omega) \rightarrow [0, 1]$$
        eine Funktion. $Bel$ hei&szlig;t Glaubensfunktion, wenn gilt:

        $$Bel(X) := \sum_{Y \subseteq X} m(Y)$$

        Die Glaubensfunktion stellt also eine untere Grenze f&uuml;r eine unbekannte
        Wahrscheinlichkeitsfunktion dar.

    </dd>
<dt><dfn id="plausibility-function">Plausibility function</dfn> (<dfn>Plausibilit&auml;tsfunktion</dfn>)</dt>
<dd>

        Sei $\Omega$ ein Wahrnehmungsrahmen, $m$ ein Basisma&szlig; und
        $$Pl: \mathcal{P}(\Omega) \rightarrow [0, 1]$$
        eine Funktion. $Pl$ hei&szlig;t Plausibilit&auml;tsfunktion, wenn gilt:

        $$Bel(X) := \sum_{Y \cap X \neq \emptyset} m(Y)$$

        Die Glaubensfunktion stellt also eine obere Grenze f&uuml;r eine unbekannte
        Wahrscheinlichkeitsfunktion dar.

    </dd>
<dt><dfn>Fokale Ereignisse</dfn></dt>
<dd>

        Ein Ereignis $A$ hei&szlig;t fokal bzg. eines Basisma&szlig;es $m$, wenn $m(A) \neq 0$ gilt.

    </dd>
<dt><dfn>Dempsters Kombinationsregel</dfn> (<dfn>Dempsters rule of combination</dfn>, <dfn>DRC</dfn>)</dt>
<dd>$$m_1 \oplus m_2 (A) := \begin{cases}0&amp;\text{for } A = \emptyset\\
                                             \frac{\sum_{X, Y: X \cap Y = A} m_1(X) m_2(Y)}{|1-K|}\end{cases}$$
        f&uuml;r Konfliktgrad $$K := \sum_{X, Y: X \cap Y = \emptyset} m_1(X) m_2(Y)$$
        Bei einem Konfliktgrad von $0 &lt; K &lt; 1$ spricht man von einem
        partiellen Konflikt. Ist der Konfliktgrad gleich $K=1$, so ist DRC
        nicht anwendbar.<br/>
<br/>
        DRC ist assoziativ und kommutativ, allerdings nicht idempotent.
        Es gilt also im Allgemeinen nicht $m \oplus m = m$.<br/>
        Bei der Berechnung des Konfliktgrades gen&uuml;gt es fokale Ereignisse zu
        betrachten.</dd>
<dt><dfn>Bayessche Fusion</dfn></dt>
<dd>

        Angenommen man hat eine Klassifikationsaufgabe. $z$ geh&ouml;rt einer der
        Klassen $A, B, C$ an. Nun liefert ein Klassifizierer $d_1$ die
        Wahrscheinlichkeitsverteilung

        $$m_1(A) = 0.01 \qquad m_1(B) = 0.99 \qquad m_1(C) = 0$$

        und ein zweiter Klassifizierer $d_2$ liefert

        $$m_2(A) = 0.01 \qquad m_2(B) = 0 \qquad m_2(C) = 0.99$$

        Gesucht ist eine Wahrscheinlichkeitsverteilung, welche die beiden
        Ergebnisse fusioniert.<br/>
<br/>

        Da kein Vorwissen existiert, wird das Maximum-Entropie-Prinzip f&uuml;r die
        a priori Wahrscheinlichkeitsverteilung verwendet. Man geht also a
        priori davon aus, dass jede Klasse gleich wahrscheinlich ist:
        $$P(z) = (\frac{1}{3}; \frac{1}{3}; \frac{1}{3})$$<br/>
<br/>

        Nun gilt:

$$
\begin{align}
    P(z | d_1, d_2)&amp;= \frac{P(d_1, d_2 | z) \cdot P(z)}{P(d_1, d_2)}\\
                   &amp;= \frac{P(d_1 | z) \cdot P(d_2 | d_1, z) \cdot P(z)}{P(d_1, d_2)}\\
                   &amp;\overset{(1)}{=} \frac{P(d_1 | z) \cdot P(d_2 | z) \cdot P(z)}{P(d_1, d_2)}\\
                   &amp;= \frac{\begin{pmatrix}0.1\\0.99\\0\end{pmatrix} \cdot \begin{pmatrix}0.1\\0\\0.99\end{pmatrix} \cdot \begin{pmatrix}1/3\\1/3\\1/3\end{pmatrix}}{P(d_1, d_2)}\\
                   &amp;= \frac{\begin{pmatrix}1/300\\0\\0\end{pmatrix}}{P(d_1, d_2)}\\
                   &amp;\overset{(2)}{=} \begin{pmatrix}1\\0\\0\end{pmatrix}\\
\end{align}
$$

        bei (1) wurde Unabh&auml;ngigkeit vorausgesetzt, bei (2) wurde auf 1
        normiert, damit eine Wahrscheinlichkeitsverteilung herauskommt.

    </dd>
</dl>
<h3 id="fuzzy-systeme">Fuzzy-Systeme</h3>
<p>Slides: <code>IF-Kap4_160125.pdf</code></p>
<p>Zur Einf&uuml;hrung:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=r804UF8Ia4c">Fuzzy Logic - Computerphile</a></li>
<li><a href="https://www.youtube.com/watch?v=P8wY6mi1vV8">Fuzzy Logic: An Introduction</a></li>
<li><a href="https://www.youtube.com/watch?v=rln_kZbYaWc">An Introduction to Fuzzy Logic</a>: An example with breaks</li>
</ul>
<dl>
<dt><dfn>Zugeh&ouml;rigkeitsfunktion</dfn> (<dfn id="membership-function">membership function</dfn>)</dt>
<dd>Sei $\Omega$ ein Grundraum und $A$ eine unscharfe Menge, f&uuml;r die
        $\mu_A: X \rightarrow [0, 1]$ den Grad der Zugeh&ouml;rigkeit definiert.

        $\mu_A$ hei&szlig;t Zugeh&ouml;rigkeitsfunktion, wenn gilt
        $$\mu_{\Omega \setminus A}(t) = 1 - \mu_{A}$$</dd>
<dt><a href="https://de.wikipedia.org/wiki/Satz_vom_ausgeschlossenen_Dritten"><dfn>Gesetz vom ausgeschlossenen Dritten</dfn></a></dt>
<dd>F&uuml;r eine beliebige Aussage muss mindestens die Aussage selbst oder ihr Gegenteil gelten. Dies gilt in der klassischen Mengenlehre, jedoch nicht f&uuml;r unscharfe Mengen.</dd>
<dt><a href="https://de.wikipedia.org/wiki/Satz_vom_Widerspruch"><dfn>Gesetz vom ausgeschlossenen Widerspruch</dfn></a></dt>
<dd>

        Zwei einander widersprechende Aussagen k&ouml;nnen nicht zugleich zutreffen.

        Dies gilt in der klassischen Mengenlehre, jedoch nicht f&uuml;r unscharfe Mengen.

    </dd>
<dt><dfn>Fuzzy-Operationen</dfn></dt>
<dd>

        Es seien $\mu_A, \mu_B$ die Zugeh&ouml;rigkeitsfunktionen zweier unscharfer
        Mengen $A, B$ &uuml;ber dem Grundraum $\Omega$. Dann gilt:<br/>
<br/>
        Not:
        $$\forall x \in \Omega: \mu_{\Omega \setminus A}(x) = 1 - \mu_A(x)$$
        <br/>
        Konjunktion (AND, Minimum-T-Norm):
        $$\forall x \in \Omega: \mu_{A \land B}(x) = \min(\mu_A(x), \mu_B(x))$$
        <br/>
        Disjunktion (OR, Maximum-T-Norm):
        $$\forall x \in \Omega: \mu_{A \lor B}(x) = \max(\mu_A(x), \mu_B(x))$$

    </dd>
<dt><dfn>Defuzzifizierung</dfn> (<a href="https://en.wikipedia.org/wiki/Defuzzification"><dfn>Defuzzification</dfn></a>)</dt>
<dd>Unter Defuzzifizierung versteht man die Berechnung des scharfen
        Wertes der Ausgangsgr&ouml;&szlig;e.<br/>
<br/>
        Methoden:

        <ul>
<li><a href="#center-of-gravity-defuzzification">Schwerpunktverfahren</a></li>
<li><a href="#mean-of-maxima-defuzzification">Maximum-Mittelwert-Methode</a></li>
</ul>
</dd>
<dt><dfn>Schwerpunktverfahren</dfn> (<dfn id="center-of-gravity-defuzzification">center of gravity</dfn>, <dfn>COG</dfn>)</dt>
<dd>Das Schwerpunktverfahren dient zur Defuzzifizierung.</dd>
<dt><dfn>Maximum-Mittelwert-Methode</dfn> (<dfn id="mean-of-maxima-defuzzification">Mean of maxima</dfn>, <dfn>MOM</dfn>)</dt>
<dd>Das Schwerpunktverfahren dient zur Defuzzifizierung.</dd>
</dl>
<h3 id="neuronale-netze">Neuronale Netze</h3>
<p>Slides: <code>IF-Kap5_160125.pdf</code></p>
<p>Siehe <a href="https://martin-thoma.com/neuronale-netze-vorlesung/">Vorlesung Neuronale Netze</a></p>
<h3 id="registrierung">Registrierung</h3>
<p>Slides: <code>IF-Kap6_160125.pdf</code></p>
<p>Wurde nicht besprochen.</p>
<h3 id="energiefunktionale">Energie&shy;funktionale</h3>
<p>Slides: <code>IF-Kap7_160125.pdf</code></p>
<dl>
<dt><a href="https://de.wikipedia.org/wiki/Funktional"><dfn>Funktional</dfn></a></dt>
<dd>Ein <i>Funktional</i> ist eine Funktion aus einem Vektorraum $V$ in
        den K&ouml;rper, der dem Vektorraum zugrunde liegt. Oft ist $V$ ein
        Funktionenraum, also ein Vektorraum, dessen Elemente reell- oder
        komplexwertige Funktionen sind. Ein Funktional ist somit eine Funktion
        auf Funktionen.</dd>
<dt><dfn id="energiefunktionale">Energiefunktionale</dfn></dt>
<dd>Durch die Einf&uuml;hrung von Energietermen $E_k$ lassen sich
        fusionsrelevante Informationen modellieren. Die Fusionsaufgabe wird
        dann durch das Energiefunktional

        $$E = \sum_k \lambda_k E_k, \qquad \lambda_k &gt; 0$$

        repr&auml;sentiert. Die unterschiedliche Relevanz der Energieterme $E_k$
        wird durch die Vorfaktoren $\lambda_k$ ber&uuml;cksichtigt.</dd>
<dt><a href="https://en.wikipedia.org/wiki/Boltzmann_distribution"><dfn id="gibbs-distribution">Gibbs-Verteilung</dfn></a></dt>
<dd>Die Gibbs-Verteilung mit dem Energiefunktional $E$ ist
        $$\pi_{\beta, E}(x) = \frac{1}{Z} e^{- \beta E(x)},$$
        mit Normierungskonstante $Z$ und der inversen Temperatur
        $\beta = \frac{1}{T}$.<br/>
<br/>
<u>Interpretation</u>:
        Es sei $E: V \rightarrow \mathbb{R}$, $V$ endlich.
        <ul>
<li>$V$: Die Menge aller Konfigurationen eines physikalischen Systems.</li>
<li>$E(x)$: Energie des Systems, wenn es sich in der Konfiguration $x$ befindet.</li>
<li>$T$: Temperatur. Ist die Temperatur gro&szlig;, so sind alle
                Konfigurationen etwa gleich wahrscheinlich. Bei niedriger
                Temperatur werden Konfigurationen mit niedriger Energie
                bevorzugt.</li>
<li>$\pi_{\beta, E}(x)$: Wahrscheinlichkeit, dass sich das System
                in der Konfiguration $x$ befindet.</li>
</ul>

        F&uuml;r $E$ kann dann eine Gibbsche Wahrscheinlichkeitsdichtefunktion WDF
        $$WDF \propto e^{- \beta E} = \prod_k e^{-\frac{\lambda_k E_k}{T}}$$
        definiert werden.
        </dd>
<dt><dfn>Energieminimierung</dfn></dt>
<dd>
<ol>
<li>L&ouml;sung eines linearen Gleichungssystems (selten m&ouml;glich)</li>
<li>Graph-Cuts-Verfahren</li>
<li>Approximative L&ouml;sung durch sukzessive Optimierung</li>
<li>Methode des steilsten abstiegs</li>
<li>Monte-Carlo-Methode</li>
<li>Simulated Annealing</li>
<li>Lineare Programme</li>
<li>Dynamische Programmierung</li>
<li>Mean Field Theorie (Betrachte Erwartungswerte)</li>
</ol>
</dd>
</dl>
<h2 id="abkurzungen_1">Abk&uuml;rzungen</h2>
<ul>
<li>EKF: Extended Kalman Filter</li>
<li>GUM: Guide to the Expression of Uncertainty in Measurement</li>
<li>KF: Kalman Filter</li>
<li>LS: Least Squares</li>
<li>UKF: Unscented Kalman Filter</li>
</ul>
<h2 id="meine-fragen">Meine Fragen</h2>
<ul>
<li>Kapitel 1, Folie 61: Was ist der Definitions / Wertebereich von Information?</li>
<li>Kapitel 2, Folie 5: Alle Ereignisse paarweise disjunkt</li>
<li>Kapitel 2, Folie 22: Man muss f&uuml;r wirksame Sch&auml;tzer noch fordern, dass sie
                       erwartungstreu sind. Es gibt immer den konstanten
                       Sch&auml;tzer, welcher die Stichprobe ignoriert und somit
                       eine Varianz von 0 hat.</li>
<li>Kapitel 2, Folie 37: Was ist ein Arbeitspunkt?</li>
<li>Kapitel 2, Folie 44f: Fusion 2er gr&ouml;&szlig;en / Verteilungen</li>
<li>Kapitel 2, Folie 79: Was ist der Trunkation error? Was ist der base point error und warum ist es ein Problem, dass man um den Sch&auml;tzwert und nicht um den wahren Wert linearisiert?</li>
</ul>
<h2 id="ubungsaufgaben">&Uuml;bungsaufgaben</h2>
<p>Die L&ouml;sungen sind auch online (ausf&uuml;hrlicher und besser als ich es hier habe).</p>
<h3 id="ub-1">&Uuml;B 1</h3>
<ul>
<li>Aufgabe 1.1: http://math.stackexchange.com/q/1919394/6876</li>
<li>Aufgabe 1.2: <span class="math">\(P(A) = 0.5 = P(B) = P(C)\)</span>,
  <div class="math">$$
  \begin{align}
  P(A) \cdot P(B) &amp;= 0.25 = P(A \cap B)\\
  P(A) \cdot P(C) &amp;= 0.25 = P(A \cap C)\\
  P(A) \cdot P(B) \cdot P(C) &amp;= 0.125 \neq 0.25 = P(A \cap B \cap C)
  \end{align}
  $$</div>
  Daher sind die Ereignisse <span class="math">\(A\)</span> und <span class="math">\(B\)</span>, die Ereignisse <span class="math">\(A, C\)</span>, die Ereignisse
  <span class="math">\(B, C\)</span> unabh&auml;ngig. Die Ereignisse <span class="math">\(A, B, C\)</span> sind jedoch nicht unabh&auml;ngig.</li>
<li>Aufgabe 1.3a: <span class="math">\(5 \cdot (\frac{1}{6} \cdot \frac{1}{6}) = \frac{5}{36}\)</span></li>
<li>Aufgabe 1.3b: <span class="math">\(\frac{\frac{2}{5} \cdot \frac{5}{36}}{1-0.25} = \frac{2}{27}\)</span></li>
<li>Aufgabe 1.4:<ul>
<li><span class="math">\(P(X = 2) = P(X=12) = \frac{1}{36}\)</span></li>
<li><span class="math">\(P(X = 3) = P(X=11) = \frac{2}{36}\)</span></li>
<li><span class="math">\(P(X = 4) = P(X=10) = \frac{3}{36}\)</span></li>
<li><span class="math">\(P(X = 5) = P(X=9) = \frac{4}{36}\)</span></li>
<li><span class="math">\(P(X = 6) = P(X=8) = \frac{5}{36}\)</span></li>
<li><span class="math">\(P(X = 7) = \frac{6}{36}\)</span></li>
<li><span class="math">\(F(x) = \sum_{i=2}^x P(X = i)\)</span></li>
<li><span class="math">\(\mathbb{E}(X) = 2 \cdot 3.5 = 7\)</span></li>
</ul>
</li>
<li>Aufgabe 1.5a: <span class="math">\(\int_0^\infty (\alpha \cdot \exp(-\alpha x)) \mathrm{d}x = \alpha \int_0^\infty \exp(-\alpha x) \mathrm{d}x = \alpha [-\frac{1}{\alpha} \exp(-\alpha x)]_0^\infty = 1\)</span></li>
<li>Aufgabe 1.5b: TODO</li>
<li>Aufgabe 1.6:<ul>
<li><span class="math">\(G\)</span>: E-mail ist gesch&auml;ftlich, <span class="math">\(\bar{G}\)</span> ist privat</li>
<li><span class="math">\(S\)</span>: E-mail ist spam, <span class="math">\(\bar{S}\)</span> ist ham</li>
<li><span class="math">\(F\)</span>: E-mail enth&auml;lt das Wort "Free"</li>
<li><span class="math">\(P(S | F) = \frac{P(F | S) \cdot P(S)}{P(F)} = \frac{0.9 \cdot 0.7}{0.9 \cdot 0.7 + 0.01 \cdot 0.3} = \frac{210}{211}\)</span></li>
</ul>
</li>
</ul>
<h3 id="ub-2">&Uuml;B 2</h3>
<ul>
<li>Aufgabe 1.1: Die kontinuierliche Entropie ist kein resultat immer feiner werdender Diskretisierungen der diskreten Entropie</li>
<li>Aufgabe 1.2a: <span class="math">\(P(B \cap L) = P(B) \cdot P(L) = 1/3 \cdot 1/3 = 1/9\)</span></li>
<li>Aufgabe 1.2b: 1/9</li>
<li>Aufgabe 1.2c: Das Prinzip der maximalen Entropie f&uuml;r Zufallsvariablen f&uuml;hrt zur Unabh&auml;ngigkeitsannahme.</li>
<li>Aufgabe 1.2d: Die Entropie wird im Erwartungswert reduziert, wenn eine Gr&ouml;&szlig;e durch eine andere bedingt wird.</li>
<li>Aufgabe 1.3a: <span class="math">\(P(z=z_A| d_A) = 0.45\)</span>, <span class="math">\(P(z=z_B| d_A) = 0.45\)</span>, <span class="math">\(P(z=z_K| d_A) = 0.1\)</span></li>
<li>Aufgabe 1.3b: Ist hier ein Zahlendreher passiert?</li>
<li>Aufgabe 1.4a: Mit "Detektionsleistung" ist gemeint, wie wahrscheinlich der
                Sensor ein Objekt detektiert, wenn eines da ist. Mit
                "Klassifikationsleistung" ist gemeint, wie Wahrscheinlich
                der Sensor bei vorhandenem Objekt dieses richtig klassifiziert.</li>
<li>Aufgabe 1.4b: TODO ???</li>
<li>Aufgabe 1.4c: TODO ???</li>
<li>Aufgabe 1.4d: TODO ???</li>
<li>Aufgabe 1.5: TODO ???</li>
<li>Aufgabe 1.6: 0.043 (Das typische Patenten-Test-Beispiel)</li>
</ul>
<h3 id="ub-3">&Uuml;B 3</h3>
<ul>
<li>Aufgabe 1.1:<ul>
<li><span class="math">\(Bel(A) = \sum_{B \subseteq A} m(B)\)</span></li>
<li><span class="math">\(Pl(A) = 1 - Bel(\bar{A}) = \sum_{B \cap A \neq \emptyset} m(B)\)</span></li>
</ul>
</li>
<li>Aufgabe 1.2a: Obwohl beide Basisma&szlig;e dem Ereignis A eine sehr niedriges Ma&szlig;
  zuweisen, ist es durch DRC das Ereignis mit dem h&ouml;chsten Wert. Das liegt
  daran, dass die anderen jeweils exakt 0 haben.</li>
<li>Aufgabe 1.2b: Jeder einzelne Experte gab A nur geringen glauben. Dennoch wird A deutlich am meisten Glauben nach der Fusion geschenkt.</li>
<li>Aufgabe 1.2c: Das gleiche Ergebnis.</li>
<li>Aufgabe 1.3a: <span class="math">\(m_{123}(111) = 0.82\)</span>, das Ergebnis ist also zu 82% glaubw&uuml;rdig. (Sch&ouml;nes beispiel, dass DRC nicht idempotent ist)</li>
<li>Aufgabe 1.3b: Rechnen mit Basisma&szlig;en / DRC</li>
<li>Aufgabe 1.3c: Rechnen mit Basisma&szlig;en / DRC</li>
<li>Aufgabe 1.4: <span class="math">\(P(s=A | w=A) + P(s=B | w=B) = 0.6\)</span></li>
<li>Aufgabe 2.1: Spielen mit Fuzzy-Mengen</li>
<li>Aufgabe 2.2: XOR f&uuml;r Fuzzy-Mengen</li>
</ul>
<h2 id="prufungsfragen_1">Pr&uuml;fungsfragen</h2>
<ul>
<li>Welche Arten von Unsicherheit kennen Sie?<br/>
  &rarr; Unsicherheit kann man mit Wahrscheinlichkeiten, der Dempster-Shafer-Theorie
     (Basisma&szlig;e) und &uuml;ber Fuzzy-Systeme beschreiben.</li>
<li>Was ist der Unterschied zwischen Wahrscheinlichkeiten und Basisma&szlig;en?<br/>
  &rarr; Basisma&szlig;e sind nicht monoton und nicht additiv.</li>
<li>Wie lautet die Formel f&uuml;r verteilte Fusion?<br/>
  &rarr; TODO</li>
<li>Wie lauten die Axiome von Kolmogorov?<br/>
  &rarr; Siehe <a href="#kolmogorov-axioms">oben</a></li>
<li>Was sind Zugeh&ouml;rigkeitsfunktionen?<br/>
  &rarr; Siehe <a href="#membership-function">oben</a></li>
</ul>
<h3 id="kalman-filter">Kalman-Filter</h3>
<ul>
<li>Aus welchen Schritten besteht der Kalman-Filter?<br/>
  &rarr; Pr&auml;diktion, Innovation</li>
<li>Welche Erweiterungen zum Kalman-Filter kennen Sie?<br/>
  &rarr; Extended Kalman Filter (EKF), UKF (Unscented Kalman Filter)</li>
<li>F&uuml;r welche Systeme ist der Kalman-Filter geeignet?<br/>
  &rarr; Lineare Zeitinvariante Systeme (LTI-Systeme)</li>
<li>Wie entwickeln sich die Wahrscheinlichkeiten beim Kalman-Filter?<br/>
  &rarr; Bei der Pr&auml;diktion steigt die Unsicherheit, bei der Innovation sinkt sie.</li>
<li>Wie lautet das Systemmodell im Kalman-Filter?<br/>
  &rarr; vgl. <a href="https://martin-thoma.com/kalman-filter/#step-2-modelling">Kalman-Filter Artikel</a></li>
</ul>
<h2 id="absprachen_1">Absprachen</h2>
<ul>
<li>Kapitel 5 (Neuronale Netze) und Kapitel 6 (Registrierung) kommen nicht dran.</li>
<li>&Uuml;bungsaufgaben sind auch Pr&uuml;fungsrelevant.</li>
</ul>
<h2 id="material-und-links">Material und Links</h2>
<ul>
<li><a href="http://ies.anthropomatik.kit.edu/lehre_informationsfusion.php">Vorlesungswebsite</a></li>
<li><a href="https://ankiweb.net/shared/info/1070725022">Anki-Deck</a></li>
<li><a href="https://statistik.econ.kit.edu/download/Artikel%20-%20Anwendung%20der%20Dempster-Shafer%20Evidenztheorie%20auf%20die%20Bonit%C3%A4tspr%C3%BCfung.pdf">Anwendung der Dempster-Shafer Evidenztheorie auf die Bonit&auml;tspr&uuml;fung</a></li>
</ul>
<h2 id="vorlesungsempfehlungen">Vorlesungs&shy;empfehlungen</h2>
<p>Folgende Vorlesungen sind &auml;hnlich:</p>
<ul>
<li><a href="https://martin-thoma.com/analysetechniken-grosser-datenbestaende/">Analysetechniken gro&szlig;er Datenbest&auml;nde</a></li>
<li><a href="https://martin-thoma.com/informationsfusion/">Informationsfusion</a></li>
<li><a href="https://martin-thoma.com/machine-learning-1-course/">Machine Learning 1</a></li>
<li><a href="https://martin-thoma.com/machine-learning-2-course/">Machine Learning 2</a></li>
<li><a href="https://martin-thoma.com/mustererkennung-klausur/">Mustererkennung</a></li>
<li><a href="https://martin-thoma.com/neuronale-netze-vorlesung/">Neuronale Netze</a></li>
<li><a href="https://martin-thoma.com/lma/">Lokalisierung Mobiler Agenten</a></li>
<li><a href="https://martin-thoma.com/probabilistische-planung/">Probabilistische Planung</a></li>
</ul>
<h2 id="termine-und-klausurablauf">Termine und Klausurablauf</h2>
<p>Es ist eine m&uuml;ndliche Pr&uuml;fung.</p>
<p>Ich habe meine am <b>Fraunhofer IOSB, Fraunhoferstr. 1, 76131 Karlsruhe</b>
am <b>11.10.2016</b> um <b>15:30 Uhr</b> bei Herrn Dr. Heizmann.</p>
            
            <div id="disqus_thread"></div>
<script>
    /*
    var disqus_config = function () {
        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW
        var d = document, s = d.createElement('script');

        s.src = '//martinthoma.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

            <hr/>
        </div>
        <section>
        <div class="col-sm-2 col-md-2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time pubdate="pubdate" datetime="2016-05-23T20:00:00+02:00">Mai 23, 2016</time>
            <br/>
            by <a rel="author" class="vcard author post-author" itemprop="author" href="../author/martin-thoma/"><span class="fn" itemscope="" itemtype="https://schema.org/Person"><span itemprop="name">Martin Thoma</span></span></a>
            <h4>Category</h4>
            <a class="category-link" href="../categories.html#german-posts-ref">German posts</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="../tags.html#klausur-ref">Klausur
                    <span>33</span>
</a></li>
            </ul>
<h4>Contact</h4>
    <a href="https://twitter.com/themoosemind" title="My Twitter Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-twitter sidebar-social-links"></i></a>
    <a href="mailto:info@martin-thoma.de" title="My Email Address" class="sidebar-social-links" target="_blank">
    <i class="fa fa-envelope sidebar-social-links"></i></a>
    <a href="https://github.com/MartinThoma" title="My Github Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-github sidebar-social-links"></i></a>
    <a href="http://stackoverflow.com/users/562769/martin-thoma" title="My Stackoverflow Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-stackoverflow sidebar-social-links"></i></a>
        </div>
        </section>
</div>
</article>
                </div>
                <div class="col-sm-1 col-md-1"></div>
            </div>
        </div>
        <div id="push"></div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-subtitle"><span class="site-name">Martin Thoma</span> - A blog about Code, the Web and Cyberculture</li>
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>        <script src="//code.jquery.com/jquery.min.js"></script>
        <!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>
        <script>
(function(){
    'use strict';

    /*
    Create intra-page links
    Requires that your headings already have an `id` attribute set (because that's what jekyll does)
    For every heading in your page, this adds a little anchor link `#` that you can click to get a permalink to the heading.
    Ignores `h1`, because you should only have one per page.
    The text content of the tag is used to generate the link, so it will fail "gracefully-ish" if you have duplicate heading text.

    Credit: https://gist.github.com/SimplGy/a229d25cdb19d7f21231
     */

    var headingNodes = [], results, link,
        tags = ['h2', 'h3', 'h4', 'h5', 'h6'];

    tags.forEach(function(tag){
        var contentTag = document.getElementById('contentAfterTitle');
      results = contentTag.getElementsByTagName(tag);
      Array.prototype.push.apply(headingNodes, results);
    });

    headingNodes.forEach(function(node){
      link = document.createElement('a');
      link.className = 'deepLink';
      link.textContent = ' ¶';
      link.href = '#' + node.getAttribute('id');
      node.appendChild(link);
    });

  })();
</script>
    </body>
    <!-- Theme: Elegant built for Pelican
    License : http://oncrashreboot.com/pelican-elegant -->
</html>