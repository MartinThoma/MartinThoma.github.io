<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Martin Thoma - Machine Learning</title><link href="https://martin-thoma.com/" rel="alternate"></link><link href="https://martin-thoma.com/feeds/machine-learning.atom.xml" rel="self"></link><id>https://martin-thoma.com/</id><updated>2017-03-25T20:00:00+01:00</updated><entry><title>ML Review 4</title><link href="https://martin-thoma.com/ml-review-4/" rel="alternate"></link><published>2017-03-25T20:00:00+01:00</published><updated>2017-03-25T20:00:00+01:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2017-03-25:/ml-review-4/</id><summary type="html">&lt;p&gt;This Review gives an overview of intersting stuff I stumbled over which are
related to machine learning.&lt;/p&gt;
&lt;h2 id="new-developments"&gt;New Developments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/MarvinTeichmann/KittiSeg"&gt;KittiSeg&lt;/a&gt; (&lt;a href="https://www.reddit.com/r/MachineLearning/comments/5y8c5w/p_kittiseg_a_toolkit_to_perform_semantic/"&gt;reddit&lt;/a&gt;): A toolkit for semantic segmentation based on &lt;a href="https://github.com/TensorVision/TensorVision"&gt;TensorVision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://research.google.com/audioset/"&gt;AudioSet&lt;/a&gt;: A dataset for accoustic events&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- 
## Live Demos and Websites

* [universe.openai.com](https://universe.openai.com/): Related to the OpenAI gym.
* [Project Malmo](http://blogs.microsoft.com/next/2016/03/13/project-malmo-using-minecraft-build-intelligent-technology/): Train RL agents in Minecraft
* [VISIIR](http://visiir.lip6.fr/): VIsual Seek for Interactive Image Retrieval - classifying food
* [Image-to-Image](http://affinelayer.com/pixsrv/index.html) --&gt;
&lt;h2 id="publications"&gt;Publications&lt;/h2&gt;
&lt;!-- e.g. arXiv --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1703.03864"&gt;Evolution Strategies as a Scalable Alternative to Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1703.00955"&gt;Controllable Text Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1703.02528"&gt;Stopping GAN …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;This Review gives an overview of intersting stuff I stumbled over which are
related to machine learning.&lt;/p&gt;
&lt;h2 id="new-developments"&gt;New Developments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/MarvinTeichmann/KittiSeg"&gt;KittiSeg&lt;/a&gt; (&lt;a href="https://www.reddit.com/r/MachineLearning/comments/5y8c5w/p_kittiseg_a_toolkit_to_perform_semantic/"&gt;reddit&lt;/a&gt;): A toolkit for semantic segmentation based on &lt;a href="https://github.com/TensorVision/TensorVision"&gt;TensorVision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://research.google.com/audioset/"&gt;AudioSet&lt;/a&gt;: A dataset for accoustic events&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- 
## Live Demos and Websites

* [universe.openai.com](https://universe.openai.com/): Related to the OpenAI gym.
* [Project Malmo](http://blogs.microsoft.com/next/2016/03/13/project-malmo-using-minecraft-build-intelligent-technology/): Train RL agents in Minecraft
* [VISIIR](http://visiir.lip6.fr/): VIsual Seek for Interactive Image Retrieval - classifying food
* [Image-to-Image](http://affinelayer.com/pixsrv/index.html) --&gt;
&lt;h2 id="publications"&gt;Publications&lt;/h2&gt;
&lt;!-- e.g. arXiv --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1703.03864"&gt;Evolution Strategies as a Scalable Alternative to Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1703.00955"&gt;Controllable Text Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1703.02528"&gt;Stopping GAN Violence: Generative Unadversarial Networks&lt;/a&gt;: Probably one of the funniest ML things I've seen so far. Reminds me of &lt;a href="https://www.youtube.com/watch?v=DQWI1kvmwRg"&gt;Machine Learning A Cappella - Overfitting Thriller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1703.06857"&gt;Deep Neural Networks Do Not Recognize Negative Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1703.06618"&gt;Twitter100k: A Real-world Dataset for Weakly Supervised Cross-Media Retrieval&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://opensurfaces.cs.cornell.edu/publications/minc/"&gt;MINC-2500&lt;/a&gt; dataset&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1703.06817"&gt;Second-order Convolutional Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="software"&gt;Software&lt;/h2&gt;
&lt;!-- e.g. Theano, Keras, ... --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Pi-DeepLearning/RaspberryPi-FaceDetection-MTCNN-Caffe-With-Motion"&gt;Pi-DeepLearning&lt;/a&gt; (&lt;a href="https://www.reddit.com/r/MachineLearning/comments/5xrt2m/pmtcnn_face_detection_on_raspberry_pi_3_with/"&gt;reddit&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://deepdetect.com/"&gt;DeepDetect&lt;/a&gt; (&lt;a href="https://github.com/beniz/deepdetect"&gt;GitHub&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="interesting-questions"&gt;Interesting Questions&lt;/h2&gt;
&lt;!-- For example StackExchange --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/q/42648465/562769"&gt;How to predict an item's category given its name?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.reddit.com/r/MLQuestions/comments/5yuc22/how_do_you_share_models/"&gt;How do you share models?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/q/41251698/562769"&gt;How many FLOPs does tanh need?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="miscallenious"&gt;Miscallenious&lt;/h2&gt;
&lt;h3 id="color-maps"&gt;Color Maps&lt;/h3&gt;
&lt;p&gt;Color Maps are important for visualizing data. But the default color map for
many applications is jet, which is bad for several reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It's hard to estimate distances from jet&lt;/li&gt;
&lt;li&gt;Doesn't work well when printed in grayscale&lt;/li&gt;
&lt;li&gt;Even worse if you are colorblind&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class="wp-caption aligncenter img-thumbnail"&gt;
&lt;img alt="Jet and the new colormaps" src="../images/2017/03/colormaps.png"/&gt;
&lt;figcaption class="text-center"&gt;Jet and the new colormaps&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The YouTube clip &lt;a href="https://www.youtube.com/watch?v=xAoljeRJ3lU"&gt;A Better Default Colormap for Matplotlib&lt;/a&gt;
by Nathaniel Smith and St&amp;eacute;fan van der Walt gives a short introduction into
color theory. They introduce &lt;a href="http://colorspacious.readthedocs.io/en/latest/"&gt;colorspacious&lt;/a&gt;
and &lt;a href="https://github.com/matplotlib/viscm"&gt;viscm&lt;/a&gt;. &lt;code&gt;viscm&lt;/code&gt; is a tool for
creating new color maps. They created &lt;code&gt;viridis&lt;/code&gt; as a better alternative to
&lt;code&gt;jet&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;A blog post with roughly the same content is at &lt;a href="https://bids.github.io/colormap/"&gt;bids.github.io/colormap&lt;/a&gt;.
This is the default for matplotlib 2.0. If you wonder which matplotlib version
you have:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt; &lt;span class="n"&gt;python&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="s2"&gt;"import matplotlib;print(matplotlib.__version__)"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That is how you update matplotlib:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo -H pip install matplotlib --upgrade
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here is a list of other matplotlib colormaps:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;['Accent', 'afmhot', 'autumn', 'binary', 'Blues', 'bone', 'BrBG', 'brg',
'BuGn', 'BuP', 'bwr', 'CMRmap', 'cool', 'coolwarm', 'copper', 'cubehelix',
'Dark2', 'flag', 'gist_earth', 'gist_gray', 'gist_heat', 'gist_ncar',
'gist_rainbow', 'gist_stern', 'gist_yarg', 'GnB', 'gnuplot', 'gnuplot2',
'gray', 'Greens', 'Greys', 'hot', 'hsv', 'jet', 'nipy_spectral', 'ocean',
'Oranges', 'OrRd', 'Paired', 'Pastel1', 'Pastel2', 'pink', 'PiYG', 'PRGn',
'prism', 'PuB', 'PuBuGn', 'PuOr', 'PuRd', 'Purples', 'rainbow', 'RdB', 'RdGy',
'RdP', 'RdYlB', 'RdYlGn', 'Reds', 'seismic', 'Set1', 'Set2', 'Set3',
'Spectral', 'spectral', 'spring', 'summer', 'terrain', 'Vega10', 'Vega20',
'Vega20b', 'Vega20c', 'winter', 'Wistia', 'YlGn', 'YlGnB', 'YlOrBr', 'YlOrRd']
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, some interesting links:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/a/32484915/562769"&gt;How to use viridis in matplotlib 1.4&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Matplotlib: &lt;a href="http://matplotlib.org/users/colormaps.html"&gt;Choosing Colormaps&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="class-distribution"&gt;Class distribution&lt;/h3&gt;
&lt;p&gt;You should always know if your data is severly unevenly distributed. Here is
a little script to visualize the data distribution:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# your labels&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# yes, +2.&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For the CIFAR100 training data, this is pretty boring:&lt;/p&gt;
&lt;figure class="wp-caption aligncenter img-thumbnail"&gt;
&lt;img alt="Distribution of the CIFAR 100 training data." src="../images/2017/03/cifar100_data_dist.png" style="width: 512px;"/&gt;
&lt;figcaption class="text-center"&gt;Distribution of the CIFAR 100 training data.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id="blogs-websites"&gt;Blogs / Websites&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://martin-thoma.com/ensembles/"&gt;Ensembles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/blog/big-data/2016/12/how-to-train-and-classify-images-using-google-cloud-machine-learning-and-cloud-dataflow"&gt;How to train and classify images using Google Cloud Machine Learning and Cloud Dataflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloudplatformonline.com/Machine-Learning-Startup-Competition.html"&gt;Machine Learning Startup Competition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/athelas/paper-1-baidus-deep-voice-675a323705df#.z937u7dki"&gt;Baidu Deep Voice explained: Part 1 &amp;mdash; the Inference Pipeline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://techcrunch.com/2017/03/17/laying-a-trap-for-self-driving-cars/"&gt;Laying a trap for self-driving cars&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://martin-thoma.com/image-classification/"&gt;SotA in Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://martin-thoma.com/object-detection/"&gt;SotA in Object detection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- ## Meetings

* London, 4. December 2016: [Data Visualization Challenge](https://www.eventbrite.com/e/immigration-by-numbers-insights-through-data-visualisation-tickets-28920900191?aff=twitter)
* Barcelona, 5. December 2016 - 10. December 2016: Neural Information Processing Systems (NIPS) ([Link](https://nips.cc/))
* Mannheim, 7. April 2017: [DataFest Germany](https://hiwissml.github.io/datafest2017.github.io/)
 --&gt;
&lt;div class="navigation clearfix"&gt;
&lt;div class="alignleft"&gt;
&lt;a href="https://martin-thoma.com/ml-review-3/" rel="prev"&gt;&amp;laquo; Previous Review&lt;/a&gt;
&lt;/div&gt;&lt;!--
    &lt;div class="alignright"&gt;
        &lt;a href="https://martin-thoma.com/ml-review-5/" rel="next"&gt;Next Review »&lt;/a&gt;
    &lt;/div&gt;--&gt;
&lt;/div&gt;</content><category term="Machine Learning"></category><category term="matplotlib"></category></entry><entry><title>Image Classification</title><link href="https://martin-thoma.com/image-classification/" rel="alternate"></link><published>2017-03-15T20:00:00+01:00</published><updated>2017-03-15T20:00:00+01:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2017-03-15:/image-classification/</id><summary type="html">&lt;p&gt;Image classification is the following task: You have an image and you want to
assign it one label. The set of possible labels is finite and typically not
bigger than 1000.&lt;/p&gt;
&lt;p&gt;So for example, you might ask: What can you see in this image?&lt;/p&gt;
&lt;figure class="wp-caption aligncenter img-thumbnail"&gt;
&lt;img alt="A jellyfish" src="../images/2017/03/moon-jelly.jpg" style="width: 512px;"/&gt;
&lt;figcaption class="text-center"&gt;A jellyfish&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;It is one of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Image classification is the following task: You have an image and you want to
assign it one label. The set of possible labels is finite and typically not
bigger than 1000.&lt;/p&gt;
&lt;p&gt;So for example, you might ask: What can you see in this image?&lt;/p&gt;
&lt;figure class="wp-caption aligncenter img-thumbnail"&gt;
&lt;img alt="A jellyfish" src="../images/2017/03/moon-jelly.jpg" style="width: 512px;"/&gt;
&lt;figcaption class="text-center"&gt;A jellyfish&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;It is one of the most common and probably simplest tasks in the intersection of
machine learning and computer vision. A commonly used dataset is &lt;a href="https://en.wikipedia.org/wiki/ImageNet"&gt;ImageNet&lt;/a&gt;,
which consists of exactly 1000&amp;nbsp;classes and has more than 1&amp;thinsp;000&amp;thinsp;000
training samples. To be exact, it is the ImageNet Large Scale Visual Recognition Challenge (ILSVRC).&lt;/p&gt;
&lt;p&gt;However, I miss easy to use examples. So here you are.&lt;/p&gt;
&lt;h2 id="prerequisites"&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/install/"&gt;Tensorflow&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="http://askubuntu.com/q/799184/10425"&gt;CUDA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://askubuntu.com/q/767269/10425"&gt;CuDNN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://keras.io/#installation"&gt;Keras&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="code"&gt;Code&lt;/h2&gt;
&lt;p&gt;The following code is taken from &lt;a href="https://github.com/fchollet/keras/blob/master/keras/applications/resnet50.py"&gt;Keras&lt;/a&gt; / &lt;a href="https://github.com/fchollet/deep-learning-models"&gt;Fran&amp;ccedil;ois Chollet&lt;/a&gt;. Full credit to him for doing the difficult work.&lt;/p&gt;
&lt;p&gt;The code defines one of the state of the art
models, a so called ResNet. See &lt;a href="https://arxiv.org/abs/1512.03385"&gt;Deep Residual Learning for Image Recognition&lt;/a&gt; for details. Then it downloads the weights, stores them for
subsequent uses and applies it to the data.&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
 28
 29
 30
 31
 32
 33
 34
 35
 36
 37
 38
 39
 40
 41
 42
 43
 44
 45
 46
 47
 48
 49
 50
 51
 52
 53
 54
 55
 56
 57
 58
 59
 60
 61
 62
 63
 64
 65
 66
 67
 68
 69
 70
 71
 72
 73
 74
 75
 76
 77
 78
 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env python&lt;/span&gt;
&lt;span class="c1"&gt;# -*- coding: utf-8 -*-&lt;/span&gt;
&lt;span class="sd"&gt;"""ResNet50 model for Keras."""&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;print_function&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;backend&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.preprocessing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.applications&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ResNet50&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.utils.data_utils&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;get_file&lt;/span&gt;

&lt;span class="n"&gt;CLASS_INDEX&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
&lt;span class="n"&gt;CLASS_INDEX_PATH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'https://s3.amazonaws.com/deep-learning-models/'&lt;/span&gt;
                    &lt;span class="s1"&gt;'image-models/imagenet_class_index.json'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;preprocess_input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim_ordering&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'default'&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    Standard preprocessing of image data.&lt;/span&gt;

&lt;span class="sd"&gt;    1. Make sure the order of the channels is correct (RGB, BGR, depending on&lt;/span&gt;
&lt;span class="sd"&gt;       the backend)&lt;/span&gt;
&lt;span class="sd"&gt;    2. Mean subtraction by channel.&lt;/span&gt;

&lt;span class="sd"&gt;    Parameters&lt;/span&gt;
&lt;span class="sd"&gt;    ----------&lt;/span&gt;
&lt;span class="sd"&gt;    x : numpy array&lt;/span&gt;
&lt;span class="sd"&gt;        The image&lt;/span&gt;
&lt;span class="sd"&gt;    dim_ordering : string, optional (default: 'default')&lt;/span&gt;
&lt;span class="sd"&gt;        Either 'th' for Theano or 'tf' for Tensorflow&lt;/span&gt;

&lt;span class="sd"&gt;    Returns&lt;/span&gt;
&lt;span class="sd"&gt;    -------&lt;/span&gt;
&lt;span class="sd"&gt;    numpy array&lt;/span&gt;
&lt;span class="sd"&gt;        The preprocessed image&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;dim_ordering&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;'default'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;dim_ordering&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;image_dim_ordering&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;dim_ordering&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'tf'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'th'&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;dim_ordering&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;'th'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mf"&gt;103.939&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mf"&gt;116.779&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mf"&gt;123.68&lt;/span&gt;
        &lt;span class="c1"&gt;# 'RGB'-&amp;gt;'BGR'&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mf"&gt;103.939&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mf"&gt;116.779&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mf"&gt;123.68&lt;/span&gt;
        &lt;span class="c1"&gt;# 'RGB'-&amp;gt;'BGR'&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;decode_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;top&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    Decode the predictionso of the ImageNet trained network.&lt;/span&gt;

&lt;span class="sd"&gt;    Parameters&lt;/span&gt;
&lt;span class="sd"&gt;    ----------&lt;/span&gt;
&lt;span class="sd"&gt;    preds : numpy array&lt;/span&gt;
&lt;span class="sd"&gt;    top : int&lt;/span&gt;
&lt;span class="sd"&gt;        How many predictions to return&lt;/span&gt;

&lt;span class="sd"&gt;    Returns&lt;/span&gt;
&lt;span class="sd"&gt;    -------&lt;/span&gt;
&lt;span class="sd"&gt;    list of tuples&lt;/span&gt;
&lt;span class="sd"&gt;        e.g. (u'n02206856', u'bee', 0.71072823) for the WordNet identifier,&lt;/span&gt;
&lt;span class="sd"&gt;        the class name and the probability.&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="k"&gt;global&lt;/span&gt; &lt;span class="n"&gt;CLASS_INDEX&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'`decode_predictions` expects '&lt;/span&gt;
                         &lt;span class="s1"&gt;'a batch of predictions '&lt;/span&gt;
                         &lt;span class="s1"&gt;'(i.e. a 2D array of shape (samples, 1000)). '&lt;/span&gt;
                         &lt;span class="s1"&gt;'Found array with shape: '&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;CLASS_INDEX&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;fpath&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_file&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'imagenet_class_index.json'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;CLASS_INDEX_PATH&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;cache_subdir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'models'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;CLASS_INDEX&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fpath&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;pred&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;top_indices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;top&lt;/span&gt;&lt;span class="p"&gt;:][::&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;CLASS_INDEX&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;top_indices&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;is_valid_file&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arg&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    Check if arg is a valid file that already exists on the file system.&lt;/span&gt;

&lt;span class="sd"&gt;    Parameters&lt;/span&gt;
&lt;span class="sd"&gt;    ----------&lt;/span&gt;
&lt;span class="sd"&gt;    parser : argparse object&lt;/span&gt;
&lt;span class="sd"&gt;    arg : str&lt;/span&gt;

&lt;span class="sd"&gt;    Returns&lt;/span&gt;
&lt;span class="sd"&gt;    -------&lt;/span&gt;
&lt;span class="sd"&gt;    arg&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;arg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abspath&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exists&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arg&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"The file &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt; does not exist!"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;arg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;arg&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_parser&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;"""Get parser object."""&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;argparse&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ArgumentParser&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ArgumentDefaultsHelpFormatter&lt;/span&gt;
    &lt;span class="n"&gt;parser&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ArgumentParser&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="vm"&gt;__doc__&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                            &lt;span class="n"&gt;formatter_class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ArgumentDefaultsHelpFormatter&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"-f"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"--file"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;dest&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"filename"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;is_valid_file&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                        &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Classify image"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;metavar&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"IMAGE"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;required&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;parser&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;"__main__"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;args&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_parser&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse_args&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="c1"&gt;# Load model&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ResNet50&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;include_top&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'imagenet'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;img_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_img&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;224&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;224&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;img_to_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;preprocess_input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Input image shape:'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;t0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;t1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Prediction time: {:0.3f}s"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;t0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;wordnet_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;class_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;decode_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"{wid}&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s2"&gt;{prob:&amp;gt;6}%&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s2"&gt;{name}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;wid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;wordnet_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                 &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;class_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                 &lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="si"&gt;%0.2f&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;p&gt;Store it as &lt;code&gt;resnet50.py&lt;/code&gt; and make it executable.&lt;/p&gt;
&lt;p&gt;(In case the JSON becomes unavailable: &lt;a href="https://github.com/MartinThoma/algorithms/blob/master/ML/ImageNet-classification/imagenet_class_index.json"&gt;Here you are&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id="how-to-use"&gt;How to use&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ./resnet50.py -f honey-bee.jpg
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;alternatively, if you have a GPU but not that much memory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nv"&gt;CUDA_VISIBLE_DEVICES&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;""&lt;/span&gt; ./resnet50.py -f honey-bee.jpg
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you apply this to the jellyfish image from above, you get:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Input image shape: (1, 224, 224, 3)
n01910747    100.00%    jellyfish
n01496331      0.00%    electric_ray
n10565667      0.00%    scuba_diver
n01914609      0.00%    sea_anemone
n02607072      0.00%    anemone_fish
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This takes about 6&amp;nbsp;seconds on CPU on my laptop.&lt;/p&gt;
&lt;h2 id="alternative-models"&gt;Alternative Models&lt;/h2&gt;
&lt;p&gt;If you are building an application, you might want to look into alternatives:&lt;/p&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;th&gt;Modelname&lt;/th&gt;
&lt;th&gt;Model size&lt;/th&gt;
&lt;th&gt;Input Size&lt;/th&gt;
&lt;th&gt;Top1-Accuracy&lt;/th&gt;
&lt;th&gt;Top5-Accuracy&lt;/th&gt;
&lt;th&gt;Time&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1512.03385"&gt;ResNet50&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;102.9 MB&lt;/td&gt;
&lt;td&gt;224 &amp;times; 224&lt;/td&gt;
&lt;td&gt;77.15%&lt;/td&gt;
&lt;td&gt;93.29%&lt;/td&gt;
&lt;td&gt;0.495s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1409.1556"&gt;VGG16&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;553.5 MB&lt;/td&gt;
&lt;td&gt;224 &amp;times; 224&lt;/td&gt;
&lt;td&gt;73.0%&lt;/td&gt;
&lt;td&gt;91.2%&lt;/td&gt;
&lt;td&gt;0.488s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://arxiv.org/abs/1512.00567"&gt;InceptionV3&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;95.1 MB&lt;/td&gt;
&lt;td&gt;299 &amp;times; 299&lt;/td&gt;
&lt;td&gt;78.8%&lt;/td&gt;
&lt;td&gt;94.4%&lt;/td&gt;
&lt;td&gt;0.681s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1610.02357"&gt;Xception&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;91.9 MB&lt;/td&gt;
&lt;td&gt;299 &amp;times; 299&lt;/td&gt;
&lt;td&gt;79.0%&lt;/td&gt;
&lt;td&gt;94.5%&lt;/td&gt;
&lt;td&gt;0.761s&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;div class="important"&gt;The speed only for the prediction. The model size is several 100&amp;nbsp;MB, so this takes a while. In a real application you can (1) load the model only once and (2) run the evaluation on a batch of many images to speed things up.&lt;/div&gt;
&lt;p&gt;More models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/titu1994/Inception-v4"&gt;titu1994/Inception-v4&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="see-also"&gt;See also&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"&gt;Building powerful image classification models using very little data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="machine learning"></category><category term="Computer Vision"></category><category term="ImageNet"></category></entry><entry><title>How to download ImageNet</title><link href="https://martin-thoma.com/download-data/" rel="alternate"></link><published>2017-03-06T20:00:00+01:00</published><updated>2017-03-06T20:00:00+01:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2017-03-06:/download-data/</id><summary type="html">&lt;p&gt;Machine Learning algorithms for computer vision need huge amounts of data.
Here are a few remarks on how to download them.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make sure you have enough space (&lt;code&gt;df -h&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Get a download manager. I use aria2c (&lt;code&gt;sudo apt-get install aria2&lt;/code&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For ImageNet, you have to register at &lt;a href="http://image-net.org/"&gt;image-net.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Download …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Machine Learning algorithms for computer vision need huge amounts of data.
Here are a few remarks on how to download them.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make sure you have enough space (&lt;code&gt;df -h&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Get a download manager. I use aria2c (&lt;code&gt;sudo apt-get install aria2&lt;/code&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For ImageNet, you have to register at &lt;a href="http://image-net.org/"&gt;image-net.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Download the files like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ aria2c -s &lt;span class="m"&gt;16&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;URL&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After downloading the file, use&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ md5sum &lt;span class="o"&gt;[&lt;/span&gt;Filename&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and compare the hash with the provided hash. If it differs, download the file
again.&lt;/p&gt;
&lt;p&gt;The ImageNet training data tar file contains 1000&amp;nbsp;files of the form
&lt;code&gt;n01440764.tar&lt;/code&gt;, &lt;code&gt;n01443537.tar&lt;/code&gt;, ...&lt;/p&gt;
&lt;p&gt;Each of those files contains JPEGs of one class. You can look the class label
up with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nltk.corpus&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;wordnet&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;wn&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;wn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_synset_from_pos_and_offset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'n'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1440764&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;wn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_synset_from_pos_and_offset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'n'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1443537&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;which reveals&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Synset('tench.n.01')&lt;/li&gt;
&lt;li&gt;Synset('goldfish.n.01')&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you extract all 1000 of those tar files into one directory, this takes about
6 hours with a script like this:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env python&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;glob&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tarfile&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;untar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fname&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;targetd_dir&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tarfile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fname&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;tar&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;tar&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extractall&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;targetd_dir&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;files&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;glob&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;glob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"ILSVRC2012_img_train/*.tar"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;files&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;untar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"extracted"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;p&gt;This gives 1281170 files in total.&lt;/p&gt;
&lt;h2 id="datasets"&gt;Datasets&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://image-net.org/download-images"&gt;Download ImageNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://places2.csail.mit.edu/download.html"&gt;Download Places365&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="download"></category><category term="machine learning"></category></entry><entry><title>ML Review 3</title><link href="https://martin-thoma.com/ml-review-3/" rel="alternate"></link><published>2017-02-25T20:00:00+01:00</published><updated>2017-02-25T20:00:00+01:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2017-02-25:/ml-review-3/</id><summary type="html">&lt;p&gt;This Review gives an overview of intersting stuff I stumbled over which are
related to machine learning.&lt;/p&gt;
&lt;h2 id="new-developments"&gt;New Developments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://developers.googleblog.com/2017/02/announcing-tensorflow-10.html"&gt;Tensorflow 1.0 is released&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="human-strenght-and-super-human-strength-programs"&gt;Human-strenght and Super-human strength programs&lt;/h3&gt;
&lt;p&gt;Super-human strength programs are programs, which surpass even the best human
(on the long run) in a specified task. Human-strength programs …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This Review gives an overview of intersting stuff I stumbled over which are
related to machine learning.&lt;/p&gt;
&lt;h2 id="new-developments"&gt;New Developments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://developers.googleblog.com/2017/02/announcing-tensorflow-10.html"&gt;Tensorflow 1.0 is released&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="human-strenght-and-super-human-strength-programs"&gt;Human-strenght and Super-human strength programs&lt;/h3&gt;
&lt;p&gt;Super-human strength programs are programs, which surpass even the best human
(on the long run) in a specified task. Human-strength programs behave similar
to an (untrained) human.&lt;/p&gt;
&lt;p&gt;Although those are not new, seeing them as a list (&lt;a href="http://blog.evjang.com/2017/01/nips2016.html"&gt;source&lt;/a&gt;)
was new to me. However, except for the games and lip reading, I doubt that we
are there yet. Interesting, non the less:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Games&lt;ul&gt;
&lt;li&gt;1995, Checkers: &lt;a href="https://en.wikipedia.org/wiki/Chinook_(draughts_player)"&gt;Chinook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;1996, Chess: &lt;a href="https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)"&gt;DeepBlue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2016, Go: &lt;a href="https://en.wikipedia.org/wiki/AlphaGo"&gt;AlphaGo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Lip reading: 2016, &lt;a href="https://arxiv.org/abs/1611.05358"&gt;Lip Reading Sentences in the Wild&lt;/a&gt; (&lt;a href="https://www.youtube.com/watch?v=5aogzAUPilE"&gt;YouTube&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Geolocation by photos, &lt;a href="https://arxiv.org/abs/1602.05314"&gt;PlaNet - Photo Geolocation with Convolutional Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Speech transcription: 2016, &lt;a href="https://arxiv.org/abs/1610.05256"&gt;Achieving Human Parity in Conversational Speech Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Translation: 2016, &lt;a href="https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html"&gt;Zero-Shot Translation with Google&amp;rsquo;s Multilingual Neural Machine Translation System&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Driving: 2016, &lt;a href="https://waymo.com/"&gt;Waymo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="live-demos-and-websites_1"&gt;Live Demos and Websites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://universe.openai.com/"&gt;universe.openai.com&lt;/a&gt;: Related to the OpenAI gym.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blogs.microsoft.com/next/2016/03/13/project-malmo-using-minecraft-build-intelligent-technology/"&gt;Project Malmo&lt;/a&gt;: Train RL agents in Minecraft&lt;/li&gt;
&lt;li&gt;&lt;a href="http://visiir.lip6.fr/"&gt;VISIIR&lt;/a&gt;: VIsual Seek for Interactive Image Retrieval - classifying food&lt;/li&gt;
&lt;li&gt;&lt;a href="http://affinelayer.com/pixsrv/index.html"&gt;Image-to-Image&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="publications"&gt;Publications&lt;/h2&gt;
&lt;!-- e.g. arXiv --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1611.09969"&gt;High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis&lt;/a&gt; and &lt;a href="https://github.com/leehomyc/High-Res-Neural-Inpainting"&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1702.05663.pdf"&gt;The Game Imitation: Deep Supervised Convolutional Networks for Quick Video&lt;/a&gt; and &lt;a href="https://www.youtube.com/watch?v=Pvesq6LEQxg&amp;amp;list=PLegUCwsQzmnUpPwVv8ygMa19zNnDgJ6OC&amp;amp;index=1"&gt;YouTube playlist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://openreview.net/pdf?id=rJv6ZgHYg"&gt;Deep Nets Don't Learn Via Memorization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="software"&gt;Software&lt;/h2&gt;
&lt;!-- e.g. Theano, Keras, ... --&gt;
&lt;h2 id="interesting-questions"&gt;Interesting Questions&lt;/h2&gt;
&lt;!-- For example StackExchange --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/42326466/why-is-the-accuracy-of-my-cnn-not-reproducible"&gt;Why is the accuracy of my CNN not reproducible?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.reddit.com/r/MLQuestions/comments/5s0jnc/how_much_does_a_gpu_instance_cost/"&gt;How much does a GPU instance cost?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="miscallenious"&gt;Miscallenious&lt;/h2&gt;
&lt;h3 id="trump-qa-idea"&gt;Trump QA idea&lt;/h3&gt;
&lt;p&gt;I was just watching &lt;a href="https://www.youtube.com/watch?v=CSx-N9ayCvU&amp;amp;feature=youtu.be&amp;amp;t=3m40s"&gt;this clip&lt;/a&gt; and wodered how well a question answering system would work which is trained on Trump speaches. Very often, when reporters / journalists / moderators ask Trump a question, he answers with "I am the [most / best / least] [positive / negative statement]. [Inconsistent answer follows]".
The answers themself would almost certainly be hilarous. Second, one could make an experiment and ask people if Trump actually answered a question like this.&lt;/p&gt;
&lt;h2 id="meetings_1"&gt;Meetings&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;London, 4. December 2016: &lt;a href="https://www.eventbrite.com/e/immigration-by-numbers-insights-through-data-visualisation-tickets-28920900191?aff=twitter"&gt;Data Visualization Challenge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Barcelona, 5. December 2016 - 10. December 2016: Neural Information Processing Systems (NIPS) (&lt;a href="https://nips.cc/"&gt;Link&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Mannheim, 7. April 2017: &lt;a href="https://hiwissml.github.io/datafest2017.github.io/"&gt;DataFest Germany&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="navigation clearfix"&gt;
&lt;div class="alignleft"&gt;
&lt;a href="https://martin-thoma.com/ml-review-2/" rel="prev"&gt;&amp;laquo; Previous Review&lt;/a&gt;
&lt;/div&gt;
&lt;div class="alignright"&gt;
&lt;a href="https://martin-thoma.com/ml-review-4/" rel="next"&gt;Next Review &amp;raquo;&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;</content><category term="Machine Learning"></category></entry><entry><title>Best of ML</title><link href="https://martin-thoma.com/best-of-ml/" rel="alternate"></link><published>2017-02-13T20:00:00+01:00</published><updated>2017-02-13T20:00:00+01:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2017-02-13:/best-of-ml/</id><summary type="html">&lt;p&gt;This post is a summary of articles, websites and material in general about
machine learning.&lt;/p&gt;
&lt;h2 id="articles"&gt;Articles&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;RNNs&lt;ul&gt;
&lt;li&gt;Get an overview: &lt;a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Understand them: &lt;a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;Understanding LSTM Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/"&gt;Using convolutional neural nets to detect facial keypoints tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://hunch.net/?p=22"&gt;Clever Methods of Overfitting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scott.fortmann-roe.com/docs/BiasVariance.html"&gt;Understanding the Bias-Variance Tradeoff&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sebastianruder.com/optimizing-gradient-descent/"&gt;An …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;This post is a summary of articles, websites and material in general about
machine learning.&lt;/p&gt;
&lt;h2 id="articles"&gt;Articles&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;RNNs&lt;ul&gt;
&lt;li&gt;Get an overview: &lt;a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Understand them: &lt;a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;Understanding LSTM Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/"&gt;Using convolutional neural nets to detect facial keypoints tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://hunch.net/?p=22"&gt;Clever Methods of Overfitting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scott.fortmann-roe.com/docs/BiasVariance.html"&gt;Understanding the Bias-Variance Tradeoff&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sebastianruder.com/optimizing-gradient-descent/"&gt;An overview of gradient descent optimization algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;cs231n: &lt;a href="http://cs231n.github.io/convolutional-networks/"&gt;Convolutional Neural Networks (CNNs / ConvNets)&lt;/a&gt; (&lt;a href="https://www.youtube.com/playlist?list=PL16j5WbGpaM0_Tj8CRmurZ8Kk1gEBc7fg"&gt;YouTube playlist&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="books"&gt;Books&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://neuralnetworksanddeeplearning.com/"&gt;Neural Networks and Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ian Goodfellow, Yoshua Bengio, and Aaron Courville: &lt;a href="http://www.deeplearningbook.org/"&gt;Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="moocs"&gt;MOOCs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Coursera: &lt;a href="https://www.coursera.org/learn/machine-learning"&gt;Machine Learning&lt;/a&gt; by Andrew Ng&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cs231n.stanford.edu/"&gt;CS224d: Deep Learning for Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/"&gt;Machine Learning&lt;/a&gt;: Kurs der Universit&amp;auml;t Oxford&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cs231n.stanford.edu/"&gt;Convolutional Neural Networks for Visual Recognition&lt;/a&gt;: Kurs von Stanford&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="tools"&gt;Tools&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://caffe.berkeleyvision.org/"&gt;Caffe&lt;/a&gt;: Used often for Computer Vision, but more and more people jump to TensorFlow&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/"&gt;sklearn&lt;/a&gt;: Python Machine learning toolkit&lt;/li&gt;
&lt;li&gt;&lt;a href="http://deeplearning.net/software/theano/"&gt;Theano&lt;/a&gt;: Used often for Speech Recognition&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Lasagne/Lasagne"&gt;Lasagne&lt;/a&gt;: Python, supports nVidia GPU training of neural networks&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/dnouri/nolearn"&gt;nolearn&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/"&gt;&lt;strong&gt;TensorFlow&lt;/strong&gt;&lt;/a&gt;: C++ and Python, supports nVidia GPU training of neural networks&lt;ul&gt;
&lt;li&gt;&lt;a href="http://keras.io/"&gt;&lt;strong&gt;Keras.io&lt;/strong&gt;&lt;/a&gt;: Extremely nice for beginners&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="data"&gt;Data&lt;/h2&gt;
&lt;p&gt;Collections&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.openml.org/"&gt;OpenML&lt;/a&gt;: A lot of datasets (it also has a Python package)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/datasets"&gt;Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Benchmark Datasets&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://yann.lecun.com/exdb/mnist/"&gt;MNIST&lt;/a&gt;: 70&amp;thinsp;000 images of &lt;span class="math"&gt;\(28 \times 28\)&lt;/span&gt; px with labels (digits 0-9)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1701.08380"&gt;HASY&lt;/a&gt;: 168&amp;thinsp;233 images of &lt;span class="math"&gt;\(32 \times 32\)&lt;/span&gt; px with labels (369 classes, all of them are characters)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.martin-thoma.de/write-math/data/"&gt;HWRT&lt;/a&gt;: Handwritten symbols (similar to HASY, but online data)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://archive.ics.uci.edu/ml/datasets/Iris"&gt;IRIS&lt;/a&gt;: 3 classes, 50 items per class, 3 features per item&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cvlibs.net/datasets/kitti/"&gt;KITTI&lt;/a&gt;: Road vision dataset&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lists:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.metacademy.org/"&gt;metacademy.org&lt;/a&gt;: A lot of material when you know what to look for&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.computervisiononline.com/datasets"&gt;computervisiononline.com&lt;/a&gt;: Eine Liste sehr vieler Datens&amp;auml;tze&lt;/li&gt;
&lt;li&gt;&lt;a href="http://riemenschneider.hayko.at/vision/dataset/"&gt;YACVID&lt;/a&gt;: Computer Vision Index To Datasets&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.dmoz.org/Computers/Artificial_Intelligence/Machine_Learning/Datasets/"&gt;dmoz.org&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="cheat-cheats"&gt;Cheat Cheats&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/tutorial/machine_learning_map/"&gt;Choosing the right estimator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://azure.microsoft.com/en-in/documentation/articles/machine-learning-algorithm-cheat-sheet/"&gt;Machine learning algorithm cheat sheet&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="lists"&gt;Lists&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/ujjwalkarn/Machine-Learning-Tutorials"&gt;Machine Learning Tutorials&lt;/a&gt; by Ujjwal Karn (Facebook employee)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jiwonkim.org/awesome-random-forest/"&gt;Awesome Random Forest&lt;/a&gt;: A
  curated list of resources regarding tree-based methods and more, including
  but not limited to random forest, bagging and boosting.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="miscallenious"&gt;Miscallenious&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/"&gt;Kaggle&lt;/a&gt;: Machine Learning Challenges&lt;/li&gt;
&lt;li&gt;Stack Exchange&lt;ul&gt;
&lt;li&gt;&lt;a href="http://datascience.stackexchange.com/"&gt;datascience.stackexchange.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stats.stackexchange.com/"&gt;stats.stackexchange.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/josephmisiti/awesome-machine-learning"&gt;awesome-machine-learning&lt;/a&gt;: A list with MANY links to machine learning tools&lt;/li&gt;
&lt;li&gt;Demos:&lt;ul&gt;
&lt;li&gt;&lt;a href="http://104.131.78.120/"&gt;Neural Machine Translation&lt;/a&gt;: English &amp;rarr; German, French&lt;/li&gt;
&lt;li&gt;&lt;a href="http://write-math.com"&gt;write-math.com&lt;/a&gt;: Symbol recognition&lt;/li&gt;
&lt;li&gt;&lt;a href="http://playground.tensorflow.org/"&gt;Tensorflow Playground&lt;/a&gt;: Demo for decision boundary of neural network&lt;/li&gt;
&lt;li&gt;&lt;a href="https://lecture-demo.ira.uka.de/"&gt;lecture-demo.ira.uka.de&lt;/a&gt;: Rosenblatt-Perceptron, GMMs, ...&lt;/li&gt;
&lt;li&gt;&lt;a href="http://demos.algorithmia.com/colorize-photos/"&gt;demos.algorithmia.com/colorize-photos&lt;/a&gt;: Colorize a grayscale photo&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="Machine Learning"></category></entry><entry><title>State of the Art in ML</title><link href="https://martin-thoma.com/sota/" rel="alternate"></link><published>2017-02-06T20:00:00+01:00</published><updated>2017-02-06T20:00:00+01:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2017-02-06:/sota/</id><summary type="html">&lt;p&gt;It is difficult to keep track of the current state of the art (SotA). Also, it
might not be directly clear which datasets are relevant. The following list
should help. If you think some datasets / problems / SotA results are missing,
let me know in the comments or via E-mail (&lt;code&gt;info …&lt;/code&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;It is difficult to keep track of the current state of the art (SotA). Also, it
might not be directly clear which datasets are relevant. The following list
should help. If you think some datasets / problems / SotA results are missing,
let me know in the comments or via E-mail (&lt;code&gt;info@martin-thoma.de&lt;/code&gt;).
I will update it.&lt;/p&gt;
&lt;p&gt;Papers and blog posts which summarize a topic or give a good introduction are
always welcome.&lt;/p&gt;
&lt;p&gt;In the following, a &lt;code&gt;+&lt;/code&gt; will indicate "higher is better" and a
&lt;code&gt;-&lt;/code&gt; will indicate "lower is better".&lt;/p&gt;
&lt;h2 id="computer-vision"&gt;Computer Vision&lt;/h2&gt;
&lt;h3 id="image-classification"&gt;Image Classification&lt;/h3&gt;
&lt;table class="table" id="image-classification-table"&gt;
&lt;tr&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Paper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.image-net.org/challenges/LSVRC/2012/nonpub-downloads"&gt;ImageNet 2012&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;td style="text-align: right;"&gt;3.57 %&lt;/td&gt;
&lt;td&gt;Top-5 error &lt;span title="lower is better"&gt;-&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1512.03385v1.pdf" title="Deep Residual Learning for Image Recognition"&gt;[HZRS15a]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://yann.lecun.com/exdb/mnist/"&gt;MNIST&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2013&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.21 %&lt;/td&gt;
&lt;td&gt;error &lt;span title="lower is better"&gt;-&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://www.matthewzeiler.com/pubs/icml2013/icml2013.pdf" title="Regularization of Neural Networks using DropConnect"&gt;[WZZ+13]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cs.toronto.edu/~kriz/cifar.html"&gt;CIFAR-10&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;td style="text-align: right;"&gt;2.72 %&lt;/td&gt;
&lt;td&gt;error &lt;span title="lower is better"&gt;-&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://openreview.net/forum?id=HkO-PCmYl" title="Shake-Shake regularization of 3-branch residual networks"&gt;[G17]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cs.toronto.edu/~kriz/cifar.html"&gt;CIFAR-100&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;td style="text-align: right;"&gt;17.18 %&lt;/td&gt;
&lt;td&gt;error &lt;span title="lower is better"&gt;-&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1608.06993v1" title="Densely Connected Convolutional Networks"&gt;[HLW16]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://cs.stanford.edu/~acoates/stl10/"&gt;STL-10&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;td style="text-align: right;"&gt;74.80 %&lt;/td&gt;
&lt;td&gt;accuracy &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1506.02351v1" title="Stacked What-Where Auto-encoders"&gt;[ZMGL15]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://ufldl.stanford.edu/housenumbers/"&gt;SVHN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;td style="text-align: right;"&gt;1.59 %&lt;/td&gt;
&lt;td&gt;error &lt;span title="lower is better"&gt;-&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1608.06993v1" title="Densely Connected Convolutional Networks"&gt;[HLW16]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/Caltech101.html"&gt;Caltech-101&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2014&lt;/td&gt;
&lt;td style="text-align: right;"&gt;86.5 %&lt;/td&gt;
&lt;td&gt;accuracy &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1311.2901" title="Visualizing and Understanding Convolutional Networks"&gt;[ZF14]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.vision.caltech.edu/Image_Datasets/Caltech256/"&gt;Caltech-256&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2014&lt;/td&gt;
&lt;td style="text-align: right;"&gt;74.2 %&lt;/td&gt;
&lt;td&gt;accuracy &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1311.2901" title="Visualizing and Understanding Convolutional Networks"&gt;[ZF14]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://zenodo.org/record/259444" title="The HASYv2 dataset"&gt;HASYv2&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;td style="text-align: right;"&gt;81.00 %&lt;/td&gt;
&lt;td&gt;accuracy &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1701.08380" title="The HASYv2 dataset"&gt;[Tho17]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://lear.inrialpes.fr/people/marszalek/data/ig02/"&gt;Graz-02&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2010&lt;/td&gt;
&lt;td style="text-align: right;"&gt;78.98 %&lt;/td&gt;
&lt;td&gt;accuracy &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://imagine.enpc.fr/publications/papers/ECCV2010b.pdf" title="Towards Optimal Naive Bayes Nearest Neighbor"&gt;[BMDP10]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://yfcc100m.appspot.com/"&gt;YFCC100m&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td style="text-align: right;"&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.vision.caltech.edu/visipedia/CUB-200-2011.html"&gt;CUB-200-2011&lt;/a&gt; Birds&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;td style="text-align: right;"&gt;84.1&lt;/td&gt;
&lt;td&gt;accuracy &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://vis-www.cs.umass.edu/bcnn/docs/bcnn_iccv15.pdf"&gt;[LRM15]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://mohammadmahoor.com/databases/denver-intensity-of-spontaneous-facial-action/"&gt;DISFA&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;td style="text-align: right;"&gt;48.5&lt;/td&gt;
&lt;td&gt;accuracy &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1702.02925" title="EAC-Net: A Region-based Deep Enhancing and Cropping Approach for Facial Action Unit Detection"&gt;[LAZY17]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BP4D&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td style="text-align: right;"&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1702.05373" title="EMNIST: an extension of MNIST to handwritten letters"&gt;EMNIST&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;td style="text-align: right;"&gt;50.93&lt;/td&gt;
&lt;td&gt;accuracy &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1702.05373" title="EMNIST: an extension of MNIST to handwritten letters"&gt;[CATS17]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://megaface.cs.washington.edu/"&gt;Megaface&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;td style="text-align: right;"&gt;74.6%&lt;/td&gt;
&lt;td&gt;accuracy &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;Google - &lt;a href="https://arxiv.org/abs/1503.03832"&gt;FaceNet&lt;/a&gt; v8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://benchmark.ini.rub.de/?section=gtsrb&amp;amp;subsection=news"&gt;GTSRB&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2012&lt;/td&gt;
&lt;td style="text-align: right;"&gt;99.46%&lt;/td&gt;
&lt;td&gt;accuracy &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://ieeexplore.ieee.org/document/6033589" title="Multi-column deep neural network for traffic sign classification"&gt;[CMMS12]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;State of the art in this category are CNN models which use skip connections
in the form of residual connections or dense connections.&lt;/p&gt;
&lt;p&gt;The evaluation metrics are straight-forward:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Accuracy&lt;/strong&gt;: Count how many elements of the test dataset you got right,
  divided by the total number of elements in the test dataset. The accuracy is
  in &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt;. Higher is better.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Error&lt;/strong&gt; = 1 - accuracy. The error is in &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt;. Lower is better.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Top-k accuracy&lt;/strong&gt;: Sometimes, there are either extremely similar classes or
  the application allows having multiple guesses. Hence not the Top-1 guess
  of the network has to be right, but the correct label has to be within the
  top &lt;span class="math"&gt;\(k\)&lt;/span&gt; guesses. The top-&lt;span class="math"&gt;\(k\)&lt;/span&gt; accuracy is in &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt;. Higher is better.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="detection-images"&gt;Detection (Images)&lt;/h3&gt;
&lt;p&gt;Face recognition is a special case of detection.&lt;/p&gt;
&lt;p&gt;Common metrics are:
&lt;ul&gt;
&lt;li&gt;mAP (Mean Average Precision): A detection is successfull, if the bounding
        box prediction and the true bounding box &lt;span class="math"&gt;\(\frac{intersection}{union}\)&lt;/span&gt; (IU, IoU)
        ratio is at least 0.5. Then the average precision = &lt;span class="math"&gt;\(\frac{TP}{TP + FP}\)&lt;/span&gt; is
        calculated for each class and the mean is calculated of those (see &lt;a href="http://stackoverflow.com/q/36274638/562769"&gt;Explanation&lt;/a&gt;, &lt;a href="http://datascience.stackexchange.com/q/16797/8820"&gt;What does the notation mAP@[.5:.95] mean?&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;MR (miss rate)&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Paper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html"&gt;PASCAL VOC 2012&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;td style="text-align: right;"&gt;75.9&lt;/td&gt;
&lt;td&gt;mAP@.5 &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1506.01497" title="Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"&gt;[RHGS15]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2011/index.html"&gt;PASCAL VOC 2011&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2014&lt;/td&gt;
&lt;td style="text-align: right;"&gt;62.7&lt;/td&gt;
&lt;td&gt;mean IU &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1411.4038" title="Fully Convolutional Networks for Semantic Segmentation"&gt;[LSD14]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2010/index.html"&gt;PASCAL VOC 2010&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2011&lt;/td&gt;
&lt;td style="text-align: right;"&gt;30.2&lt;/td&gt;
&lt;td&gt;mean accuracy &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://papers.nips.cc/paper/4296-efficient-inference-in-fully-connected-crfs-with-gaussian-edge-potentials.pdf" title="Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials"&gt;[Kol11]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2007/index.html"&gt;PASCAL VOC 2007&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;td style="text-align: right;"&gt;71.6&lt;/td&gt;
&lt;td&gt;mAP@.5 &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1512.02325" title="SSD: Single Shot MultiBox Detector"&gt;[LAES+15]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://mscoco.org/"&gt;MS COCO&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;td style="text-align: right;"&gt;46.5&lt;/td&gt;
&lt;td&gt;mAP@.5 &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1512.02325" title="SSD: Single Shot MultiBox Detector"&gt;[LAES+15]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1702.05693" title="CityPersons: A Diverse Dataset for Pedestrian Detection"&gt;CityPersons&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;td style="text-align: right;"&gt;33.10&lt;/td&gt;
&lt;td&gt;&lt;abbr title="log miss-rate"&gt;MR&lt;/abbr&gt; &lt;span title="lower is better"&gt;-&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1702.05693" title="CityPersons: A Diverse Dataset for Pedestrian Detection"&gt;[ZBS17]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id="detection-videos"&gt;Detection (Videos)&lt;/h3&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Paper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://research.google.com/youtube-bb/"&gt;YouTube-BoundingBoxes&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id="person-re-identitification"&gt;Person Re-Identitification&lt;/h3&gt;
&lt;p&gt;Person Re-ID is the task of identifying a person again which was already seen
in a video stream. Person following and &lt;abbr title="Multi Target Multi
Camera"&gt;MTMCT&lt;/abbr&gt; seems to be very similar if not identical.&lt;/p&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Paper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.liangzheng.org/Project/project_reid.html"&gt;Market-1501&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;td&gt;62.1&lt;/td&gt;
&lt;td&gt;mAP &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1703.05693"&gt;[SZDW17]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.ee.cuhk.edu.hk/~xgwang/CUHK_identification.html"&gt;CUHK03&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;td&gt;84.8&lt;/td&gt;
&lt;td&gt;mAP &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1703.05693"&gt;[SZDW17]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://vision.cs.duke.edu/DukeMTMC/"&gt;DukeMTMC&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;td&gt;56.8&lt;/td&gt;
&lt;td&gt;mAP &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1703.05693"&gt;[SZDW17]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id="semantic-segmentation"&gt;Semantic Segmentation&lt;/h3&gt;
&lt;p&gt;A summary of classical methods for semantic segmentation, more information
to several datasets and metrics for evaluation can be found in &lt;a href="https://arxiv.org/abs/1602.06541"&gt;A Survey of Semantic Segmentation&lt;/a&gt;.&lt;/p&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Paper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.microsoft.com/en-us/research/project/image-understanding/"&gt;MSRC-21&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2011&lt;/td&gt;
&lt;td style="text-align: right;"&gt;84.7&lt;/td&gt;
&lt;td&gt;mean accuracy &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://papers.nips.cc/paper/4296-efficient-inference-in-fully-connected-crfs-with-gaussian-edge-potentials.pdf" title="Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials"&gt;[Kol11]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.cvlibs.net/datasets/kitti/eval_road.php"&gt;KITTI Road&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td style="text-align: right;"&gt;96.69&lt;/td&gt;
&lt;td&gt;Max F1 &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html" title="NYU Depth Dataset V2"&gt;NYUDv2&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2014&lt;/td&gt;
&lt;td style="text-align: right;"&gt;34.0&lt;/td&gt;
&lt;td&gt;mean IO &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://papers.nips.cc/paper/4296-efficient-inference-in-fully-connected-crfs-with-gaussian-edge-potentials.pdf" title="Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials"&gt;[Kol11]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://people.csail.mit.edu/celiu/SIFTflow/"&gt;SIFT Flow&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2014&lt;/td&gt;
&lt;td style="text-align: right;"&gt;39.5&lt;/td&gt;
&lt;td&gt;mean IU &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1411.4038" title="Fully Convolutional Networks for Semantic Segmentation"&gt;[LSD14]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.it.lut.fi/project/imageret/diaretdb1/"&gt;DIARETDB1&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www2.warwick.ac.uk/fac/sci/dcs/research/combi/research/bic/glascontest/download/"&gt;Warwick-QU&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://dataverse.scholarsportal.info/dataset.xhtml?persistentId=doi:10.5683/SP/NTUOK9"&gt;Ciona17&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;td&gt;51.36 %&lt;/td&gt;
&lt;td&gt;mean IoU &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1702.05564.pdf" title="The Ciona17 Dataset for Semantic Segmentation of Invasive Species in a Marine Aquaculture Environment"&gt;[GTRM17]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id="instance-segmentation"&gt;Instance Segmentation&lt;/h3&gt;
&lt;p&gt;See &lt;a href="https://arxiv.org/abs/1512.04412" title="Instance-aware Semantic Segmentation via Multi-task Network Cascades"&gt;[DHS15]&lt;/a&gt;&lt;/p&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Paper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cityscapes-dataset.com/benchmarks/"&gt;CityScapes&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id="action-recognition"&gt;Action Recognition&lt;/h3&gt;
&lt;p&gt;Action recognition is a classification problem over a short video clip.&lt;/p&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Paper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://research.google.com/youtube8m/"&gt;YouTube-8M&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/gtoderici/sports-1m-dataset/blob/wiki/ProjectHome.md"&gt;Sports-1M&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;td&gt;68.7 %&lt;/td&gt;
&lt;td&gt;Clip Hit@1 accuracy &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ng_Beyond_Short_Snippets_2015_CVPR_paper.pdf" title="Beyond Short Snippets: Deep Networks for Video Classification"&gt;[NHV+15]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UCF-101&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;td&gt;70.8 %&lt;/td&gt;
&lt;td&gt;Clip Hit@1 accuracy &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ng_Beyond_Short_Snippets_2015_CVPR_paper.pdf" title="Beyond Short Snippets: Deep Networks for Video Classification"&gt;[NHV+15]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.nada.kth.se/cvap/actions/"&gt;KTH&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;td style="text-align: right;"&gt;95.6 %&lt;/td&gt;
&lt;td&gt;EER &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1512.03980.pdf" title="Action Recognition with Image Based CNN Features"&gt;[RMRMD15]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://crcv.ucf.edu/data/UCF_Sports_Action.php"&gt;UCF Sport&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;td style="text-align: right;"&gt;97.8 %&lt;/td&gt;
&lt;td&gt;EER &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1512.03980.pdf" title="Action Recognition with Image Based CNN Features"&gt;[RMRMD15]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://crcv.ucf.edu/data/UCF_YouTube_Action.php"&gt;UCF-11 Human Action&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;td style="text-align: right;"&gt;89.5 %&lt;/td&gt;
&lt;td&gt;EER &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1512.03980.pdf" title="Action Recognition with Image Based CNN Features"&gt;[RMRMD15]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id="super-resolution"&gt;Super Resolution&lt;/h3&gt;
&lt;p&gt;See &lt;a href="https://github.com/huangzehao/Super-Resolution.Benckmark"&gt;github.com/huangzehao&lt;/a&gt;&lt;/p&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Paper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;I'm not sure how super resolution is benchmarked. One way to do it would be
to get high resolution images, scale them down, feed them to the network and
measure the mean squared error for each pixel:&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{1}{|I|} \sum_{t \in I} {(t - \hat{t})}^2$$&lt;/div&gt;
&lt;p&gt;However, this might be sensitive to the way the images were downsampled.&lt;/p&gt;
&lt;h3 id="lip-reading"&gt;Lip Reading&lt;/h3&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Paper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://spandh.dcs.shef.ac.uk/gridcorpus/" title="The GRID audiovisual sentence corpus

"&gt;GRID&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;td&gt;95.2 %&lt;/td&gt;
&lt;td&gt;accuracy &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://openreview.net/forum?id=BkjLkSqxg" title="LipNet: End-to-End Sentence-level Lipreading"&gt;[ASWF16]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id="other-datasets"&gt;Other Datasets&lt;/h3&gt;
&lt;p&gt;For the following datasets, I was not able to find where to download them&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mapping global urban areas using MODIS 500-m data: New methods and datasets
  based on urban ecoregions&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1612.00423"&gt;TorontoCity: Seeing the World with a Million Eyes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="asr_1"&gt;ASR&lt;/h2&gt;
&lt;p&gt;Automatic Speech Recognition (ASR).&lt;/p&gt;
&lt;h3 id="sentence-level"&gt;Sentence-Level&lt;/h3&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Paper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="" title="Wall Street Journal"&gt;WSJ&lt;/a&gt; (eval92)&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;td&gt;3.47&lt;/td&gt;
&lt;td&gt;WER &lt;span title="lower is better"&gt;-&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1504.01482v1.pdf" title="Deep Recurrent Neural Networks for Acoustic Modelling"&gt;[CL15]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Switchboard Hub5'00&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;td&gt;6.3%&lt;/td&gt;
&lt;td&gt;WER &lt;span title="lower is better"&gt;-&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1609.03528v1.pdf" title="The Microsoft 2016 conversational speech recognition system"&gt;[XDSS+16]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;See &lt;a href="https://martin-thoma.com/word-error-rate-calculation/"&gt;Word Error Rate&lt;/a&gt; (WER)
for an explanation of the metric.&lt;/p&gt;
&lt;p&gt;Relevant papers might be&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1512.02595v1"&gt;Deep Speech 2: End-to-End Speech Recognition in English and Mandarin&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="phoneme-level"&gt;Phoneme-Level&lt;/h3&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Paper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://catalog.ldc.upenn.edu/ldc93s1" title="TIMIT Acoustic-Phonetic Continuous Speech Corpus"&gt;TIMIT&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2013&lt;/td&gt;
&lt;td&gt;17.7 %&lt;/td&gt;
&lt;td&gt;error rate &lt;span title="lower is better"&gt;-&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://ieeexplore.ieee.org/abstract/document/6638947/" title="Speech recognition with deep recurrent neural networks"&gt;[GMH13]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h2 id="language_1"&gt;Language&lt;/h2&gt;
&lt;p&gt;Natural Language Processing (NLP) deals with how to represent language. It is
related and often a part of ASR.&lt;/p&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Paper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://metamind.io/research/the-wikitext-long-term-dependency-language-modeling-dataset/"&gt;WikiText-103&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;td style="text-align: right;"&gt;48.7&lt;/td&gt;
&lt;td&gt;Perplexity &lt;span title="lower is better"&gt;-&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://openreview.net/pdf?id=B184E5qee" title="Improving Neural Language Models with a Continuous Cache"&gt;[GJU16]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Penn Treebank (PTB)&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;td style="text-align: right;"&gt;62.4&lt;/td&gt;
&lt;td&gt;Perplexity &lt;span title="lower is better"&gt;-&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1611.01578" title="Neural Architecture Search with Reinforcement Learning"&gt;[ZL16]&lt;/a&gt; (&lt;a href="http://www.shortscience.org/paper?bibtexKey=journals/corr/1611.01578#martinthoma"&gt;summary&lt;/a&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.reddit.com/r/MachineLearning/comments/5s6ixw/d_what_are_the_current_benchmark_datasets_and_the/ddcwoay/"&gt;Stanford Sentiment Treebank&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;NLP benchmarks use &lt;a href="https://en.wikipedia.org/wiki/Perplexity"&gt;perplexity&lt;/a&gt; to
measure how good a result is.&lt;/p&gt;
&lt;h2 id="translation"&gt;Translation&lt;/h2&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Paper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MT03&lt;/td&gt;
&lt;td&gt;2003&lt;/td&gt;
&lt;td&gt;35.76&lt;/td&gt;
&lt;td&gt;BLEU&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.cs.sfu.ca/~anoop/papers/pdf/jhu-ws03-report.pdf" title="Syntax for Statistical Machine Translation"&gt;[OGKS+03]&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/BLEU" title="bilingual evaluation understudy"&gt;BLEU&lt;/a&gt;
score is used to measure how good a translation system is.&lt;/p&gt;
&lt;p&gt;Another score is the &lt;em&gt;Translation Edit Rate&lt;/em&gt; (TER) introduced by
&lt;a href="http://mt-archive.info/AMTA-2006-Snover.pdf"&gt;Snover et al., 2006&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="matrix-completion"&gt;Matrix completion&lt;/h2&gt;
&lt;p&gt;Collaborative filtering is an application of matrix completion.
More datasets are on &lt;a href="https://gist.github.com/entaroadun/1653794"&gt;entaroadun/gist:1653794&lt;/a&gt;.&lt;/p&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Paper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://grouplens.org/datasets/movielens/"&gt;MovieLens&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://eigentaste.berkeley.edu/dataset/"&gt;Jester&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h2 id="reinforcment-learning"&gt;Reinforcment Learning&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://gym.openai.com/"&gt;OpenAI Gym&lt;/a&gt; offers many environments
for testing RL algorithms.&lt;/p&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;th&gt;Challenge&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Paper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Chess&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;3395&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://stockfishchess.org/"&gt;Stockfishchess&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Go&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;td&gt;3,168&lt;/td&gt;
&lt;td&gt;ELO &lt;span title="higher is better"&gt;+&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html" title="Mastering the game of Go with deep neural networks and tree search"&gt;AlphaGo&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Star Craft&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h2 id="control"&gt;Control&lt;/h2&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Paper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="CartPole-v0"&gt;Cart Pole&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h2 id="see-also"&gt;See also&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://rodrigob.github.io/are_we_there_yet"&gt;Are we there yet ?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.reddit.com/r/MachineLearning/comments/4dkrw1/some_stateofthearts_in_natural_language/"&gt;Some state-of-the-arts in natural language processing and their discussion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;aclweb.org: &lt;a href="https://www.aclweb.org/aclwiki/index.php?title=State_of_the_art"&gt;State of the art&lt;/a&gt; - NLP tasks&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/syhw/wer_are_we/tree/master"&gt;wer_are_we&lt;/a&gt;: SotA in ASR&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/michalwols/ml-sota"&gt;github.com/michalwols/ml-sota&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Machine Learning"></category><category term="Datasets"></category></entry><entry><title>skdata</title><link href="https://martin-thoma.com/skdata/" rel="alternate"></link><published>2017-01-30T20:00:00+01:00</published><updated>2017-01-30T20:00:00+01:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2017-01-30:/skdata/</id><summary type="html">&lt;p&gt;I really like Machine Learning. I like reading papers, understanding and
evaluating new ideas. But one part I always have to spend quite a bit of time
on is loading the data. It's always a mess to find the datasets, understand
where exactly I can download them and how they've …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I really like Machine Learning. I like reading papers, understanding and
evaluating new ideas. But one part I always have to spend quite a bit of time
on is loading the data. It's always a mess to find the datasets, understand
where exactly I can download them and how they've packaged the information.
Just a few days ago I found &lt;code&gt;skdata&lt;/code&gt;. It is a Python package which aims at
helping to load standard datasets. If I can trust the git commit message, then
the development was started in August 2011 by James Bergstra! This is before
AlexNet!&lt;/p&gt;
&lt;p&gt;edit: Although it seemed to be a cool project, it seems to be dead, too. The
last commit is from July 2015.&lt;/p&gt;
&lt;h2 id="usage"&gt;Usage&lt;/h2&gt;
&lt;p&gt;One way to use &lt;code&gt;skdata&lt;/code&gt; is the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env python&lt;/span&gt;

&lt;span class="sd"&gt;"""MNIST example with skdata."""&lt;/span&gt;

&lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;skdata.mnist.view&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;OfficialVectorClassification&lt;/span&gt;
&lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;ImportError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;# Fallback, if you have an old version&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;skdata.mnist.views&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;OfficialVectorClassification&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.tree&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;

&lt;span class="c1"&gt;# Load the data&lt;/span&gt;
&lt;span class="n"&gt;view&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;OfficialVectorClassification&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;train_idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_idxs&lt;/span&gt;  &lt;span class="c1"&gt;# indices of training data&lt;/span&gt;
&lt;span class="n"&gt;val_idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;val_idxs&lt;/span&gt;  &lt;span class="c1"&gt;# incices of validation data&lt;/span&gt;
&lt;span class="n"&gt;test_idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tst_idxs&lt;/span&gt;  &lt;span class="c1"&gt;# indices of test data&lt;/span&gt;

&lt;span class="c1"&gt;# Fit a simple classifier&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Start fitting DecisionTreeClassifier."&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_vectors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_idx&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  &lt;span class="c1"&gt;# select features of training data&lt;/span&gt;
&lt;span class="n"&gt;targets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_idx&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  &lt;span class="c1"&gt;# select labels of training data&lt;/span&gt;
&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Evaluate the classifier&lt;/span&gt;
&lt;span class="n"&gt;predict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_vectors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;test_idx&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;test_idx&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_idx&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Fitted DecisionTreeClassifier has test accuracy of &lt;/span&gt;&lt;span class="si"&gt;%0.4f&lt;/span&gt;&lt;span class="s2"&gt;."&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;However, it is inteded to be used like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;skdata.mnist.view&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;OfficialVectorClassification&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.tree&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;

&lt;span class="c1"&gt;# Load the data&lt;/span&gt;
&lt;span class="n"&gt;mnist_view&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;OfficialVectorClassification&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;train_idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist_view&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_idxs&lt;/span&gt;
&lt;span class="n"&gt;val_idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist_view&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;val_idxs&lt;/span&gt;
&lt;span class="n"&gt;test_idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist_view&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tst_idxs&lt;/span&gt;

&lt;span class="c1"&gt;# Fit a simple classifier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;skdata.base&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SklearnClassifier&lt;/span&gt;
&lt;span class="n"&gt;learning_algo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SklearnClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;mnist_view&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;protocol&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_algo&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learn_algo&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'loss'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;'task_name'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;'tst'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;... but this doesn't work (for me)&lt;/p&gt;
&lt;h2 id="other-data-loading-projects"&gt;Other Data-Loading Projects&lt;/h2&gt;
&lt;p&gt;You can access R data with &lt;a href="https://rpy2.bitbucket.io/?"&gt;&lt;code&gt;rpy2&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;rpy2.robjects&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;rpy2.robjects&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pandas2ri&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;pandas2ri&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ri2py&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'iris'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;describe&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.quandl.com/tools/python"&gt;quandl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mila-udem/fuel"&gt;fuel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can also load &lt;code&gt;mnist&lt;/code&gt;, &lt;code&gt;cifar10&lt;/code&gt;,&lt;code&gt;cifar100&lt;/code&gt;, &lt;code&gt;imdb&lt;/code&gt;, &lt;code&gt;reuters&lt;/code&gt; with keras:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_data&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="see-also"&gt;See also&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://jaberg.github.io/skdata/"&gt;skdata documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jaberg/skdata"&gt;skdata on Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jaberg/skdata/wiki/How-to-Create-a-New-Dataset-Module"&gt;How to Create a New Dataset Module&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/jaberg/skdata/wiki/Protocol"&gt;Protocol&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jaberg/skdata/wiki/Data-Set-Modules"&gt;List of datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://fuel.readthedocs.io/en/latest/built_in_datasets.html"&gt;fuel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://martin-thoma.com/download-data/"&gt;ImageNet&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Machine Learning"></category><category term="Python"></category><category term="dataset"></category></entry><entry><title>Label Correction Algorithm</title><link href="https://martin-thoma.com/label-correction-algorithm/" rel="alternate"></link><published>2017-01-25T20:00:00+01:00</published><updated>2017-01-25T20:00:00+01:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2017-01-25:/label-correction-algorithm/</id><summary type="html">&lt;p&gt;The label-correction algorithm is a generalization which includes very common
graph search algorithms like breadth first search (BFS), depth first search (DFS),
&lt;a href="https://en.wikipedia.org/wiki/A*_search_algorithm"&gt;A*&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm"&gt;Dijkstra's algorithm&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Branch_and_bound"&gt;Branch and bound&lt;/a&gt; as special cases.&lt;/p&gt;
&lt;h2 id="pseudocode"&gt;Pseudocode&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter img-thumbnail"&gt;
&lt;img alt="Pseudocode for the Label correction algorithm" src="../images/2016/07/label-correction.png"/&gt;
&lt;figcaption class="text-center"&gt;Pseudocode for the Label correction algorithm&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Explanation:&lt;/p&gt;
&lt;p&gt;First &lt;code&gt;if&lt;/code&gt;: The left hand side is a lower bound …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The label-correction algorithm is a generalization which includes very common
graph search algorithms like breadth first search (BFS), depth first search (DFS),
&lt;a href="https://en.wikipedia.org/wiki/A*_search_algorithm"&gt;A*&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm"&gt;Dijkstra's algorithm&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Branch_and_bound"&gt;Branch and bound&lt;/a&gt; as special cases.&lt;/p&gt;
&lt;h2 id="pseudocode"&gt;Pseudocode&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter img-thumbnail"&gt;
&lt;img alt="Pseudocode for the Label correction algorithm" src="../images/2016/07/label-correction.png"/&gt;
&lt;figcaption class="text-center"&gt;Pseudocode for the Label correction algorithm&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Explanation:&lt;/p&gt;
&lt;p&gt;First &lt;code&gt;if&lt;/code&gt;: The left hand side is a lower bound to get from start to
&lt;code&gt;v&lt;/code&gt;, to &lt;code&gt;c&lt;/code&gt; and then to &lt;code&gt;t&lt;/code&gt;. If this lower bound is not lower than
either &lt;code&gt;u&lt;/code&gt; or the distance to &lt;code&gt;c&lt;/code&gt; directly, then it will not be part
of the optimal solution.&lt;/p&gt;
&lt;p&gt;Special cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Depth-first_search"&gt;Depth-first search&lt;/a&gt;: K is LIFO list / Stack&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Breadth-first_search"&gt;Breadth-first search&lt;/a&gt;: K is FIFO list&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm"&gt;Dijkstra's algorithm&lt;/a&gt;: K is &lt;a href="https://en.wikipedia.org/wiki/Priority_queue"&gt;priority queue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/A*_search_algorithm"&gt;A*&lt;/a&gt;: K ist priority queue, &lt;span class="math"&gt;\(h_j\)&lt;/span&gt; is non-trivial&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Branch_and_bound"&gt;Branch and bound&lt;/a&gt;: K ist priority queue, &lt;span class="math"&gt;\(h_j\)&lt;/span&gt; and &lt;span class="math"&gt;\(m_j\)&lt;/span&gt; are non-trivial&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="python"&gt;Python&lt;/h2&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
 28
 29
 30
 31
 32
 33
 34
 35
 36
 37
 38
 39
 40
 41
 42
 43
 44
 45
 46
 47
 48
 49
 50
 51
 52
 53
 54
 55
 56
 57
 58
 59
 60
 61
 62
 63
 64
 65
 66
 67
 68
 69
 70
 71
 72
 73
 74
 75
 76
 77
 78
 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env python&lt;/span&gt;

&lt;span class="sd"&gt;"""Label Correction algorithm."""&lt;/span&gt;


&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;logging&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;

&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;basicConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="si"&gt;%(asctime)s&lt;/span&gt;&lt;span class="s1"&gt; &lt;/span&gt;&lt;span class="si"&gt;%(levelname)s&lt;/span&gt;&lt;span class="s1"&gt; &lt;/span&gt;&lt;span class="si"&gt;%(message)s&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="n"&gt;level&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DEBUG&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stdout&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;LIFO&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""A LIFO storage."""&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;insert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;el&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;el&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""An undirected graph."""&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nodes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name2index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index2name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;neighbors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;add_node&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;"""Add a new node and return its index."""&lt;/span&gt;
        &lt;span class="n"&gt;node_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nodes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;startswith&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'index-'&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;warning&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Node names beginning with "index-" may cause '&lt;/span&gt;
                            &lt;span class="s1"&gt;'problems.'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"index-&lt;/span&gt;&lt;span class="si"&gt;%i&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;node_index&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nodes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;node_index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name2index&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;node_index&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index2name&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;node_index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;

        &lt;span class="c1"&gt;# Add weight from new node to other nodes and vice-versa&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([])&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n1&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nodes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;node_index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'inf'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n1&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;node_index&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'inf'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="c1"&gt;# From the node to itself has distance 0&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;node_index&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;node_index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;neighbors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([])&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;node_index&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_node_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;"""Get node index by name."""&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name2index&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;set_edge_by_name&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;        Set edge weight by node names.&lt;/span&gt;

&lt;span class="sd"&gt;        Parameters&lt;/span&gt;
&lt;span class="sd"&gt;        ----------&lt;/span&gt;
&lt;span class="sd"&gt;        a : str&lt;/span&gt;
&lt;span class="sd"&gt;            First edge name&lt;/span&gt;
&lt;span class="sd"&gt;        b : str&lt;/span&gt;
&lt;span class="sd"&gt;            Second edge name&lt;/span&gt;
&lt;span class="sd"&gt;        weight : number&lt;/span&gt;
&lt;span class="sd"&gt;            New edge weight&lt;/span&gt;
&lt;span class="sd"&gt;        """&lt;/span&gt;
        &lt;span class="n"&gt;i1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_node_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;i2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_node_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i2&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;neighbors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;neighbors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;label_correction&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                     &lt;span class="n"&gt;start_node&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                     &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                     &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    Label correction algorithm for graph searches.&lt;/span&gt;

&lt;span class="sd"&gt;    Parameters&lt;/span&gt;
&lt;span class="sd"&gt;    ----------&lt;/span&gt;
&lt;span class="sd"&gt;    graph :&lt;/span&gt;
&lt;span class="sd"&gt;        Needs 'graph.childs' which returns a list of child indices for each&lt;/span&gt;
&lt;span class="sd"&gt;        node, 'graph.edges[node1][node2]' which always returns an edge weight,&lt;/span&gt;
&lt;span class="sd"&gt;    start_node : int&lt;/span&gt;
&lt;span class="sd"&gt;        Index of start node as given by the graph node iterator&lt;/span&gt;
&lt;span class="sd"&gt;    t : int&lt;/span&gt;
&lt;span class="sd"&gt;        Index of target node as given by the graph node iterator&lt;/span&gt;
&lt;span class="sd"&gt;    h : lower_heuristic, optional&lt;/span&gt;
&lt;span class="sd"&gt;        Takes (graph, node1, node2) and returns a number which underestimates&lt;/span&gt;
&lt;span class="sd"&gt;        the distance from node1 to node2. If this is not given, the trivial&lt;/span&gt;
&lt;span class="sd"&gt;        distance 0 is chosen.&lt;/span&gt;
&lt;span class="sd"&gt;    m : upper_heuristic, optional&lt;/span&gt;
&lt;span class="sd"&gt;    K : list-like data structure, optional&lt;/span&gt;
&lt;span class="sd"&gt;        Needs 'insert', 'pop'&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'inf'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;K&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LIFO&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;parents&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;node&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nodes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'inf'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;parents&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;start_node&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'inf'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# shortest distance from start_node to t&lt;/span&gt;
    &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;start_node&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"K=&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;neighbors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="n"&gt;parents&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;insert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="c1"&gt;# Reconstruct the path&lt;/span&gt;
    &lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;named_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;current&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;current&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;start_node&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;named_path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index2name&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;current&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;current&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;parents&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;current&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;named_path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index2name&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;current&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'shortest_distance'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s1"&gt;'path'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;[::&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="s1"&gt;'named_path'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;named_path&lt;/span&gt;&lt;span class="p"&gt;[::&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]}&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample_1&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;"""A simple search problem."""&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_node&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;chr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;ord&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'A'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_edge_by_name&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'A'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'B'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_edge_by_name&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'A'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'C'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_edge_by_name&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'B'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'D'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_edge_by_name&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'B'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'E'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_edge_by_name&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'C'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'F'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_edge_by_name&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'C'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'G'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_edge_by_name&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'D'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'H'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_edge_by_name&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'D'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'I'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_edge_by_name&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'E'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'J'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_edge_by_name&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'G'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'K'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_edge_by_name&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'H'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'L'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_edge_by_name&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'J'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'M'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;i1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_node_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'A'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;i2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_node_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'F'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ret&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;label_correction&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ret&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;'__main__'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;sample_1&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;h2 id="examples"&gt;Examples&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://projecteuler.net/problem=18"&gt;Project Euler 18&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.hackerrank.com/challenges/pacman-dfs"&gt;hackerrank&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="see-also"&gt;See also&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/MartinThoma/algorithms/tree/master/label-korrektur-algorithmus"&gt;My implementations on GitHub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.idiotinside.com/2015/03/01/python-lists-as-fifo-lifo-queues-using-deque-collections/"&gt;Python Lists as Fifo, Lifo Queues Using Deque Collections&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="A.I."></category><category term="Algorithms"></category><category term="Programming"></category><category term="Python"></category><category term="Machine Learning"></category></entry><entry><title>Reinforcement Learning</title><link href="https://martin-thoma.com/reinforcement-learning/" rel="alternate"></link><published>2016-12-29T20:00:00+01:00</published><updated>2016-12-29T20:00:00+01:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2016-12-29:/reinforcement-learning/</id><summary type="html">&lt;p&gt;Reinforcement learning is a sub-field of mathematics and computer science. It
deals with the following kind of problems: You're given a set of states
&lt;span class="math"&gt;\(\mathcal{X} \subseteq \mathbb{R}^n\)&lt;/span&gt; and a starting state &lt;span class="math"&gt;\(x_0 \in \mathcal{X}\)&lt;/span&gt;.
For every time step &lt;span class="math"&gt;\(k = 0, 1, 2, \dots\)&lt;/span&gt; you have a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Reinforcement learning is a sub-field of mathematics and computer science. It
deals with the following kind of problems: You're given a set of states
&lt;span class="math"&gt;\(\mathcal{X} \subseteq \mathbb{R}^n\)&lt;/span&gt; and a starting state &lt;span class="math"&gt;\(x_0 \in \mathcal{X}\)&lt;/span&gt;.
For every time step &lt;span class="math"&gt;\(k = 0, 1, 2, \dots\)&lt;/span&gt; you have a set of possible actions,
depending on your current state:
&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{A}_k(x_k)$$&lt;/div&gt;
&lt;p&gt;
Depending on what your action and your current state is, the new state is
&lt;/p&gt;
&lt;div class="math"&gt;$$P(x_k, a_k, x_{k+1}) \in [0, 1]$$&lt;/div&gt;
&lt;p&gt;
So the transition from state &lt;span class="math"&gt;\(x_k\)&lt;/span&gt; to state &lt;span class="math"&gt;\(x_{k+1}\)&lt;/span&gt; with action &lt;span class="math"&gt;\(a_k\)&lt;/span&gt; is
stochastic.&lt;/p&gt;
&lt;p&gt;For some states &lt;span class="math"&gt;\(x_k\)&lt;/span&gt;, actions &lt;span class="math"&gt;\(a_k\)&lt;/span&gt; at time &lt;span class="math"&gt;\(k\)&lt;/span&gt;, you receive rewards:&lt;/p&gt;
&lt;div class="math"&gt;$$r_k(x_k, a_k) \in \mathbb{R}$$&lt;/div&gt;
&lt;p&gt;Your goal is to maximize&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{E}(\sum_{k=0}^\infty \gamma^k \cdot r_k(x_k, a_k))$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\gamma in (0, 1)\)&lt;/span&gt; is a discounting factor which makes sure we don't get
infinite rewards. &lt;span class="math"&gt;\(\gamma = 0.99\)&lt;/span&gt; is a typical choice.&lt;/p&gt;
&lt;h2 id="applications"&gt;Applications&lt;/h2&gt;
&lt;p&gt;This very general problem description can be applied in almost any scenario:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learning automatically to play games&lt;/li&gt;
&lt;li&gt;Learning to control robots&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="why-rl-is-difficult"&gt;Why RL is difficult&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Credit assignment: In chess, you only get a reward (positiv or negative) at
  the end of the game. How to you tell which move was good or bad?&lt;/li&gt;
&lt;li&gt;&lt;a href="https://martin-thoma.com/probabilistische-planung/#exporation-exploitation"&gt;Exploration vs. exploitation&lt;/a&gt;:
  When should you stick to what you know and when should you try something new?&lt;/li&gt;
&lt;li&gt;State equivalence: Typically, your state is very high-dimensional. For example
  when learning very old computer games from raw pixels you have
  &lt;div class="math"&gt;$$210 \cdot 160 \cdot 3 = 100800$$&lt;/div&gt;
  dimensions in your feature vector. But the relevant game states might be
  much less.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="approaches"&gt;Approaches&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;policy network&lt;/strong&gt; gets the state as input and outputs the action. It learns
by executing many episodes (e.g. a complete game; from start until you reach a
final state or at least a state with reward) and labels all actions before with
the received reward. There might be many which were good even in a lost game,
but in average you expect to punish bad decisions and encourage good decisions.&lt;/p&gt;
&lt;h2 id="resources"&gt;Resources&lt;/h2&gt;
&lt;p&gt;If you are a student at KIT, I can recommend to visit the lecture
&lt;a href="https://martin-thoma.com/probabilistische-planung/"&gt;Probabilistic Planning&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Other resources you might want to have a look at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://karpathy.github.io/2016/05/31/rl/"&gt;Deep Reinforcement Learning: Pong from Pixels&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5"&gt;Pong example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/"&gt;Guest Post (Part I): Demystifying Deep Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.nature.com/nature/journal/v518/n7540/pdf/nature14236.pdf"&gt;Human-level control through deep reinforcement learning&lt;/a&gt; by V. Mnih et al.&lt;ul&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1312.5602"&gt;Playing Atari with Deep Reinforcement Learning&lt;/a&gt; on arXiv&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jmlr.org/proceedings/papers/v32/silver14.pdf"&gt;Deterministic Policy Gradient Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Machine Learning"></category><category term="RL"></category></entry><entry><title>ML Review 2</title><link href="https://martin-thoma.com/ml-review-2/" rel="alternate"></link><published>2016-12-27T11:00:00+01:00</published><updated>2016-12-27T11:00:00+01:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2016-12-27:/ml-review-2/</id><summary type="html">&lt;p&gt;This Review gives an overview of intersting stuff I stumbled over which are
related to machine learning. Most of it was posted in &lt;a href="https://ml-ka.de/"&gt;KITs machine learning group&lt;/a&gt; (on Facebook).&lt;/p&gt;
&lt;p&gt;A lot of stuff can be found in my article about &lt;a href="https://martin-thoma.com/nips-2016/"&gt;NIPS 2016&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="new-developments"&gt;New Developments&lt;/h2&gt;
&lt;!-- Trends --&gt;
&lt;ul&gt;
&lt;li&gt;Random forests for courier detection: &lt;a href="https://www.theguardian.com/science/the-lay-scientist/2016/feb/18/has-a-rampaging-ai-algorithm-really-killed-thousands-in-pakistan"&gt;Has …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;This Review gives an overview of intersting stuff I stumbled over which are
related to machine learning. Most of it was posted in &lt;a href="https://ml-ka.de/"&gt;KITs machine learning group&lt;/a&gt; (on Facebook).&lt;/p&gt;
&lt;p&gt;A lot of stuff can be found in my article about &lt;a href="https://martin-thoma.com/nips-2016/"&gt;NIPS 2016&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="new-developments"&gt;New Developments&lt;/h2&gt;
&lt;!-- Trends --&gt;
&lt;ul&gt;
&lt;li&gt;Random forests for courier detection: &lt;a href="https://www.theguardian.com/science/the-lay-scientist/2016/feb/18/has-a-rampaging-ai-algorithm-really-killed-thousands-in-pakistan"&gt;Has a rampaging AI algorithm called Skynet really killed thousands in Pakistan?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="live-demos-and-websites"&gt;Live Demos and Websites&lt;/h2&gt;
&lt;h3 id="quickdraw"&gt;Quickdraw&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://quickdraw.withgoogle.com/"&gt;Quickdraw&lt;/a&gt; is a program which tries to
guess what you drew. However, it is difficult to check if they really apply
machine learning, because it tells you what to draw and then tries to recognize
it.&lt;/p&gt;
&lt;figure style="display:table;margin: 0 auto 0.55em;"&gt;
&lt;a href="https://martin-thoma.com/images/2016/11/quickdraw.png"&gt;&lt;img align="middle" src="https://martin-thoma.com/images/2016/11/quickdraw.png" width="512"/&gt;&lt;/a&gt;
&lt;figcaption style="display:table-caption;caption-side:bottom"&gt;I had to draw a piano, a floor lamp, a chandelier, a suitcase, a candle and a lipstick each in under 20 seconds.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;It looks very much like an attempt to get lots of training data. However, this
plan might not work that well: &lt;a href="https://imgur.com/a/hUrOj"&gt;Interesting Quickdraw Fails&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You might find more stuff like Quickdraw on
&lt;a href="https://aiexperiments.withgoogle.com/"&gt;aiexperiments.withgoogle.com&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="loss-functions"&gt;Loss Functions&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://lossfunctions.tumblr.com/"&gt;lossfunctions.tumblr.com&lt;/a&gt; is a blog created
by &lt;a href="http://cs.stanford.edu/people/karpathy/"&gt;Andrej Karpathy&lt;/a&gt; where he collects
- well, let's call them "interesting" - loss functions.&lt;/p&gt;
&lt;h3 id="eyescream"&gt;Eyescream&lt;/h3&gt;
&lt;p&gt;Have you heard about &lt;abbr title="Generative Adversarial Networks"&gt;GANs&lt;/abbr&gt;?&lt;/p&gt;
&lt;p&gt;&lt;a href="http://soumith.ch/eyescream/"&gt;Eyescream&lt;/a&gt; is a demo for the
generator.&lt;/p&gt;
&lt;h2 id="publications_1"&gt;Publications&lt;/h2&gt;
&lt;!-- e.g. arXiv --&gt;
&lt;h3 id="deep-neural-networks-are-easily-fooled"&gt;Deep Neural Networks are Easily Fooled&lt;/h3&gt;
&lt;p&gt;The input of CNNs for image classification can be manipulated in two ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;An image, on which a human does not recognize anything (e.g. white noise)
   gets a high score for some object class.&lt;/li&gt;
&lt;li&gt;An image on which a human is certain to recognize one class
   (e.g.&amp;nbsp;"cat")
   is manipulated in a way that the CNN classifies with high certainty something
   different
   (e.g.&amp;nbsp;"factory").&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;See also:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Anh Nguyen, Jason Yosinski, Jeff Clune: &lt;a href="http://arxiv.org/abs/1412.1897"&gt;Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images&lt;/a&gt; on arxiv.&lt;/li&gt;
&lt;li&gt;Evolving AI Lab: &lt;a href="https://www.youtube.com/watch?v=M2IebCN9Ht4"&gt;Deep Neural Networks are Easily Fooled&lt;/a&gt; on YouTube in 5:33 min.&lt;/li&gt;
&lt;li&gt;Google: &lt;a href="http://googleresearch.blogspot.de/2015/06/inceptionism-going-deeper-into-neural.html"&gt;Inceptionism: Going Deeper into Neural Networks&lt;/a&gt;. 17.06.2016.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="breaking-linear-classifiers-on-imagenet"&gt;Breaking Linear Classifiers on ImageNet&lt;/h3&gt;
&lt;p&gt;Andrej Karpathy has once again written a nice article. The article describes
the problem that &lt;a href="http://karpathy.github.io/2015/03/30/breaking-convnets/"&gt;linear classifiers can be broken easily&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Hinton commented something simmilar &lt;a href="https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/clyjbai"&gt;on Reddit&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="where-am-i"&gt;Where am I?&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.technologyreview.com/s/600889/google-unveils-neural-network-with-superhuman-ability-to-determine-the-location-of-almost/"&gt;Google Unveils Neural Network with &amp;ldquo;Superhuman&amp;rdquo; Ability to Determine the Location of Almost Any Image&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;One gives the neural network a photo and it tells you where it was taken.&lt;/p&gt;
&lt;h3 id="lime"&gt;LIME&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1602.04938"&gt;"Why Should I Trust You?": Explaining the Predictions of Any Classifier&lt;/a&gt; deals with the problem of analyzing black box models decision making process.&lt;/p&gt;
&lt;h3 id="lip-reading"&gt;Lip Reading&lt;/h3&gt;
&lt;iframe allowfullscreen="" frameborder="0" height="288" src="https://www.youtube-nocookie.com/embed/fa5QGremQf8?rel=0" width="512"&gt;&lt;/iframe&gt;
&lt;p&gt;See the paper &lt;a href="https://openreview.net/pdf?id=BkjLkSqxg"&gt;LipNet: Sentence-Level Lipreading&lt;/a&gt; for details.&lt;/p&gt;
&lt;h3 id="more"&gt;More&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1610.06918"&gt;Learning to Protect Communications with Adversarial Neural Cryptography&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ai100.stanford.edu/2016-report"&gt;2016 Report&lt;/a&gt;: One Hundred Year Study on Artificial Intelligence (AI100)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="software_1"&gt;Software&lt;/h2&gt;
&lt;!-- e.g. Theano, Keras, ... --&gt;
&lt;h3 id="seaborn"&gt;Seaborn&lt;/h3&gt;
&lt;figure style="display:table;margin: 0 auto 0.55em;"&gt;
&lt;a href="https://martin-thoma.com/images/2016/11/seaborn_hexbin_marginals.png"&gt;&lt;img align="middle" src="https://martin-thoma.com/images/2016/11/seaborn_hexbin_marginals.png" width="512"/&gt;&lt;/a&gt;
&lt;figcaption style="display:table-caption;caption-side:bottom"&gt;Example plot generated by Seaborn&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Seaborn is a Python package for the visualization of data and statistics.&lt;/p&gt;
&lt;p&gt;See &lt;a href="http://stanford.edu/~mwaskom/software/seaborn/"&gt;stanford.edu/~mwaskom/software/seaborn&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="recnet"&gt;RecNet&lt;/h3&gt;
&lt;p&gt;J&amp;ouml;rg made &lt;a href="https://github.com/joergfranke/recnet"&gt;recnet&lt;/a&gt; publicly available.
It is a framework based on Theano to simplify the creation of recurrent
networks.&lt;/p&gt;
&lt;h3 id="image-segmentation-using-digits-5"&gt;Image Segmentation Using DIGITS 5&lt;/h3&gt;
&lt;p&gt;I didn't try it by now, but the images in the article
&lt;a href="https://devblogs.nvidia.com/parallelforall/image-segmentation-using-digits-5/"&gt;Image Segmentation Using DIGITS 5&lt;/a&gt; look awesome. I would be happy to hear what
you think about it.&lt;/p&gt;
&lt;h3 id="kerasjs"&gt;Keras.js&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Run Keras models (trained using Tensorflow backend) in your browser, with GPU support. Models are created directly from the Keras JSON-format configuration file, using weights serialized directly from the corresponding HDF5 file.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;See &lt;a href="https://github.com/transcranial/keras-js"&gt;github.com/transcranial/keras-js&lt;/a&gt; for more.&lt;/p&gt;
&lt;h2 id="interesting-questions_1"&gt;Interesting Questions&lt;/h2&gt;
&lt;!-- For example StackExchange --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://datascience.stackexchange.com/q/10286/8820"&gt;When being in a perfect &amp;ldquo;Long Valley&amp;rdquo; situation, does momentum help?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://datascience.stackexchange.com/q/15188/8820"&gt;Are non-zero paddings used?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://datascience.stackexchange.com/q/15081/8820"&gt;Why do CNNs with ReLU learn that well?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cs.stackexchange.com/q/65828/2914"&gt;Is there a metric for the similarity of two image filters?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="miscallenious"&gt;Miscallenious&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Eugenio Culurciello's blog: &lt;a href="https://culurciello.github.io/tech/2016/06/04/nets.html"&gt;Neural Network Architectures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Aaditya Prakash's blog: &lt;a href="http://iamaaditya.github.io/2016/03/one-by-one-convolution/"&gt;One by One [ 1 x 1 ] Convolution - counter-intuitively useful&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Adit Deshpande's blog: &lt;a href="https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html"&gt;The 9 Deep Learning Papers You Need To Know About (Understanding CNNs Part 3)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://news.mit.edu/2016/artificial-intelligence-system-surfs-web-improve-performance-1110"&gt;Artificial-intelligence system surfs web to improve its performance&lt;/a&gt; (&lt;a href="https://arxiv.org/pdf/1603.07954v3.pdf"&gt;paper&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html"&gt;The 9 Deep Learning Papers you need to know about&lt;/a&gt;: Explains AlexNet, ZDNet, ResNets, (Fast(er)) RCNNs, GANs&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.asimovinstitute.org/neural-network-zoo/"&gt;The Neural Network Zoo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf"&gt;Selective Search&lt;/a&gt;: Creating region proposals for object detection&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1406.2661v1"&gt;GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1312.6199v4"&gt;Adverserial Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://projector.tensorflow.org/"&gt;Google Projector&lt;/a&gt;: Display high-dimensional data&lt;/li&gt;
&lt;li&gt;Andrej Karpathy: &lt;a href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.p3xez1lr8"&gt;Yes you should understand backprop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://opennmt.net/"&gt;OpenNMT&lt;/a&gt;: A machine translation system&lt;/li&gt;
&lt;li&gt;Pete Warden's blog: &lt;a href="https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/"&gt;Why GEMM is at the heart of deep learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://archive.org/details/stackexchange"&gt;StackExchange Dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="meetings"&gt;Meetings&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;London, 4. December 2016: &lt;a href="https://www.eventbrite.com/e/immigration-by-numbers-insights-through-data-visualisation-tickets-28920900191?aff=twitter"&gt;Data Visualization Challenge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Barcelona, 5. December 2016 - 10. December 2016: Neural Information Processing Systems (NIPS) (&lt;a href="https://nips.cc/"&gt;Link&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="navigation clearfix"&gt;
&lt;div class="alignleft"&gt;
&lt;a href="https://martin-thoma.com/ml-review-1/" rel="prev"&gt;&amp;laquo; Previous Review&lt;/a&gt;
&lt;/div&gt;
&lt;div class="alignright"&gt;
&lt;a href="https://martin-thoma.com/ml-review-3/" rel="next"&gt;Next Review &amp;raquo;&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;</content><category term="Machine Learning"></category></entry><entry><title>NIPS 2016</title><link href="https://martin-thoma.com/nips-2016/" rel="alternate"></link><published>2016-12-24T20:00:00+01:00</published><updated>2016-12-24T20:00:00+01:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2016-12-24:/nips-2016/</id><summary type="html">&lt;p&gt;The Conference and Workshop on Neural Information Processing Systems (NIPS) is
probably the biggest conference with machine learning / deep learning as a
main topic. This year, about 6000 people attended it. My friend Marvin and me
were supported by &lt;a href="https://www.informatik.kit.edu/begabtenstiftung_informatik_karlsruhe.php"&gt;Begabtenstiftung Informatik Karlsruhe&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The complete program can be found in the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The Conference and Workshop on Neural Information Processing Systems (NIPS) is
probably the biggest conference with machine learning / deep learning as a
main topic. This year, about 6000 people attended it. My friend Marvin and me
were supported by &lt;a href="https://www.informatik.kit.edu/begabtenstiftung_informatik_karlsruhe.php"&gt;Begabtenstiftung Informatik Karlsruhe&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The complete program can be found in the &lt;a href="https://media.nips.cc/Conferences/2016/NIPS-2016-Conference-Book.pdf"&gt;Conference Book&lt;/a&gt;, but I would like to point out some of my highlights.&lt;/p&gt;
&lt;h2 id="hot-topics"&gt;Hot Topics&lt;/h2&gt;
&lt;p&gt;To get an idea what NIPS 2016 was about, I generated a word cloud from the titles
of the accepted papers:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Wordcloud of NIPS titles of 2016" src="https://martin-thoma.com/images/2016/12/nips-2016-wordcloud.png"/&gt;&lt;/p&gt;
&lt;p&gt;The organization team made something similar, but they had access to the
information how the papers were tagged:&lt;/p&gt;
&lt;p&gt;&lt;img alt="NIPS 2016 areas submitted" src="https://martin-thoma.com/images/2016/12/subject_areas_submitted1.png"/&gt;&lt;/p&gt;
&lt;p&gt;(Image source: &lt;a href="http://www.tml.cs.uni-tuebingen.de/team/luxburg/misc/nips2016/index.php"&gt;www.tml.cs.uni-tuebingen.de&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;The top 10 topics were:
&lt;ol&gt;
&lt;li&gt;01: Deep Learning or Neural Networks&lt;/li&gt;
&lt;li&gt;42: (Application) Computer Vision&lt;/li&gt;
&lt;li&gt;02: Large Scale Learning and Big Data&lt;/li&gt;
&lt;li&gt;05: Learning Theory&lt;/li&gt;
&lt;li&gt;53: (Other) Optimization&lt;/li&gt;
&lt;li&gt;08: Sparsity and Feature Selection&lt;/li&gt;
&lt;li&gt;51: (Other) Classification&lt;/li&gt;
&lt;li&gt;03: Convex Optimization&lt;/li&gt;
&lt;li&gt;54: (Other) Probabilistic Models and Methods&lt;/li&gt;
&lt;li&gt;56: (Other) Unsupervised Learning Methods&lt;/li&gt;
&lt;/ol&gt;&lt;/p&gt;
&lt;p&gt;I would say the top five hot topics (first is hottest) are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;GANs&lt;/li&gt;
&lt;li&gt;Reinforcment Learning: Look for "bandit" in the paper titles&lt;/li&gt;
&lt;li&gt;unsupervised learning&lt;/li&gt;
&lt;li&gt;alternative ways to train DNNs&lt;/li&gt;
&lt;li&gt;reducing the need for data (transfer learning, domain adaptation, semi-supervised learning)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="gans"&gt;GANs&lt;/h3&gt;
&lt;p&gt;Generative Adverserial Networks (short: GANs) were one hot topic at NIPS. The
idea is to train two networks: A generator &lt;span class="math"&gt;\(G\)&lt;/span&gt; and a discriminator &lt;span class="math"&gt;\(D\)&lt;/span&gt;. The
generator creates content (e.g. images) and the discriminator has to decide if
the content is of the natural distribution (the training set) or made by the
generator.&lt;/p&gt;
&lt;p&gt;An introduction can be found at &lt;a href="http://blog.evjang.com/2016/06/generative-adversarial-nets-in.html"&gt;blog.evjang.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Noteworthy papers and ideas are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6111-learning-what-and-where-to-draw.pdf"&gt;Learning What and Where to Draw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets.pdf"&gt;InfoGAN&lt;/a&gt;: Get more control about properties of the generated content.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/soumith/ganhacks"&gt;How to Train a GAN?&lt;/a&gt; Tips and tricks to make GANs work&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1609.03552v2"&gt;Generative Visual Manipulation on the Natural Image Manifold&lt;/a&gt;: Allow interactive generation of images (see also: &lt;a href="https://www.reddit.com/r/MachineLearning/comments/5m61d6/r_demystifying_neural_style_transfer/"&gt;Reddit&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe allowfullscreen="" frameborder="0" height="288" src="https://www.youtube-nocookie.com/embed/9c4z6YsBGQ0?rel=0" width="512"&gt;&lt;/iframe&gt;
&lt;p&gt;Applications of GANs are (according to &lt;a href="http://blog.evjang.com/2017/01/nips2016.html"&gt;Eric Jang&lt;/a&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reinforcement learning: &lt;a href="https://arxiv.org/abs/1611.03852"&gt;paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;domain adaptation&lt;/li&gt;
&lt;li&gt;security ML&lt;/li&gt;
&lt;li&gt;compression&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="bayesian-deep-learning"&gt;Bayesian Deep Learning&lt;/h3&gt;
&lt;p&gt;Combinding deep learning with graphical models like CRFs / Markov Random Fields
has been done for semantic segmentation for a while now. It seems like the
combination of those two is called "bayesian deep learning". If you look for
the keyword "variational" it seems to belong in this category.&lt;/p&gt;
&lt;p&gt;I don't really know this area, so I leav it to &lt;a href="http://blog.evjang.com/2017/01/nips2016.html"&gt;Eric Jang&lt;/a&gt;
to point out important papers.&lt;/p&gt;
&lt;h2 id="nuts-and-bolts-of-ml_1"&gt;Nut's and Bolts of ML&lt;/h2&gt;
&lt;p&gt;Andrew Ng gave a talk in which he summarized what he thinks are some of the
most important topics when training machine learning systems. Most of it
is probably also in his book &lt;a href="http://www.mlyearning.org/"&gt;Machine Learning Yearning&lt;/a&gt;
or in his &lt;a href="https://www.coursera.org/learn/machine-learning"&gt;Coursera course&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here are some of the things he talked about:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When you design a speech recognition system, you can measure 3 types of errors:
  Human error, training set error and test set error. The difference between
  the human error and the training error is "avoidable error" (bias), the difference
  between training and test error is "variance".&lt;/li&gt;
&lt;li&gt;Human level performance is ambiguous: In a medical application, is an
  amateur, a doctor, an experienced doctor or a team of (experienced) doctors
  the "human level performance"?&lt;/li&gt;
&lt;li&gt;Role of an "AI Product Manager"&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="more-papers"&gt;More Papers&lt;/h2&gt;
&lt;p&gt;Clustering:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6478-fast-and-provably-good-seedings-for-k-means"&gt;Fast and Provably Good Seedings for k-Means&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Optimization:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6268-metagrad-multiple-learning-rates-in-online-learning"&gt;MetaGrad: Multiple Learning Rates in Online Learning&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;https://bitbucket.org/wmkoolen/metagrad&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6213-optimal-learning-for-multi-pass-stochastic-gradient-methods"&gt;Optimal Learning for Multi-pass Stochastic Gradient Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1607.06450"&gt;Layer Normalization&lt;/a&gt;: An successor for Batch Normalization?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Theory:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6112-deep-learning-without-poor-local-minima"&gt;Deep Learning without Poor Local Minima&lt;/a&gt;: Local Minima are global minima in "typical" networks&lt;/li&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6048-matrix-completion-has-no-spurious-local-minimum.pdf"&gt;Matrix Completion has No Spurious Local Minimum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cs.toronto.edu/~wenjie/papers/nips16/top.pdf"&gt;Understanding the Effective Receptive Field in Deep Convolutional Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Topology learning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6372-learning-the-number-of-neurons-in-deep-networks"&gt;Learning the Number of Neurons in Deep Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;From another talk. Most add nodes / edges over time from an initial seed network:&lt;ul&gt;
&lt;li&gt;1960, Erd&amp;ouml;s &amp;amp; Renyi: Random graphs&lt;/li&gt;
&lt;li&gt;1998, Watts &amp;amp; Strogatz: Small-world graph&lt;/li&gt;
&lt;li&gt;1999, Barabasi &amp;amp; Albert: Preferential attachment&lt;/li&gt;
&lt;li&gt;1999, Kleinburg et al.: Copying model&lt;/li&gt;
&lt;li&gt;2003, Vazquez et al.: Duplication-divergence&lt;/li&gt;
&lt;li&gt;2007, Leskovec et al.: Forest fire&lt;/li&gt;
&lt;li&gt;2008, Clauset et al.: Hierarchical random graphs&lt;/li&gt;
&lt;li&gt;2010, Leskovec et al.: Kronecker graphs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Network compression&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1608.04493"&gt;Dynamic Network Surgery for Efficient DNNs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1504.08362"&gt;PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1605.06465"&gt;Swapout: Learning an ensemble of deep architectures&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Analysis of ML models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6482-blind-attacks-on-machine-learners"&gt;Blind Attacks on Machine Learners&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1605.07262"&gt;Measuring Neural Net Robustness with Constraints&lt;/a&gt;: Measure robustnes against adverserial examples&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1608.08967"&gt;Robustness of classifiers: from adversarial to random noise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1606.05313"&gt;Unsupervised Risk Estimation Using Only Conditional Independence Structure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6482-blind-attacks-on-machine-learners"&gt;Blind Attacks on Machine Learners&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://people.csail.mit.edu/beenkim/papers/KIM2016NIPS_MMD.pdf"&gt;Examples are not Enough, Learn to Criticize! Criticism for Interpretability&lt;/a&gt; (&lt;a href="https://github.com/BeenKim/MMD-critic"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1610.09064"&gt;Identifying Unknown Unknowns in the Open World: Representations and Policies for Guided Exploration&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Content creation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://visualdynamics.csail.mit.edu/"&gt;Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Labeling:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1610.09730"&gt;Active Learning from Imperfect Labelers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1608.07328"&gt;Fundamental Limits of Budget-Fidelity Trade-off in Label Crowdsourcing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1606.05374"&gt;Avoiding Imposters and Delinquents: Adversarial Crowdsourcing and Peer Prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Content based Image Retrival (CBIR):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6200-improved-deep-metric-learning-with-multi-class-n-pair-loss-objective.pdf"&gt;Improved Deep Metric Learning with Multi-class N-pair Loss Objective&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1611.00822"&gt;Learning Deep Embeddings with Histogram Loss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1610.08904"&gt;Local Similarity-Aware Deep Feature Embedding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1608.08792"&gt;CliqueCNN: Deep Unsupervised Exemplar Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6192-what-makes-objects-similar-a-unified-multi-metric-learning-approach"&gt;What Makes Objects Similar: A Unified Multi-Metric Learning Approach&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Misc:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6323-learnable-visual-markers"&gt;Learnable Visual Markers&lt;/a&gt;: Visual markers are something like barcodes&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1603.06143"&gt;Neurally-Guided Procedural Models: Amortized Inference for Procedural Graphics Programs using Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1606.03558v3.pdf"&gt;Universal Correspondence Network&lt;/a&gt;: Find semantically meaningful similar points in two images. For example, to frontal images of different humans, where the network finds eyes, nose, chin, lips in both images.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://places.csail.mit.edu/demo.html"&gt;Scene Recognition Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1612.00423"&gt;TorontoCity: Seeing the World with a Million Eyes&lt;/a&gt;: A new benchmark dataset by Raquel Urtasun&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://www.dropbox.com/s/vfmncjnyh57glkc/NIPS_LSCVS_ImageNet%20Analysis.pdf?dl=0"&gt;What makes ImageNet good for Transfer Learning?&lt;/a&gt;&lt;/strong&gt; by Jacob Huh (UC Berkeley), Pulkit Agrawal (UC Berkeley), and Alexei Efros (UC Berkeley)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="lessons-learned-for-conferences"&gt;Lessons learned for conferences&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bring a camera: The information comes very fast. Too fast to take notes, but
  you can shoot a photo of the slides. In fact, quite a lot of people do so.&lt;/li&gt;
&lt;li&gt;Shoot a photo of the first slide, so that you know what the talk was about
  when you look at your slides.&lt;/li&gt;
&lt;li&gt;If you give a talk / poster...&lt;ul&gt;
&lt;li&gt;... let the first slide be there long enough, so that people can take a
  photo of it.&lt;/li&gt;
&lt;li&gt;... or have a URL / the title of the paper on every single slide&lt;/li&gt;
&lt;li&gt;... let every slide be visible long enough, so that people can take photos&lt;/li&gt;
&lt;li&gt;... don't use QR-codes only, but also (shortened) URLs&lt;/li&gt;
&lt;li&gt;... make it available online as PDF&lt;/li&gt;
&lt;li&gt;... answer key questions: (1) Which problem did you tackle? (2) How did you test your results? (3) To what is your "solution" similar?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;As a session organizer...&lt;ul&gt;
&lt;li&gt;... make sure there is a schedule at the door (outside)&lt;/li&gt;
&lt;li&gt;... make sure the schedule is online&lt;/li&gt;
&lt;li&gt;... make sure the schedule is changed everywhere, if it is actually changed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="miscallenious"&gt;Miscallenious&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/book/advances-in-neural-information-processing-systems-29-2016"&gt;Advances in Neural Information Processing Systems 29 (NIPS 2016) pre-proceedings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jobs&lt;ul&gt;
&lt;li&gt;&lt;a href="https://unify.id/fellowship-fall2016.html"&gt;unify.id&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.bluevisionlabs.com/"&gt;bluevisionlabs.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/benhamner/nips-papers"&gt;NIPS Papers Kaggle Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;blog.aylien.com: &lt;a href="http://blog.aylien.com/highlights-nips-2016"&gt;Highlights of NIPS 2016: Adversarial Learning, Meta-learning and more&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://salon-des-refuses.org/"&gt;salon-des-refuses.org&lt;/a&gt; should contain papers
  which were refused, but are also of high quality. However, the website seems to be down.&lt;/li&gt;
&lt;li&gt;Martin Zinkevich: &lt;a href="http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf"&gt;Rules of Machine Learning: Best Practices for ML Engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.reddit.com/r/MachineLearning/comments/5hwqeb/project_all_code_implementations_for_nips_2016/"&gt;Implementations for NIPS 2016 papers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;YouTube:&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLJscN9YDD1buxCitmej1pjJkR5PMhenTF&amp;amp;app=desktop"&gt;GAN Workshop playlist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Yann Le Cun &lt;a href="https://www.youtube.com/watch?v=88nKI-qqWEo&amp;amp;feature=youtu.be"&gt;Energy-based GANs &amp;amp; other adverserial training&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="Machine Learning"></category><category term="research"></category></entry><entry><title>ML Review 1</title><link href="https://martin-thoma.com/ml-review-1/" rel="alternate"></link><published>2016-11-22T20:00:00+01:00</published><updated>2016-11-22T20:00:00+01:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2016-11-22:/ml-review-1/</id><summary type="html">&lt;p&gt;This Review gives an overview of intersting stuff I stumbled over which are
related to machine learning. Most of it was posted in &lt;a href="https://ml-ka.de/"&gt;KITs machine learning group&lt;/a&gt; (on Facebook).&lt;/p&gt;
&lt;h2 id="new-developments"&gt;New Developments&lt;/h2&gt;
&lt;!-- Trends --&gt;
&lt;ul&gt;
&lt;li&gt;Random forests for courier detection: &lt;a href="https://www.theguardian.com/science/the-lay-scientist/2016/feb/18/has-a-rampaging-ai-algorithm-really-killed-thousands-in-pakistan"&gt;Has a rampaging AI algorithm called Skynet really killed thousands in Pakistan?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="live-demos-and-websites"&gt;Live Demos …&lt;/h2&gt;</summary><content type="html">&lt;p&gt;This Review gives an overview of intersting stuff I stumbled over which are
related to machine learning. Most of it was posted in &lt;a href="https://ml-ka.de/"&gt;KITs machine learning group&lt;/a&gt; (on Facebook).&lt;/p&gt;
&lt;h2 id="new-developments"&gt;New Developments&lt;/h2&gt;
&lt;!-- Trends --&gt;
&lt;ul&gt;
&lt;li&gt;Random forests for courier detection: &lt;a href="https://www.theguardian.com/science/the-lay-scientist/2016/feb/18/has-a-rampaging-ai-algorithm-really-killed-thousands-in-pakistan"&gt;Has a rampaging AI algorithm called Skynet really killed thousands in Pakistan?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="live-demos-and-websites"&gt;Live Demos and Websites&lt;/h2&gt;
&lt;h3 id="quickdraw"&gt;Quickdraw&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://quickdraw.withgoogle.com/"&gt;Quickdraw&lt;/a&gt; is a program which tries to
guess what you drew. However, it is difficult to check if they really apply
machine learning, because it tells you what to draw and then tries to recognize
it.&lt;/p&gt;
&lt;figure style="display:table;margin: 0 auto 0.55em;"&gt;
&lt;a href="https://martin-thoma.com/images/2016/11/quickdraw.png"&gt;&lt;img align="middle" src="https://martin-thoma.com/images/2016/11/quickdraw.png" width="512"/&gt;&lt;/a&gt;
&lt;figcaption style="display:table-caption;caption-side:bottom"&gt;I had to draw a piano, a floor lamp, a chandelier, a suitcase, a candle and a lipstick each in under 20 seconds.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;It looks very much like an attempt to get lots of training data. However, this
plan might not work that well: &lt;a href="https://imgur.com/a/hUrOj"&gt;Interesting Quickdraw Fails&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You might find more stuff like Quickdraw on
&lt;a href="https://aiexperiments.withgoogle.com/"&gt;aiexperiments.withgoogle.com&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="loss-functions"&gt;Loss Functions&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://lossfunctions.tumblr.com/"&gt;lossfunctions.tumblr.com&lt;/a&gt; is a blog created
by &lt;a href="http://cs.stanford.edu/people/karpathy/"&gt;Andrej Karpathy&lt;/a&gt; where he collects
- well, let's call them "interesting" - loss functions.&lt;/p&gt;
&lt;h2 id="publications_1"&gt;Publications&lt;/h2&gt;
&lt;!-- e.g. arXiv --&gt;
&lt;h3 id="deep-neural-networks-are-easily-fooled"&gt;Deep Neural Networks are Easily Fooled&lt;/h3&gt;
&lt;p&gt;The input of CNNs for image classification can be manipulated in two ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;An image, on which a human does not recognize anything (e.g. white noise)
   gets a high score for some object class.&lt;/li&gt;
&lt;li&gt;An image on which a human is certain to recognize one class
   (e.g.&amp;nbsp;"cat")
   is manipulated in a way that the CNN classifies with high certainty something
   different
   (e.g.&amp;nbsp;"factory").&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;See also:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Anh Nguyen, Jason Yosinski, Jeff Clune: &lt;a href="http://arxiv.org/abs/1412.1897"&gt;Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images&lt;/a&gt; on arxiv.&lt;/li&gt;
&lt;li&gt;Evolving AI Lab: &lt;a href="https://www.youtube.com/watch?v=M2IebCN9Ht4"&gt;Deep Neural Networks are Easily Fooled&lt;/a&gt; on YouTube in 5:33 min.&lt;/li&gt;
&lt;li&gt;Google: &lt;a href="http://googleresearch.blogspot.de/2015/06/inceptionism-going-deeper-into-neural.html"&gt;Inceptionism: Going Deeper into Neural Networks&lt;/a&gt;. 17.06.2016.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="breaking-linear-classifiers-on-imagenet"&gt;Breaking Linear Classifiers on ImageNet&lt;/h3&gt;
&lt;p&gt;Andrej Karpathy has once again written a nice article. The article describes
the problem that &lt;a href="http://karpathy.github.io/2015/03/30/breaking-convnets/"&gt;linear classifiers can be broken easily&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Hinton commented something simmilar &lt;a href="https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/clyjbai"&gt;on Reddit&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="where-am-i"&gt;Where am I?&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.technologyreview.com/s/600889/google-unveils-neural-network-with-superhuman-ability-to-determine-the-location-of-almost/"&gt;Google Unveils Neural Network with &amp;ldquo;Superhuman&amp;rdquo; Ability to Determine the Location of Almost Any Image&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;One gives the neural network a photo and it tells you where it was taken.&lt;/p&gt;
&lt;h3 id="lime"&gt;LIME&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1602.04938"&gt;"Why Should I Trust You?": Explaining the Predictions of Any Classifier&lt;/a&gt; deals with the problem of analyzing black box models decision making process.&lt;/p&gt;
&lt;h3 id="lip-reading"&gt;Lip Reading&lt;/h3&gt;
&lt;iframe allowfullscreen="" frameborder="0" height="288" src="https://www.youtube-nocookie.com/embed/fa5QGremQf8?rel=0" width="512"&gt;&lt;/iframe&gt;
&lt;p&gt;See the paper &lt;a href="https://openreview.net/pdf?id=BkjLkSqxg"&gt;LipNet: Sentence-Level Lipreading&lt;/a&gt; for details.&lt;/p&gt;
&lt;h3 id="more"&gt;More&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1610.06918"&gt;Learning to Protect Communications with Adversarial Neural Cryptography&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ai100.stanford.edu/2016-report"&gt;2016 Report&lt;/a&gt;: One Hundred Year Study on Artificial Intelligence (AI100)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="software_1"&gt;Software&lt;/h2&gt;
&lt;!-- e.g. Theano, Keras, ... --&gt;
&lt;h3 id="seaborn"&gt;Seaborn&lt;/h3&gt;
&lt;figure style="display:table;margin: 0 auto 0.55em;"&gt;
&lt;a href="https://martin-thoma.com/images/2016/11/seaborn_hexbin_marginals.png"&gt;&lt;img align="middle" src="https://martin-thoma.com/images/2016/11/seaborn_hexbin_marginals.png" width="512"/&gt;&lt;/a&gt;
&lt;figcaption style="display:table-caption;caption-side:bottom"&gt;Example plot generated by Seaborn&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Seaborn is a Python package for the visualization of data and statistics.&lt;/p&gt;
&lt;p&gt;See &lt;a href="http://stanford.edu/~mwaskom/software/seaborn/"&gt;stanford.edu/~mwaskom/software/seaborn&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="recnet"&gt;RecNet&lt;/h3&gt;
&lt;p&gt;J&amp;ouml;rg made &lt;a href="https://github.com/joergfranke/recnet"&gt;recnet&lt;/a&gt; publicly available.
It is a framework based on Theano to simplify the creation of recurrent
networks.&lt;/p&gt;
&lt;h3 id="image-segmentation-using-digits-5"&gt;Image Segmentation Using DIGITS 5&lt;/h3&gt;
&lt;p&gt;I didn't try it by now, but the images in the article
&lt;a href="https://devblogs.nvidia.com/parallelforall/image-segmentation-using-digits-5/"&gt;Image Segmentation Using DIGITS 5&lt;/a&gt; look awesome. I would be happy to hear what
you think about it.&lt;/p&gt;
&lt;h3 id="kerasjs"&gt;Keras.js&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Run Keras models (trained using Tensorflow backend) in your browser, with GPU support. Models are created directly from the Keras JSON-format configuration file, using weights serialized directly from the corresponding HDF5 file.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;See &lt;a href="https://github.com/transcranial/keras-js"&gt;github.com/transcranial/keras-js&lt;/a&gt; for more.&lt;/p&gt;
&lt;h2 id="interesting-questions_1"&gt;Interesting Questions&lt;/h2&gt;
&lt;!-- For example StackExchange --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://datascience.stackexchange.com/q/10286/8820"&gt;When being in a perfect &amp;ldquo;Long Valley&amp;rdquo; situation, does momentum help?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://datascience.stackexchange.com/q/15188/8820"&gt;Are non-zero paddings used?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://datascience.stackexchange.com/q/15081/8820"&gt;Why do CNNs with ReLU learn that well?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cs.stackexchange.com/q/65828/2914"&gt;Is there a metric for the similarity of two image filters?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="miscallenious"&gt;Miscallenious&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://bigstory.ap.org/article/265cd8fb02fb44a69cf0eaa2063e11d9/mexico-taking-us-factory-jobs-blame-robots-instead"&gt;Why robots, not trade, are behind so many factory job losses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://halite.io/"&gt;halite.io&lt;/a&gt;: A website for ML challenges.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.theverge.com/2016/11/4/13518210/deepmind-starcraft-ai-google-blizzard"&gt;Google DeepMind's next gaming challenge: can AI beat StarCraft II?&lt;/a&gt; (and the &lt;a href="https://deepmind.com/blog/deepmind-and-blizzard-release-starcraft-ii-ai-research-environment/"&gt;post on deepnet&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://qz.com/823820/carnegie-mellon-made-a-special-pair-of-glasses-that-lets-you-steal-a-digital-identity/"&gt;All it takes to steal your face is a special pair of glasses&lt;/a&gt; - and the paper &lt;a href="https://www.cs.cmu.edu/~sbhagava/papers/face-rec-ccs16.pdf"&gt;Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/the-physics-arxiv-blog/when-a-machine-learning-algorithm-studied-fine-art-paintings-it-saw-things-art-historians-had-never-b8e4e7bf7d3e"&gt;When A Machine Learning Algorithm Studied Fine Art Paintings, It Saw Things Art Historians Had Never Noticed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://open_nsfw.gitlab.io/"&gt;Image Synthesis from Yahoo's open_nsfw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.wired.com/2016/10/cops-database-117m-faces-youre-probably/"&gt;Cops have a database of 117M faces. You're probably in it&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.crowdai.org/challenges/ants-challenge-part-1"&gt;Ants Challenge - Part I&lt;/a&gt;: identify and track individual ants over time; recognize when ants engage in food transfer&lt;/li&gt;
&lt;li&gt;&lt;a href="https://techcrunch.com/2016/10/11/five-years-of-observations-from-tandem-satellites-produce-3d-world-map-of-unprecedented-accuracy/"&gt;Five years of observations from tandem satellites produce 3D world map of unprecedented accuracy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_tramer.pdf"&gt;Stealing Machine Learning Models via Prediction APIs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.newscientist.com/article/2108934-deepminds-ai-has-learned-to-navigate-the-tube-using-memory/"&gt;DeepMind&amp;rsquo;s AI has learned to navigate the Tube using memory&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="meetings"&gt;Meetings&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;London, 7. April 2016: Deep Learning in Healthcare Summit (&lt;a href="https://re-work.co/events/deep-learning-health-london-2016"&gt;Link&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="navigation clearfix"&gt;
&lt;div class="alignright"&gt;
&lt;a href="https://martin-thoma.com/ml-review-2/" rel="next"&gt;Next Review &amp;raquo;&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;</content><category term="Machine Learning"></category></entry><entry><title>Machine Learning Glossary</title><link href="https://martin-thoma.com/ml-glossary/" rel="alternate"></link><published>2016-10-24T20:00:00+02:00</published><updated>2016-10-24T20:00:00+02:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2016-10-24:/ml-glossary/</id><summary type="html">&lt;p&gt;The following is a list of short explanations of different terms in machine
learning. The aim is to keep things simple and brief, not to explain the terms
in full detail.&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;dfn id="active-learning"&gt;Active Learning&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;The algorithm gives a pattern and asks for a label.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="backpropagation"&gt;Backpropagation&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;A clever implementation of gradient descent …&lt;/dd&gt;&lt;/dl&gt;</summary><content type="html">&lt;p&gt;The following is a list of short explanations of different terms in machine
learning. The aim is to keep things simple and brief, not to explain the terms
in full detail.&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;dfn id="active-learning"&gt;Active Learning&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;The algorithm gives a pattern and asks for a label.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="backpropagation"&gt;Backpropagation&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;A clever implementation of gradient descent for neural networks.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="blstm"&gt;BLSTM&lt;/dfn&gt;, &lt;dfn id="bilstm"&gt;BiLSTM&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;Bidirectional long short-term memory (see &lt;a href="http://www.di.ufpe.br/~fnj/RNA/bibliografia/BRNN.pdf"&gt;paper&lt;/a&gt; and &lt;a href="https://www.cs.toronto.edu/~graves/asru_2013_poster.pdf"&gt;poster&lt;/a&gt;).&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="co-training"&gt;Co-Training&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;A form of semi-supervised learning. Two independant classifiers are
        trained on different labeled datasets. The classifiers are applied to
        the unlabeled data. Data with high confidence will be added to the
        other classifiers data.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="collaborative-filtering"&gt;Computer Vision&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;You have users and items which are rated. No user rated everything.
        You want to fill the gaps (see &lt;a href="https://martin-thoma.com/collaborative-filtering/"&gt;article&lt;/a&gt;).&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="computer-vision"&gt;Computer Vision&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;The academic discipline which deals with how to gain high-level understanding from digital images or videos. Common tasks include image classifiction, semantic segmentation, detection and localization.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="curriculum-learning"&gt;Curriculum learning&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;A method for pretraining. First optimize a smoothed objective and gradually consider less smoothing. So a curriculum is a sequence of training criteria. One might show gradually more difficult training examples. See &lt;a href="http://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf"&gt;Curriculum Learning&lt;/a&gt; by Benigo, Louradour, Collobert and Weston for details.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="curse-of-dimensionality"&gt;Curse of dimensionality&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;Various problems of high-dimensional spaces that do not occur in low-dimensional spaces.
        High-dimensional often means several 100 dimensions. See also: &lt;a href="https://martin-thoma.com/average-distance-of-points"&gt;Average Distance of Points&lt;/a&gt;&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="dcgan"&gt;DCGAN&lt;/dfn&gt; (&lt;dfn&gt;Deep Convolutional Generative Adverserial Networks&lt;/dfn&gt;)&lt;/dt&gt;
&lt;dd&gt;TODO&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="dcign"&gt;DCIGN&lt;/dfn&gt; (&lt;dfn&gt;Deep Convolutional Inverse Graphic Network&lt;/dfn&gt;)&lt;/dt&gt;
&lt;dd&gt;TODO&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="dcnn"&gt;DCNN&lt;/dfn&gt; (&lt;dfn&gt;Doubly Convolutional Neural Network&lt;/dfn&gt;)&lt;/dt&gt;
&lt;dd&gt;Introduced in &lt;a href="https://arxiv.org/pdf/1610.09716v1.pdf"&gt;this paper&lt;/a&gt; (&lt;a href="http://www.shortscience.org/paper?bibtexKey=conf/nips/ZhaiCZL16#martinthoma"&gt;summary&lt;/a&gt;).

    &lt;b&gt;Note&lt;/b&gt; Some people also call &lt;i&gt;Deep Convolutional Neural Networks&lt;/i&gt; DCNNs.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="dnn"&gt;DNN&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;Deep Neural Network. The meaning of "deep" differs. Sometimes it means at
        least one hidden layer, sometimes it means at least 12 hidden layers.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="domain-adaptation"&gt;Domain adaptation&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;A model is trained on dataset $A$. How does it have to be changed to work on dataset $B$?&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="object-detection"&gt;Detection in Computer Vision&lt;/dfn&gt; (&lt;dfn&gt;Object detection&lt;/dfn&gt;)&lt;/dt&gt;
&lt;dd&gt;Object detection in an image is a computer vision task. The input
        is an image and the output is a list with rectangles which contain
        objects of the given type. Face detection is one well-studied example.
        A photo could contain no face or hundrets of them. The rectangles
        can overlap.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="deep-learing"&gt;Deep Learning&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;Buzzword. The meaning depends on who you ask / in which year you asked.
        Sometimes it means multi-layer perceptrons with more than $N$ layers
        (some say $N=2$ is already deep learning, others want N&amp;gt;20 or nowadays
        $N&amp;gt;100$).&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="discriminative-model"&gt;Discriminative Model&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;The model gives a conditional probability of the classes $k$, given the
        feature vector $x$: $P(k | x)$.
        This kind of model is often used for prediction.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="fc7-features"&gt;FC7-Features&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;Features of an image which was run through a trained neural network.
        AlexNet called the last fully connected layer FC7. However, FC7
        features are not necessarily created by AlexNet.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="fmllr"&gt;FMLLR&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;Feature-Space Maximum Likelihood Linear Regression&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="feature-map"&gt;Feature Map&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;A feature map is the result of a single filter of a convolutional layer
        being applied. So it is the activation of that filter over the given
        input.&lt;/dd&gt;
&lt;dt&gt;&lt;a href="https://en.wikipedia.org/wiki/Mixture_model"&gt;&lt;dfn id="gmm"&gt;GMM&lt;/dfn&gt;&lt;/a&gt;&lt;/dt&gt;
&lt;dd&gt;Gaussian Mixture Model&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="gemm"&gt;GEMM&lt;/dfn&gt; (&lt;dfn&gt;GEneral Matrix to Matrix Multiplication&lt;/dfn&gt;)&lt;/dt&gt;
&lt;dd&gt;General Matrix to Matrix Multiplication is the problem of
        calculating the result of $C = A \cdot B$ with $A \in \mathbb{R}^{n \times m}, B \in \mathbb{R}^{m \times k}, C \in \mathbb{R}^{n \times k}$.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="generative-model"&gt;Generative model&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;The model gives the relationship of variables: $P(x, y)$.
        This kind of model can be used for prediction, too.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="gradient-descent"&gt;Gradient Descent&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;An iterative optimization algorithm for differentiable functions.&lt;/dd&gt;
&lt;dt&gt;&lt;a href="https://en.wikipedia.org/wiki/Hidden_Markov_model"&gt;&lt;dfn id="hmm"&gt;HMM&lt;/dfn&gt;&lt;/a&gt;&lt;/dt&gt;
&lt;dd&gt;Hidden Markov Model&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="i-vector"&gt;i-vector&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;speaker identity vector. See &lt;a href="http://ieeexplore.ieee.org/abstract/document/5545402/"&gt;Front-End Factor Analysis for Speaker Verification&lt;/a&gt;.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="machine-vision"&gt;Machine Vision&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;Computer vision applied for industrial applications.&lt;/dd&gt;
&lt;dt&gt;&lt;a href="https://en.wikipedia.org/wiki/Matrix_completion"&gt;&lt;dfn id="matrix-completion"&gt;Matrix Completion&lt;/dfn&gt;&lt;/a&gt;&lt;/dt&gt;
&lt;dd&gt;See &lt;a href="#collaborative-filtering"&gt;collaborative filtering&lt;/a&gt;.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="mllr"&gt;MLLR&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;Maximum Likelihood Linear Regression&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="mmd"&gt;MMD&lt;/dfn&gt; (&lt;dfn id="maximum-mean-descripancy"&gt;Maximum Mean Descrepancy&lt;/dfn&gt;)&lt;/dt&gt;
&lt;dd&gt;MMD is a measure of the difference between a distribution $P$ and a distribution $Q$:

        $$MMD(F, p, q) = sup_{f \in F} (\mathbb{E}_{x \sim p} [f(x)] - \mathbb{E}_{y \sim q} [f(y)])$$

    &lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="multi-task-learning"&gt;Multi-Task learning&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;Train a model which does multiple tasks at the same time, e.g.
        segmentation and detection (see &lt;a href="https://arxiv.org/abs/1612.07695"&gt;MultiNet&lt;/a&gt;).&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="object-recognition"&gt;Object recognition&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;Classification on images. The task is to decide in which class a given
        image falls, judging by the content. This can be cat, dog, plane or
        similar.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="one-shot-learning"&gt;&lt;a href="https://en.wikipedia.org/wiki/One-shot_learning"&gt;One-Shot learning&lt;/a&gt;&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;Learn only with one or very few examples per class. See &lt;a href="http://vision.stanford.edu/documents/Fei-FeiFergusPerona2006.pdf"&gt;One-Shot Learning of Object Categories&lt;/a&gt;.&lt;/dd&gt;
&lt;dt&gt;&lt;a href="https://en.wikipedia.org/wiki/Optical_flow"&gt;&lt;dfn id="optical-flow"&gt;Optical Flow&lt;/dfn&gt;&lt;/a&gt;&lt;/dt&gt;
&lt;dd&gt;Optical flow is defined for two images. It describes how the points in
        one image moved when switching to the second image.&lt;/dd&gt;
&lt;dt&gt;&lt;a href="https://en.wikipedia.org/wiki/Principal_component_analysis"&gt;&lt;dfn id="pca"&gt;PCA&lt;/dfn&gt;&lt;/a&gt;&lt;/dt&gt;
&lt;dd&gt;Principal component analysis (short: PCA) is a linear transformation
        which projects $n$ points $\mathbf{x} \in \mathbb{R}^{n \times s}$ with
        $s$ features each on a hyperplane in such a way
        that the projection error is minimal. Hence it is an unsupervised
        method for feature reduction. It simply works by finding a matrix
        $P \in \mathbb{R}^{s \times m}$, where $m \leq s$ can be chosen as small
        as desired.
        &lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="regularization"&gt;Regularization&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;Regularization are techniques to make the fitted function smoother. This
        helps to prevent overfitting.&lt;br/&gt;
        Examples: L1, L2, Dropout, Weight Decay in Neural Networks. Parameter $C$ in SVMs.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="reinforcement-learning"&gt;Reinforcement Learning&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;Reinforcment learning is a sub-field of machine learning, which focuses
        on the question how to find actions which lead to higher rewards. See
        &lt;a href="../probabilistische-planung/#reinforcement-learning"&gt;German lecture notes&lt;/a&gt;.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="self-learning"&gt;Self-Learning&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;One form of semi-supervised learning, where you train an initial system
        on the labeled data, then label the unlabeled data where the classifier
        is 'sure enough'. After that, you train a new system on all data and
        re-label the unlabeled data. This is iterated.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="semi-supervised-learning"&gt;Semi-supervised learning&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;Some training data has labels, but most has no labels.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="supervised-learning"&gt;Supervised learning&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;All training data has labels.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="spatial-pyramid-pooling"&gt;Spatial Pyramid Pooling&lt;/dfn&gt; (&lt;dfn&gt;SPP&lt;/dfn&gt;)&lt;/dt&gt;
&lt;dd&gt;SPP is the idea of dividing the image into a grid with a fixed number
        of cells and a variable size, depending on the input. Each cell computes
        one feature and hence leads to a fixed-size representation of a variable-sized
        input.&lt;br/&gt;
        See &lt;a href="https://arxiv.org/abs/1406.4729v4"&gt;paper&lt;/a&gt; and &lt;a href="http://www.shortscience.org/paper?bibtexKey=journals/corr/1406.4729"&gt;summary&lt;/a&gt;&lt;/dd&gt;
&lt;dt&gt;&lt;a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf"&gt;&lt;dfn id="tf-idf"&gt;TF-IDF&lt;/dfn&gt;&lt;/a&gt;&lt;/dt&gt;
&lt;dd&gt;TF-IDF (short for Term frequency&amp;ndash;inverse document frequency)
        is a measure that reflects how important a word is to a document in a
        collection or corpus.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="transductive-learning"&gt;Transductive learning&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;label unlabeled data (the aim here is NOT to find a hypothesis)&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="unsupervised-learning"&gt;Unsupervised learning&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;No training data has labels.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="vc-dimension"&gt;VC-Dimension&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;A theoretical natural number assigned to any classifier. The higher
        the VC dimension of a classifier, the more situations it is able
        to capture (see &lt;a href="http://datascience.stackexchange.com/a/16144/8820"&gt;longer explanation&lt;/a&gt;, &lt;a href="https://martin-thoma.com/machine-learning-1-course/#vc-dimension"&gt;german explanation&lt;/a&gt;).&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="vtln"&gt;VTLN&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;vocal tract length normalization&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="zero-shot-learning"&gt;Zero-Shot learning&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;Learning to predict classes, of which no example has been seen during
        training. For example, Flicker gets several new tags each day and they
        want to predict tags for new images. One idea is to use WordNet and
        ImageNet to generate a common embedding. This way, new words of WordNet
        could already have an embedding and thus new images categories could also automatically be classified the right way. See &lt;a href="http://www.cs.cmu.edu/afs/cs/project/theo-73/www/papers/zero-shot-learning.pdf"&gt;Zero-Shot Learning with Semantic Output Codes&lt;/a&gt; as well as &lt;a href="https://www.youtube.com/watch?v=Pmv5JHKX2y4"&gt;this YouTube video&lt;/a&gt;.&lt;/dd&gt;
&lt;/dl&gt;
&lt;h2 id="see-also"&gt;See also&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Lectures:&lt;ul&gt;
&lt;li&gt;&lt;a href="https://martin-thoma.com/analysetechniken-grosser-datenbestaende/"&gt;Analysetechniken gro&amp;szlig;er Datenbest&amp;auml;nde&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://martin-thoma.com/informationsfusion/"&gt;Informationsfusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://martin-thoma.com/machine-learning-1-course/"&gt;Machine Learning 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://martin-thoma.com/machine-learning-2-course/"&gt;Machine Learning 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://martin-thoma.com/mustererkennung-klausur/"&gt;Mustererkennung&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://martin-thoma.com/neuronale-netze-vorlesung/"&gt;Neuronale Netze&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://martin-thoma.com/lma/"&gt;Lokalisierung Mobiler Agenten&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://martin-thoma.com/probabilistische-planung/"&gt;Probabilistische Planung&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Main_Page"&gt;Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.scholarpedia.org/"&gt;scholarpedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Other&lt;ul&gt;
&lt;li&gt;&lt;a href="http://alumni.media.mit.edu/~tpminka/statlearn/glossary/"&gt;alumni.media.mit.edu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://robotics.stanford.edu/~ronnyk/glossary.html"&gt;robotics.stanford.edu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.ee.columbia.edu/~vittorio/Glossary.pdf"&gt;ee.columbia.edu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cse.unsw.edu.au/~billw/mldict.html"&gt;The Machine Learning Dictionary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://37steps.com/glossary/"&gt;37steps.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.asimovinstitute.org/neural-network-zoo/"&gt;asimovinstitute.org&lt;/a&gt;: The Neural Network Zoo&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="Machine Learning"></category></entry><entry><title>Average Distance of Random Points in a Unit Hypercube</title><link href="https://martin-thoma.com/average-distance-of-points/" rel="alternate"></link><published>2016-10-20T20:00:00+02:00</published><updated>2016-10-20T20:00:00+02:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2016-10-20:/average-distance-of-points/</id><summary type="html">&lt;p&gt;In machine learning, the "curse of dimensionality" is often stated but much
less often explained. At least not in detail. One just gets told that points
are farer away from each other in high dimensional spaces.&lt;/p&gt;
&lt;h2 id="maximum-minimal-distance"&gt;Maximum minimal distance&lt;/h2&gt;
&lt;p&gt;One approach to this is to calculate the maximum minimal distance …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In machine learning, the "curse of dimensionality" is often stated but much
less often explained. At least not in detail. One just gets told that points
are farer away from each other in high dimensional spaces.&lt;/p&gt;
&lt;h2 id="maximum-minimal-distance"&gt;Maximum minimal distance&lt;/h2&gt;
&lt;p&gt;One approach to this is to calculate the maximum minimal distance of &lt;span class="math"&gt;\(k\)&lt;/span&gt; points
in &lt;span class="math"&gt;\([0, 1]^n\)&lt;/span&gt;. So you try to place &lt;span class="math"&gt;\(k\)&lt;/span&gt; points in such a way, that the minimum
over the pairwise distances of those &lt;span class="math"&gt;\(k\)&lt;/span&gt; points is maximal.
Let's call this &lt;span class="math"&gt;\(\alpha(n, k)\)&lt;/span&gt;. However, it is not easily possible
to calculate &lt;span class="math"&gt;\(\alpha(n, k)\)&lt;/span&gt; for arbitrary &lt;span class="math"&gt;\(n &amp;gt; 2\)&lt;/span&gt; and &lt;span class="math"&gt;\(k &amp;gt; 2\)&lt;/span&gt; (see &lt;a href="http://math.stackexchange.com/q/1976250/6876"&gt;link&lt;/a&gt;).
But the special case &lt;span class="math"&gt;\(k = 2\)&lt;/span&gt; and &lt;span class="math"&gt;\(k = 2^n\)&lt;/span&gt; is easy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\alpha(n, 2) = \sqrt{n}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\alpha(n, 2^n) = 1\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So you can see that two points get can be farer apart in higher dimensions and
that it needs much more points in higher dimensions to force at least two of
them to have distance 1.&lt;/p&gt;
&lt;h2 id="average-distance"&gt;Average distance&lt;/h2&gt;
&lt;p&gt;Another approach is to calculate the average distance of &lt;span class="math"&gt;\(k\)&lt;/span&gt; uniformly randomly
sampled points in &lt;span class="math"&gt;\([0, 1]^n\)&lt;/span&gt;. Let's call it &lt;span class="math"&gt;\(\beta(n, k)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;One first insight is that &lt;span class="math"&gt;\(\beta(n, k) = \beta(n, j)\)&lt;/span&gt; for and &lt;span class="math"&gt;\(k, j \geq 2\)&lt;/span&gt;.
Hence we will only use &lt;span class="math"&gt;\(\beta(n)\)&lt;/span&gt; in the following.&lt;/p&gt;
&lt;p&gt;It is possible to
calculate this, but it is rather tedious (&lt;a href="http://math.stackexchange.com/a/1254154/6876"&gt;link&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Just two calculated solutions for &lt;span class="math"&gt;\(k=2\)&lt;/span&gt; points:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\beta(1) = \frac{1}{3}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\beta(2) = \frac{2+\sqrt{2}+5\operatorname{arcsinh}(1)}{15}=\frac{2+\sqrt{2}+5\log(1+\sqrt{2})}{15} \approx 0.52140543316472\ldots\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, it is pretty easy to simulate it.&lt;/p&gt;
&lt;h2 id="density-of-hypercubes"&gt;Density of Hypercubes&lt;/h2&gt;
&lt;p&gt;One interesting question is how much of the &lt;span class="math"&gt;\(n\)&lt;/span&gt;-dimensional hypercube can be
filled by one inscribed &lt;span class="math"&gt;\(n\)&lt;/span&gt;-dimensional hyperball.&lt;/p&gt;
&lt;p&gt;The volume of an &lt;span class="math"&gt;\(n\)&lt;/span&gt;-dimensional hypercube is &lt;span class="math"&gt;\(V_C(a) = a^n\)&lt;/span&gt; where &lt;span class="math"&gt;\(a\)&lt;/span&gt; is the
cubes side length. So for 1 dimension it is &lt;span class="math"&gt;\(a\)&lt;/span&gt;, for 2 dimensions (a square) it
is &lt;span class="math"&gt;\(a^2\)&lt;/span&gt;, for 3 dimensions it is &lt;span class="math"&gt;\(a^3\)&lt;/span&gt; (a cube).&lt;/p&gt;
&lt;p&gt;The volume of an &lt;span class="math"&gt;\(n\)&lt;/span&gt;-dimensional ball is
&lt;/p&gt;
&lt;div class="math"&gt;$$V_S(r) = r^n \frac{\pi^{n/2}}{\Gamma (\frac{n}{2} + 1)}$$&lt;/div&gt;
&lt;p&gt;
Source: &lt;a href="https://en.wikipedia.org/wiki/N-sphere#Closed_forms"&gt;Wikipedia&lt;/a&gt;&lt;br/&gt;
So for 1 dimension it is &lt;span class="math"&gt;\(r \frac{\sqrt{\pi}}{\Gamma(1.5)} = r \frac{\sqrt{\pi}}{0.5 \Gamma(0.5)} = 2r\)&lt;/span&gt;,
for 2 dimensions it is &lt;span class="math"&gt;\(r^2 \frac{\pi}{\Gamma (2)} = r^2 \frac{\pi}{\Gamma (1)} = r^2 \pi\)&lt;/span&gt;
and for 3 dimensions it is &lt;span class="math"&gt;\(r^3 \frac{\pi^{3/2}}{\Gamma (\frac{5}{2})} = r^3 \frac{\pi^{3/2}}{1.5 \cdot 0.5 \cdot \Gamma (\frac{1}{2})} = r^3 \frac{\pi}{\frac{3}{4}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This means the percentage of space of a unit hypercube which can be filled by
the biggest inscribed hyperball is&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
\frac{V_S(0.5)}{V_C(1)}
&amp;amp;= \frac{r^n \frac{\pi^{n/2}}{\Gamma (\frac{n}{2} + 1)}}{1} \\
&amp;amp;= \frac{0.5^n \pi^{n/2}}{\Gamma (\frac{n}{2} + 1)} \\
&amp;amp;= \frac{0.5^n \pi^{n/2}}{\frac{n}{2} \cdot \Gamma (\frac{n}{2})} \\
&amp;amp;= \frac{0.5^n \cdot 2 \cdot \pi^{n/2}}{n \cdot \frac{2 \frac{n}{2}!}{n}} \\
&amp;amp;= \frac{0.5^n \cdot \pi^{n/2}}{\frac{n}{2}!}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;You can see that this term goes to 0 with increasing dimension. This means most
of the volume is not in the center, but in the edges of the &lt;span class="math"&gt;\(n\)&lt;/span&gt; dimensional
hypercube. It also means that &lt;span class="math"&gt;\(k\)&lt;/span&gt; nearest neighbors with Euclidean Distance
measure will need enormously large spheres to get to the next neighbours.&lt;/p&gt;
&lt;h2 id="empirical-results"&gt;Empirical results&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env python&lt;/span&gt;

&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;Get the empirical statements about the distance of two points in [0, 1]^n.&lt;/span&gt;

&lt;span class="sd"&gt;The points are uniformly randomly sampled.&lt;/span&gt;
&lt;span class="sd"&gt;"""&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy.random&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;random_points_dist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Get the distance of one sample of 2 points in [0, 1]^n."""&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;points&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Calculate the average distance of 2 points in [0, 1]^n."""&lt;/span&gt;
    &lt;span class="n"&gt;sum_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
    &lt;span class="n"&gt;count_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;
    &lt;span class="n"&gt;less_one&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;max_d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;count_&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random_points_dist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;less_one&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;max_d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;sum_&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;dist&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sum_&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;count_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;less_one&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;count_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="n"&gt;avg_dist&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;tmp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"beta(n=&lt;/span&gt;&lt;span class="si"&gt;%i&lt;/span&gt;&lt;span class="s2"&gt;) = &lt;/span&gt;&lt;span class="si"&gt;%0.4f&lt;/span&gt;&lt;span class="s2"&gt;; "&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;avg_dist&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt; Pr(d(p1, p2) &amp;lt; 1) = &lt;/span&gt;&lt;span class="si"&gt;%0.4f&lt;/span&gt;&lt;span class="s2"&gt;; alpha(n=&lt;/span&gt;&lt;span class="si"&gt;%i&lt;/span&gt;&lt;span class="s2"&gt;, 2) = &lt;/span&gt;&lt;span class="si"&gt;%0.4f&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;
          &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_d&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;One can easily see that points get spaced much farer away in average the higher
the dimension &lt;span class="math"&gt;\(n\)&lt;/span&gt; is. Now lets try to calculate the probability that two points
in the unit hypercube have a distance of less than 1.&lt;/p&gt;
&lt;p&gt;Here are a couple of results. Just a short reminder:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\alpha(n, 2)\)&lt;/span&gt; is the maximum distance two points can have in a unit cube in &lt;span class="math"&gt;\(\mathbb{R}^n\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\beta(n)\)&lt;/span&gt; is the average distance of two points in &lt;span class="math"&gt;\(\mathbb{R}^n\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(Pr(d(p_1, p_2) &amp;lt; 1)\)&lt;/span&gt; is the probability, that two uniformly randomly placed points have a distance
  of less than 1 in &lt;span class="math"&gt;\(\mathbb{R}^n\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(V_S(0.5)/V_C(1)\)&lt;/span&gt; is the amount a unit ball can fill a unit cube&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;th&gt;$n$&lt;/th&gt;
&lt;th&gt;$\alpha(n, 2)$&lt;/th&gt;
&lt;th&gt;$\beta(n)$&lt;/th&gt;
&lt;th&gt;$Pr(d(p_1, p_2) &amp;lt; 1)$&lt;/th&gt;
&lt;th&gt;$V_S(0.5)/V_C(1)$&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: right;"&gt;1&lt;/td&gt;
&lt;td&gt;0.9994&lt;/td&gt;
&lt;td&gt;0.3332&lt;/td&gt;
&lt;td&gt;1.0000&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: right;"&gt;2&lt;/td&gt;
&lt;td&gt;1.3797&lt;/td&gt;
&lt;td&gt;0.5211&lt;/td&gt;
&lt;td&gt;0.9749&lt;/td&gt;
&lt;td&gt;0.7854&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: right;"&gt;3&lt;/td&gt;
&lt;td&gt;1.6116&lt;/td&gt;
&lt;td&gt;0.6616&lt;/td&gt;
&lt;td&gt;0.9100&lt;/td&gt;
&lt;td&gt;0.5236&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: right;"&gt;4&lt;/td&gt;
&lt;td&gt;1.8130&lt;/td&gt;
&lt;td&gt;0.7776&lt;/td&gt;
&lt;td&gt;0.8066&lt;/td&gt;
&lt;td&gt;0.3084&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: right;"&gt;5&lt;/td&gt;
&lt;td&gt;1.8645&lt;/td&gt;
&lt;td&gt;0.8786&lt;/td&gt;
&lt;td&gt;0.6787&lt;/td&gt;
&lt;td&gt;0.1645&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: right;"&gt;6&lt;/td&gt;
&lt;td&gt;1.9659&lt;/td&gt;
&lt;td&gt;0.9693&lt;/td&gt;
&lt;td&gt;0.5419&lt;/td&gt;
&lt;td&gt;0.0807&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: right;"&gt;7&lt;/td&gt;
&lt;td&gt;2.0891&lt;/td&gt;
&lt;td&gt;1.0515&lt;/td&gt;
&lt;td&gt;0.4125&lt;/td&gt;
&lt;td&gt;0.0369&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: right;"&gt;8&lt;/td&gt;
&lt;td&gt;2.1513&lt;/td&gt;
&lt;td&gt;1.1280&lt;/td&gt;
&lt;td&gt;0.3006&lt;/td&gt;
&lt;td&gt;0.0159&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: right;"&gt;9&lt;/td&gt;
&lt;td&gt;2.2888&lt;/td&gt;
&lt;td&gt;1.2002&lt;/td&gt;
&lt;td&gt;0.2096&lt;/td&gt;
&lt;td&gt;0.0064&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: right;"&gt;10&lt;/td&gt;
&lt;td&gt;2.3327&lt;/td&gt;
&lt;td&gt;1.2671&lt;/td&gt;
&lt;td&gt;0.1411&lt;/td&gt;
&lt;td&gt;0.0025&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: right;"&gt;100&lt;/td&gt;
&lt;td&gt;5.2152&lt;/td&gt;
&lt;td&gt;4.0753&lt;/td&gt;
&lt;td&gt;0.0000&lt;/td&gt;
&lt;td&gt;$\approx 10^{-70}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: right;"&gt;1000&lt;/td&gt;
&lt;td&gt;14.0719&lt;/td&gt;
&lt;td&gt;12.9073&lt;/td&gt;
&lt;td&gt;0.0000&lt;/td&gt;
&lt;td&gt;$\approx 10^{-1187}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: right;"&gt;10000&lt;/td&gt;
&lt;td&gt;41.9675&lt;/td&gt;
&lt;td&gt;40.8245&lt;/td&gt;
&lt;td&gt;0.0000&lt;/td&gt;
&lt;td&gt;$\approx 10^{-16851}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: right;"&gt;$n$&lt;/td&gt;
&lt;td&gt;?&lt;/td&gt;
&lt;td&gt;$\approx 0.41 \cdot \sqrt{n}$&lt;/td&gt;
&lt;td&gt;?&lt;/td&gt;
&lt;td&gt;$\frac{0.5^n \cdot \pi^{n/2}}{\frac{n}{2}!}$&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;You can easily see that the average distance of two points gets less and less
different from the maximal distance of two points.&lt;/p&gt;
&lt;h2 id="see-also"&gt;See also&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://perso.uclouvain.be/michel.verleysen/papers/tkde07df.pdf"&gt;The Concentration of Fractional Distances&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://math.stackexchange.com/q/1976842/6876"&gt;How is the distance of two random points in a unit hypercube distributed?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality"&gt;Curse of dimensionality&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Machine Learning"></category></entry><entry><title>Iterating over Graphs</title><link href="https://martin-thoma.com/graph-iteration/" rel="alternate"></link><published>2016-10-19T20:00:00+02:00</published><updated>2016-10-19T20:00:00+02:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2016-10-19:/graph-iteration/</id><summary type="html">&lt;p&gt;Today I was thinking if one could iterate over all possible feed forward network
architectures possible. A feed forward network is essentially only a directed
acyclic graph.&lt;/p&gt;
&lt;p&gt;To make things simpler, lets just think about multilayer perceptrons. This
means we only have connections between neighboring layers (and we have layers …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today I was thinking if one could iterate over all possible feed forward network
architectures possible. A feed forward network is essentially only a directed
acyclic graph.&lt;/p&gt;
&lt;p&gt;To make things simpler, lets just think about multilayer perceptrons. This
means we only have connections between neighboring layers (and we have layers).&lt;/p&gt;
&lt;h2 id="terms"&gt;Terms&lt;/h2&gt;
&lt;p&gt;A directed acyclic graph (DAG) is a finite graph &lt;span class="math"&gt;\(G = (V, E)\)&lt;/span&gt; with vertices &lt;span class="math"&gt;\(V\)&lt;/span&gt;
and edges &lt;span class="math"&gt;\(E \subseteq V \times V\)&lt;/span&gt; where no cycle can be found. A cycle is a
set of edges &lt;span class="math"&gt;\(e_1, \dots, e_n\)&lt;/span&gt; such that &lt;span class="math"&gt;\(e_i = (v_i, v_{i+1})\)&lt;/span&gt; with &lt;span class="math"&gt;\(e_n =
(v_n, v_1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id="iteration"&gt;Iteration&lt;/h3&gt;
&lt;p&gt;Now, what does it mean to iterate over the graphs?&lt;/p&gt;
&lt;p&gt;One can iterate over all natural numbers like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Resulting in the sequence &lt;span class="math"&gt;\(1, 2, 3, \dots\)&lt;/span&gt; which is guaranteed to reach any
natural number &lt;span class="math"&gt;\(k \in \mathbb{N}\)&lt;/span&gt; at some point.&lt;/p&gt;
&lt;p&gt;Similar, one can iterate over &lt;span class="math"&gt;\(\mathbb{Q}_0^+\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;this is the pattern in which the numbers are generated:&lt;/p&gt;
&lt;figure class="wp-caption aligncenter img-thumbnail"&gt;
&lt;img alt="Iterate over Q+" src="../images/2016/10/iterate-q.png"/&gt;
&lt;figcaption class="text-center"&gt;Iterate over $\mathbb{Q}_0^+$&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id="graph-iteration_1"&gt;Graph iteration&lt;/h2&gt;
&lt;p&gt;There are finitely many directed graphs &lt;span class="math"&gt;\(G = (V, E)\)&lt;/span&gt; with &lt;span class="math"&gt;\(E \subseteq V \times V\)&lt;/span&gt; with &lt;span class="math"&gt;\(n = |V|\)&lt;/span&gt; nodes. So if one wanted to iterate over all of them, one could iterate over all graphs with &lt;span class="math"&gt;\(n=1\)&lt;/span&gt; node (only one graph), then over all graphs with two nodes, ...&lt;/p&gt;
&lt;p&gt;At this point, it should be obvious that it is possible. However, how can we
iterate over the structures without unnecessary duplication? How can we take
into account that we also need connected graphs with a fixed size for the
first layer and a fixed size for the last layer?&lt;/p&gt;
&lt;p&gt;Similar to the iteration over &lt;span class="math"&gt;\(\mathbb{Q}_0^+\)&lt;/span&gt;, you first generate all MLPs
with exactly one neuron. Then all MLPs with exactly two neurons, ...&lt;/p&gt;
&lt;p&gt;To generate all MLPs with exactly &lt;span class="math"&gt;\(n\)&lt;/span&gt; neurons in &lt;span class="math"&gt;\(k\)&lt;/span&gt; layers, you first generate
all MLPs with &lt;span class="math"&gt;\(n-i\)&lt;/span&gt; neurons in &lt;span class="math"&gt;\(k-1\)&lt;/span&gt; layers and add &lt;span class="math"&gt;\(i\)&lt;/span&gt; neurons to the &lt;span class="math"&gt;\(k\)&lt;/span&gt;-th
layer:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/python&lt;/span&gt;
&lt;span class="c1"&gt;# -*- coding: utf-8 -*-&lt;/span&gt;

&lt;span class="sd"&gt;"""Iterate over all MLPs."""&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gen_vecs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Generate all integer vectors of length k which sums up to n."""&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"n=&lt;/span&gt;&lt;span class="si"&gt;%i&lt;/span&gt;&lt;span class="s2"&gt; AND k=&lt;/span&gt;&lt;span class="si"&gt;%i&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;xs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gen_vecs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
                &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gen_all_mlps_size_n&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    Generate all ways to have hidden layers with exactly n nodes.&lt;/span&gt;

&lt;span class="sd"&gt;    Make it by the number of hidden layers. First one hidden layer, then two,&lt;/span&gt;
&lt;span class="sd"&gt;    ... until n hidden layers with each exactly one neuron.&lt;/span&gt;

&lt;span class="sd"&gt;    There are 2^{n-1} of those.&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;vec_generator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gen_vecs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;vec&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;vec_generator&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;vec&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gen_all_mlps&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;"""Generate all MLPs."""&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;neurons&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;gen&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gen_all_mlps_size_n&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;neurons&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;graph&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;gen&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;graph&lt;/span&gt;

&lt;span class="c1"&gt;# Just for fun, generate the first 100 graphs:&lt;/span&gt;
&lt;span class="n"&gt;gen&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gen_all_mlps&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gen&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="Machine Learning"></category></entry><entry><title>Diverging Gradient Descent</title><link href="https://martin-thoma.com/diverging-gradient-descent/" rel="alternate"></link><published>2016-07-21T16:00:00+02:00</published><updated>2016-07-21T16:00:00+02:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2016-07-21:/diverging-gradient-descent/</id><summary type="html">&lt;p&gt;When you take the function&lt;/p&gt;
&lt;div class="math"&gt;$$f(x, y) = 3x^2 + 3y^2 + 2xy$$&lt;/div&gt;
&lt;p&gt;and start gradient descent at &lt;span class="math"&gt;\(x_0 = (6, 6)\)&lt;/span&gt; with learning rate &lt;span class="math"&gt;\(\eta = \frac{1}{2}\)&lt;/span&gt;
it diverges.&lt;/p&gt;
&lt;h2 id="gradient-descent"&gt;Gradient descent&lt;/h2&gt;
&lt;p&gt;Gradient descent is an optimization rule which starts at a point &lt;span class="math"&gt;\(x_0\)&lt;/span&gt; and
then applies the update rule …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When you take the function&lt;/p&gt;
&lt;div class="math"&gt;$$f(x, y) = 3x^2 + 3y^2 + 2xy$$&lt;/div&gt;
&lt;p&gt;and start gradient descent at &lt;span class="math"&gt;\(x_0 = (6, 6)\)&lt;/span&gt; with learning rate &lt;span class="math"&gt;\(\eta = \frac{1}{2}\)&lt;/span&gt;
it diverges.&lt;/p&gt;
&lt;h2 id="gradient-descent"&gt;Gradient descent&lt;/h2&gt;
&lt;p&gt;Gradient descent is an optimization rule which starts at a point &lt;span class="math"&gt;\(x_0\)&lt;/span&gt; and
then applies the update rule&lt;/p&gt;
&lt;div class="math"&gt;$$x_{k+1} = x_k + \eta d_k(x_k)$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\eta\)&lt;/span&gt; is the step length (learning rate) and &lt;span class="math"&gt;\(d_k\)&lt;/span&gt; is the direction.&lt;/p&gt;
&lt;p&gt;The direction is&lt;/p&gt;
&lt;div class="math"&gt;$$d_k(x_k) = - \nabla f(x_k)$$&lt;/div&gt;
&lt;h2 id="example"&gt;Example&lt;/h2&gt;
&lt;div class="math"&gt;$$\nabla f(x, y) = \begin{pmatrix}6x + 2y\\6y + 2x\end{pmatrix}$$&lt;/div&gt;
&lt;div class="math"&gt;\begin{align}
x_0 &amp;amp;= (6, 6)       &amp;amp; d_k(6, 6)       &amp;amp;= (-24, -24)\\
x_1 &amp;amp;= (-18, -18)   &amp;amp; d_k(-18, -18)   &amp;amp;= (72, 72\\
x_2 &amp;amp;= (54, 54)     &amp;amp; d_k(54, 54)     &amp;amp;= (-216, -216)\\
x_3 &amp;amp;= (-162, -162) &amp;amp; d_k(-162, -162) &amp;amp;= (648, 648)
\end{align}&lt;/div&gt;
&lt;p&gt;In general:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
x_n &amp;amp;= (x_{n-1} - 8 \cdot \frac{1}{2} \cdot x_{n-1}, x_{n-1} - 8 \cdot \frac{1}{2} \cdot x_{n-1})\\
x_n &amp;amp;= (-3x_{n-1}, -3x_{n-1})
\end{align}&lt;/div&gt;
&lt;p&gt;You can clearly see that any learning rate &lt;span class="math"&gt;\(\eta &amp;gt; \frac{1}{8}\)&lt;/span&gt; will diverge.
For this example, the learning rate &lt;span class="math"&gt;\(\eta = \frac{1}{8}\)&lt;/span&gt; would find the
solution in one step and any &lt;span class="math"&gt;\(\eta &amp;lt; \frac{1}{8}\)&lt;/span&gt; will converge to the global
optimum.&lt;/p&gt;</content><category term="Gradient Descent"></category><category term="Optimization"></category></entry><entry><title>XOR tutorial with TensorFlow</title><link href="https://martin-thoma.com/tf-xor-tutorial/" rel="alternate"></link><published>2016-07-19T14:00:00+02:00</published><updated>2016-07-19T14:00:00+02:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2016-07-19:/tf-xor-tutorial/</id><summary type="html">&lt;p&gt;The XOR-Problem is a classification problem, where you only have four data
points with two features. The training set and the test set are exactly
the same in this problem. So the interesting question is only if the model is
able to find a decision boundary which classifies all four …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The XOR-Problem is a classification problem, where you only have four data
points with two features. The training set and the test set are exactly
the same in this problem. So the interesting question is only if the model is
able to find a decision boundary which classifies all four points correctly.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter img-thumbnail"&gt;
&lt;img alt="The XOR classification problem. 4 datapoints and two classes. All datapoints have 2 features." src="../images/2016/07/xor-problem.png"/&gt;
&lt;figcaption class="text-center"&gt;The XOR classification problem. 4 datapoints and two classes. All datapoints have 2 features.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id="neural-network-basics"&gt;Neural Network basics&lt;/h2&gt;
&lt;p&gt;I think of neural networks as a construction kit for functions. The basic building block - called a "neuron" - is usually visualized like this:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://i.stack.imgur.com/YD9IS.png"&gt;&lt;img alt="enter image description here" src="http://i.stack.imgur.com/YD9IS.png"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It gets a variable number of inputx &lt;span class="math"&gt;\(x_0, x_1, \dots, x_n\)&lt;/span&gt;, they get multiplied with weights &lt;span class="math"&gt;\(w_0, w_1, \dots, w_n\)&lt;/span&gt;, summed and a function &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt; is applied to it. The weights is what you want to "fine tune" to make it actually work. When you have more of those neurons, you visualize it like this:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://i.stack.imgur.com/awAz8.png"&gt;&lt;img alt="enter image description here" src="http://i.stack.imgur.com/awAz8.png"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this example, it is only one output and 5 inputs, but it could be any number. The number of inputs and outputs is usually defined by your problem, the intermediate is to allow it to fit more exact to what you need (which comes with some other implications).&lt;/p&gt;
&lt;p&gt;Now you have some structure of the function set, you need to find weights which work. This is where backpropagation&lt;sup id="fnref-3"&gt;&lt;a class="footnote-ref" href="#fn-3"&gt;3&lt;/a&gt;&lt;/sup&gt; comes into play. The idea is the following: You took functions (&lt;span class="math"&gt;\(\varphi\)&lt;/span&gt;) which were differentiable and combined them in a way which makes sure the complete function is differentiable. Then you apply an error function (e.g. the euclidean distance of the output to the desired output, Cross-Entropy) which is also differentiable. Meaning you have a completely differentiable function. Now you see the weights as variables and the data as given parameters of a HUGE function. You can differentiate (calculate the gradient) and go from your random weights "a step" in the direction where the error gets lower. This adjusts your weights. Then you repeat this steepest descent step and hopefully end up some time with a good function.&lt;/p&gt;
&lt;p&gt;For two weights, this awesome image by Alec Radford visualizes how different algorithms based on gradient descent find a minimum (&lt;a href="http://imgur.com/a/Hqolp"&gt;Source&lt;/a&gt; with even more of those):&lt;/p&gt;
&lt;p&gt;&lt;a href="http://i.stack.imgur.com/ocZHU.gif"&gt;&lt;img alt="enter image description here" src="http://i.stack.imgur.com/ocZHU.gif"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So think of back propagation as a shortsighted hiker trying to find the lowest point on the error surface: He only sees what is directly in front of him. As he makes progress, he adjusts the direction in which he goes.&lt;/p&gt;
&lt;h2 id="targets-and-error-function"&gt;Targets and Error function&lt;/h2&gt;
&lt;p&gt;First of all, you should think about how your targets look like. For
classification problems, one usually takes as many output neurons as one has
classes. Then the softmax function is applied.&lt;sup id="fnref-1"&gt;&lt;a class="footnote-ref" href="#fn-1"&gt;1&lt;/a&gt;&lt;/sup&gt; The softmax function makes sure that the output of every single neuron is in &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt; and the sum of all outputs is exactly &lt;span class="math"&gt;\(1\)&lt;/span&gt;. This means the output can be interpreted as a probability distribution over all classes.&lt;/p&gt;
&lt;p&gt;Now you have to adjust your targets. It is likely that you only have a list of labels, where the &lt;span class="math"&gt;\(i\)&lt;/span&gt;-th element in the list is the label for the &lt;span class="math"&gt;\(i\)&lt;/span&gt;-th element in your feature list &lt;span class="math"&gt;\(X\)&lt;/span&gt; (or the &lt;span class="math"&gt;\(i\)&lt;/span&gt;-th row in your feature matrix &lt;span class="math"&gt;\(X\)&lt;/span&gt;). But the tools need a target value which fits to the error function. The usual error function for classification problems is cross entropy (CE). When you have a list of &lt;span class="math"&gt;\(n\)&lt;/span&gt; features &lt;span class="math"&gt;\(x\)&lt;/span&gt;, the target &lt;span class="math"&gt;\(t\)&lt;/span&gt; and a classifier &lt;span class="math"&gt;\(clf\)&lt;/span&gt;, then you calculate the cross entropy loss for this single sample by:&lt;/p&gt;
&lt;div class="math"&gt;$$CE(x, t) = - \sum_{i=1}^n \left (t^{(i)} \log \left ({clf(x)}^{(i)} \right ) \right)$$&lt;/div&gt;
&lt;p&gt;Now we need a target value for each single neuron for every sample &lt;span class="math"&gt;\(x\)&lt;/span&gt;. We get those by so called &lt;em&gt;one hot encoding&lt;/em&gt;: The &lt;span class="math"&gt;\(k\)&lt;/span&gt; classes all have their own neuron. If a sample &lt;span class="math"&gt;\(x\)&lt;/span&gt; is of class &lt;span class="math"&gt;\(i\)&lt;/span&gt;, then the &lt;span class="math"&gt;\(i\)&lt;/span&gt;-th neuron should give &lt;span class="math"&gt;\(1\)&lt;/span&gt; and all others should give &lt;span class="math"&gt;\(0\)&lt;/span&gt;.&lt;sup id="fnref-2"&gt;&lt;a class="footnote-ref" href="#fn-2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sklearn&lt;/code&gt; provides a very useful &lt;code&gt;OneHotEncoder&lt;/code&gt; class. You first have to fit
it on your labels (e.g. just give it all of them). In the next step you can
transform a list of labels to an array of one-hot encoded targets:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env python&lt;/span&gt;

&lt;span class="sd"&gt;"""Mini-demo how the one hot encoder works."""&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;OneHotEncoder&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="c1"&gt;# The most intuitive way to label a dataset "X"&lt;/span&gt;
&lt;span class="c1"&gt;# (list of features, where X[i] are the features for a datapoint i)&lt;/span&gt;
&lt;span class="c1"&gt;# is to have a flat list "labels" where labels[i] is the label for datapoint i.&lt;/span&gt;
&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# The OneHotEncoder transforms those labels to something our models can&lt;/span&gt;
&lt;span class="c1"&gt;# work with&lt;/span&gt;
&lt;span class="n"&gt;enc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;OneHotEncoder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;trans_for_ohe&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Transform a flat list of labels to what one hot encoder needs."""&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;labels_r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;trans_for_ohe&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# The encoder has to know how many classes there are and what their names are.&lt;/span&gt;
&lt;span class="n"&gt;enc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels_r&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Now you can transform&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;enc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trans_for_ohe&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toarray&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tolist&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="install-tensorflow"&gt;Install Tensorflow&lt;/h2&gt;
&lt;p&gt;The documentation about the installation makes a VERY good impression. Better
than anything I can write in a few minutes, so ... &lt;a href="http://tensorflow.org/get_started/os_setup.md"&gt;RTFM&lt;/a&gt;
 😜&lt;/p&gt;
&lt;p&gt;For Linux systems with CUDA and without root privileges, you can install it
with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl --user
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;But remember you have to set the environment variable &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; and
&lt;code&gt;CUDA_HOME&lt;/code&gt;. For many configurations, adding the following lines to your
&lt;code&gt;.bashrc&lt;/code&gt; will work:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;LD_LIBRARY_PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="nv"&gt;$LD_LIBRARY_PATH&lt;/span&gt;&lt;span class="s2"&gt;:/usr/local/cuda/lib64"&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;CUDA_HOME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/usr/local/cuda
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I currently (19.07.2016) to use Tensorflow rc0.7 (&lt;a href="https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html"&gt;installation instructions&lt;/a&gt;) with CUDA 7.5 (&lt;a href="http://askubuntu.com/a/799185/10425"&gt;installation instructions&lt;/a&gt;). I had a couple
of problems with other versions (e.g. &lt;a href="https://github.com/tensorflow/tensorflow/issues/3342"&gt;#3342&lt;/a&gt;, &lt;a href="https://github.com/tensorflow/tensorflow/issues/2810"&gt;#2810&lt;/a&gt;, &lt;a href="https://github.com/tensorflow/tensorflow/issues/2034"&gt;#2034&lt;/a&gt;, but that might only have been bad luck. Who knows.).&lt;/p&gt;
&lt;h2 id="tensorflow-basics"&gt;Tensorflow basics&lt;/h2&gt;
&lt;p&gt;Tensorflow helps you to define the neural network in a symbolic way. This means you do not explicitly tell the computer what to compute to inference with the neural network, but you tell it how the data flow works. This symbolic representation of the computation can then be used to automatically caluclate the derivates. This is awesome! So you don't have to make this your own. But keep it in mind that it is only symbolic as this makes a few things more complicated and different from what you might be used to.&lt;/p&gt;
&lt;p&gt;Tensorflow has &lt;em&gt;placeholders&lt;/em&gt; and &lt;em&gt;variables&lt;/em&gt;. Placeholders are the things in which
you later put your input. This is your features and your targets, but might be
also include more. Variables are the things the optimizer calculates.&lt;/p&gt;
&lt;p&gt;Now you should be able to understand the following code which solves the XOR
problem. It defines a neural network with two input neurons, 2&amp;nbsp;neurons in
a first hidden layer and 2&amp;nbsp;output neurons. All neurons have biases.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env python&lt;/span&gt;

&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;Solve the XOR problem with Tensorflow.&lt;/span&gt;

&lt;span class="sd"&gt;The XOR problem is a two-class classification problem. You only have four&lt;/span&gt;
&lt;span class="sd"&gt;datapoints, all of which are given during training time. Each datapoint has&lt;/span&gt;
&lt;span class="sd"&gt;two features:&lt;/span&gt;

&lt;span class="sd"&gt;      x    o&lt;/span&gt;

&lt;span class="sd"&gt;      o    x&lt;/span&gt;

&lt;span class="sd"&gt;As you can see, the classifier has to learn a non-linear transformation of&lt;/span&gt;
&lt;span class="sd"&gt;the features to find a propper decision boundary.&lt;/span&gt;
&lt;span class="sd"&gt;"""&lt;/span&gt;

&lt;span class="n"&gt;__author__&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"Martin Thoma"&lt;/span&gt;
&lt;span class="n"&gt;__email__&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"info@martin-thoma.de"&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;OneHotEncoder&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;trans_for_ohe&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Transform a flat list of labels to what one hot encoder needs."""&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;analyze_classifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;XOR_X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;XOR_T&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Visualize the classification."""&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;Epoch &lt;/span&gt;&lt;span class="si"&gt;%i&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Hypothesis &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                     &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;input_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;XOR_X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;XOR_T&lt;/span&gt;&lt;span class="p"&gt;}))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'w1=&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'b1=&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'w2=&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'b2=&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'cost (ce)=&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cross_entropy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                    &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;input_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;XOR_X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                               &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;XOR_T&lt;/span&gt;&lt;span class="p"&gt;}))&lt;/span&gt;
    &lt;span class="c1"&gt;# Visualize classification boundary&lt;/span&gt;
    &lt;span class="n"&gt;xs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ys&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pred_classes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;ys&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;pred_class&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                  &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;input_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;]]})&lt;/span&gt;
            &lt;span class="n"&gt;pred_classes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pred_class&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;
    &lt;span class="n"&gt;xs_p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ys_p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;xs_n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ys_n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;pred_classes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;xs_n&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;ys_n&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;xs_p&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;ys_p&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs_p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ys_p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'ro'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xs_n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ys_n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'bo'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;


&lt;span class="c1"&gt;# The training data&lt;/span&gt;
&lt;span class="n"&gt;XOR_X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;  &lt;span class="c1"&gt;# Features&lt;/span&gt;
&lt;span class="n"&gt;XOR_Y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  &lt;span class="c1"&gt;# Class labels&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;XOR_X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;XOR_Y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# sanity check&lt;/span&gt;

&lt;span class="c1"&gt;# Transform labels to targets&lt;/span&gt;
&lt;span class="n"&gt;enc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;OneHotEncoder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;enc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trans_for_ohe&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;XOR_Y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;XOR_T&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;enc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trans_for_ohe&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;XOR_Y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toarray&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# The network&lt;/span&gt;
&lt;span class="n"&gt;nb_classes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;input_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;XOR_X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])],&lt;/span&gt;
                        &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"input"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nb_classes&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                        &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"output"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;nb_hidden_nodes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="c1"&gt;# enc = tf.one_hot([0, 1], 2)&lt;/span&gt;
&lt;span class="n"&gt;w1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_uniform&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nb_hidden_nodes&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                 &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Weights1"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;w2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_uniform&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;nb_hidden_nodes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nb_classes&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                   &lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                 &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Weights2"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;b1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;nb_hidden_nodes&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Biases1"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;b2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;nb_classes&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Biases2"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;activation2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;hypothesis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;activation2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cross_entropy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;train_step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cross_entropy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Start training&lt;/span&gt;
&lt;span class="n"&gt;init&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize_all_variables&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;20001&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_step&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;input_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;XOR_X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;XOR_T&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;analyze_classifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;XOR_X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;XOR_T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The output is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Epoch 0
Hypothesis [[ 0.48712057  0.51287943]
 [ 0.3380821   0.66191792]
 [ 0.65063184  0.34936813]
 [ 0.50317246  0.4968276 ]]
w1=[[-0.79593647  0.93947881]
 [ 0.68854761 -0.89423609]]
b1=[-0.00733338  0.00893857]
w2=[[-0.79084051  0.93289936]
 [ 0.69278169 -0.8986907 ]]
b2=[ 0.00394399 -0.00394398]
cost (ce)=2.87031

Epoch 10000
Hypothesis [[ 0.99773693  0.00226305]
 [ 0.00290442  0.99709558]
 [ 0.00295531  0.99704474]
 [ 0.99804318  0.00195681]]
w1=[[-6.62694693  7.5230279 ]
 [ 6.91208076 -7.39292192]]
b1=[ 3.32245016  3.76204181]
w2=[[ 6.63465023 -6.49259233]
 [ 6.40471792 -6.61061859]]
b2=[-9.65064621  9.65065193]
cost (ce)=0.0100926

Epoch 20000
Hypothesis [[  9.98954773e-01   1.04520109e-03]
 [  1.35455502e-03   9.98645484e-01]
 [  1.37042452e-03   9.98629570e-01]
 [  9.99092221e-01   9.07782756e-04]]
w1=[[-7.04857063  7.84673214]
 [ 7.33061123 -7.68837786]]
b1=[ 3.53246188  3.89587545]
w2=[[ 7.35948515 -7.21742725]
 [ 7.14059925 -7.34649038]]
b2=[-10.74944687  10.74944115]
cost (ce)=0.00468077
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The resulting decision boundary looks like this:&lt;/p&gt;
&lt;figure class="wp-caption aligncenter img-thumbnail"&gt;
&lt;img alt="Decision boundary of the trained network." src="../images/2016/07/xor-classification.png"/&gt;
&lt;figcaption class="text-center"&gt;Decision boundary of the trained network.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;I recommend reading the &lt;a href="http://download.tensorflow.org/paper/whitepaper2015.pdf"&gt;Tensorflow Whitepaper&lt;/a&gt; if you want to understand Tensorflow better.&lt;/p&gt;
&lt;h2 id="footnotes"&gt;Footnotes&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn-1"&gt;
&lt;p&gt;Softmax is similar to the sigmoid function, but with normalization.&amp;nbsp;&lt;a class="footnote-backref" href="#fnref-1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-2"&gt;
&lt;p&gt;Actually, we don't want this. The probability of any class should never be exactly zero as this might cause problems later. It might get very very small, but should never be 0.&amp;nbsp;&lt;a class="footnote-backref" href="#fnref-2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-3"&gt;
&lt;p&gt;Backpropagation is only a clever implementation of gradient descent. It belongs to the bigger class of iterative descent algorithms.&amp;nbsp;&lt;a class="footnote-backref" href="#fnref-3" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/hr&gt;&lt;/div&gt;</content><category term="Machine Learning"></category><category term="Python"></category><category term="Tensorflow"></category><category term="sklearn"></category></entry><entry><title>Optimization Basics</title><link href="https://martin-thoma.com/optimization-basics/" rel="alternate"></link><published>2016-07-06T20:00:00+02:00</published><updated>2016-07-06T20:00:00+02:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2016-07-06:/optimization-basics/</id><summary type="html">&lt;p&gt;Optimization is a subfield of mathematics / computer science which deals with finding the best solution. Typically, problems in optimization are stated like this:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
&amp;amp;\underset{x}{\operatorname{minimize}}&amp;amp; &amp;amp; f(x) \\
&amp;amp;\operatorname{subject\;to}
&amp;amp; &amp;amp;g_i(x) \leq 0, \quad i = 1,\dots,m \\
&amp;amp;&amp;amp;&amp;amp;h_i(x) = 0, \quad i = 1, \dots …&lt;/div&gt;</summary><content type="html">&lt;p&gt;Optimization is a subfield of mathematics / computer science which deals with finding the best solution. Typically, problems in optimization are stated like this:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
&amp;amp;\underset{x}{\operatorname{minimize}}&amp;amp; &amp;amp; f(x) \\
&amp;amp;\operatorname{subject\;to}
&amp;amp; &amp;amp;g_i(x) \leq 0, \quad i = 1,\dots,m \\
&amp;amp;&amp;amp;&amp;amp;h_i(x) = 0, \quad i = 1, \dots,p
\end{align}
$$&lt;/div&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(f(x): \mathbb{R}^n \to \mathbb{R}\)&lt;/span&gt; is the &lt;strong&gt;loss function (objective function)&lt;/strong&gt; to be minimized over the variable &lt;span class="math"&gt;\(x\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(g_i(x) \leq 0\)&lt;/span&gt; are called &lt;strong&gt;inequality constraints&lt;/strong&gt;, and&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(h_i(x) = 0\)&lt;/span&gt; are called &lt;strong&gt;equality constraints&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By convention, the standard form defines a &lt;strong&gt;minimization problem&lt;/strong&gt;. A
&lt;strong&gt;maximization problem&lt;/strong&gt; can be treated by negating the objective function.&lt;/p&gt;
&lt;p&gt;(That was copied from &lt;a href="https://en.wikipedia.org/w/index.php?title=Optimization_problem&amp;amp;oldid=715562612#Continuous_optimization_problem"&gt;en.wikipedia.org/w/Optimization_problem&lt;/a&gt; and only slightly edited.)&lt;/p&gt;
&lt;p&gt;I'm now going to explain some very basic techniques which are used for finding good solutions to optimization problems. Please note that there are also discrete optimization problems where you have to finde a solution &lt;span class="math"&gt;\(x \in \mathbb{N}^n\)&lt;/span&gt;. I will only focus on continuous optimization problems.&lt;/p&gt;
&lt;h2 id="simulated-annealing"&gt;Simulated Annealing&lt;/h2&gt;
&lt;p&gt;Simulated Annealing is a heuristical optimization algorithm. It starts at a
random point &lt;span class="math"&gt;\(x \in \mathbb{R}^n\)&lt;/span&gt;. Then it takes a random point &lt;span class="math"&gt;\(y\)&lt;/span&gt; of the
environment of &lt;span class="math"&gt;\(x\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$y \in U(x)$$&lt;/div&gt;
&lt;p&gt;If &lt;span class="math"&gt;\(f(y) \leq f(x)\)&lt;/span&gt;, then the current position &lt;span class="math"&gt;\(x\)&lt;/span&gt; is overwritten with &lt;span class="math"&gt;\(y\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$x \leftarrow y$$&lt;/div&gt;
&lt;p&gt;Otherwise, it might be overwritten with probability &lt;span class="math"&gt;\(\exp \left (-\frac{f(y)-f(x)}{T(t)} \right )\)&lt;/span&gt; where &lt;span class="math"&gt;\(T: \mathbb{N}_0 \rightarrow \mathbb{R}_{&amp;gt; 0}\)&lt;/span&gt; is called the temperature at time &lt;span class="math"&gt;\(t\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So the optimization algorithm is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Take a random point $x \in \mathbb{R}^n$&lt;/li&gt;
&lt;li&gt;Take a random point $y \in U(x)$&lt;/li&gt;
&lt;li&gt;$$x \leftarrow \begin{cases}y &amp;amp;\text{if } f(y) \leq f(x)\\
                          y &amp;amp;\text{if } \operatorname{rand}(0,1) &amp;lt; \exp \left (-\frac{f(y)-f(x)}{T(t)} \right )\\
                          x &amp;amp;\text{otherwise}\end{cases}$$&lt;/li&gt;
&lt;li&gt;Go to step 2.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;See also my &lt;a href="https://martin-thoma.com/neuronale-netze-vorlesung/#simulated-annealing"&gt;German description&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="gradient-descent"&gt;Gradient descent&lt;/h2&gt;
&lt;p&gt;The gradient descent algorithm can easily be applied when the optimization problem has no constraints and the objective function &lt;span class="math"&gt;\(f\)&lt;/span&gt; is differentiable.
The idea is to just take a random starting point &lt;span class="math"&gt;\(x \in \mathbb{R}^n\)&lt;/span&gt; and
iteratively improve it. There are many algorithms which follow this approach (Simulated annealing, L-BFGS, Newton's method, Quasi-Newtonian, Conjugate Gradient, ...).&lt;/p&gt;
&lt;p&gt;Instead of randomly going in other directions, the
gradient &lt;span class="math"&gt;\(\nabla f\)&lt;/span&gt; is calculated at the position &lt;span class="math"&gt;\(x\)&lt;/span&gt;. The gradient points
in the direction of maximum increase, so we go in the opposite direction:&lt;/p&gt;
&lt;div class="math"&gt;$$x_{\text{new}} = x - \nabla f(x)$$&lt;/div&gt;
&lt;p&gt;The problem with this approach is that the surface of the objective function
might first go down in the direction of &lt;span class="math"&gt;\(\nabla f(x)\)&lt;/span&gt;, but if you go a bit
further it can go up by a lot. So we want to make very small steps. To achive
this, we multiply the gradient with a factor &lt;span class="math"&gt;\(\eta \in (0, 1]\)&lt;/span&gt;. In machine
learining, this &lt;span class="math"&gt;\(\eta\)&lt;/span&gt; is called the &lt;em&gt;learning rate&lt;/em&gt; and typically one
chooses &lt;span class="math"&gt;\(\eta = 0.01\)&lt;/span&gt;. However, there are &lt;a href="https://martin-thoma.com/neuronale-netze-vorlesung/#learning-rate-scheduling"&gt;learning rate scheduling algorithms&lt;/a&gt; which adapt this parameter during training.&lt;/p&gt;
&lt;p&gt;The update rule is:&lt;/p&gt;
&lt;div class="math"&gt;$$x_{\text{new}} = x - \eta \nabla f(x)$$&lt;/div&gt;
&lt;h3 id="iterative-descent"&gt;Iterative Descent&lt;/h3&gt;
&lt;p&gt;A more general formulation of the Gradient descent algorithm is called
iterative descent. The idea is to start at some arbitrary &lt;span class="math"&gt;\(x_0\)&lt;/span&gt; and iteratively
update the current guess of the minimum to&lt;/p&gt;
&lt;div class="math"&gt;$$x_{k+1} = x_k + \eta \cdot d_k$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\eta \in (0, 1]\)&lt;/span&gt; is the step length (learning rate) and&lt;/p&gt;
&lt;div class="math"&gt;$$d_k = -D_k \nabla f(x_k)$$&lt;/div&gt;
&lt;p&gt;is the direction of the descent. The direction depends on the Gradient
&lt;span class="math"&gt;\(\nabla f(x_k)\)&lt;/span&gt;, but also on a matrix &lt;span class="math"&gt;\(D_k\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$D_k = I$: Gradient descent&lt;/li&gt;
&lt;li&gt;$D_k = H_f^{-1}(x_k)$: Newtons method, where $H_f$ is the &lt;a href="https://en.wikipedia.org/wiki/Hessian_matrix"&gt;Hessian matrix&lt;/a&gt; of $f$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="linear-regression-with-mse_1"&gt;Linear Regression with MSE&lt;/h2&gt;
&lt;p&gt;In linear regression one is given a list of &lt;span class="math"&gt;\(n\)&lt;/span&gt; points &lt;span class="math"&gt;\((x, y)\)&lt;/span&gt; with &lt;span class="math"&gt;\(x \in \mathbb{R}^m\)&lt;/span&gt; and &lt;span class="math"&gt;\(y \in \mathbb{R}\)&lt;/span&gt;. The task is to find a matrix &lt;span class="math"&gt;\(A \in \mathbb{1 \times m}\)&lt;/span&gt; such that the predicted value &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; of the linear model&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{y}(x) = A \cdot x$$&lt;/div&gt;
&lt;p&gt;minimizes the term&lt;/p&gt;
&lt;div class="math"&gt;$$\text{MSE} = \sum_{i=1}^n (y_i - \hat{y}(x_i))^2$$&lt;/div&gt;
&lt;p&gt;For convenience, one can write the list of points as a matrix &lt;span class="math"&gt;\(\mathbf{X} \in \mathbb{R}^{n \times m}\)&lt;/span&gt; and
a vector &lt;span class="math"&gt;\(\mathbf{y} \in \mathbb{R}^n\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$\text{MSE} = \|\mathbf{y} - \mathbf{X} A^T\|_2$$&lt;/div&gt;
&lt;p&gt;with the Euclidean norm&lt;/p&gt;
&lt;div class="math"&gt;$$\| v \|_2 := \sqrt{ ( v_1 )^2 + ( v_2 )^2 + \dotsb + ( v_n )^2 } = \left( \sum_{i=1}^n ( v_i )^2 \right)^{1/2}$$&lt;/div&gt;
&lt;p&gt;Every part of the sum is non-negative, so exponentiating the Euclidean norm
with a positive factor will not change the result of the minimization:&lt;/p&gt;
&lt;div class="math"&gt;$$\operatorname{minimize}_{A} \|\mathbf{y} - \mathbf{X} A^T\|_2^2$$&lt;/div&gt;
&lt;p&gt;Now we can see that this is every element of the vector squared. So we can
get rid of the norm and then use distributivity:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
\operatorname{minimize}_{A}&amp;amp;(\mathbf{y} - \mathbf{X} A^T)^T \cdot (\mathbf{y} - \mathbf{X} A^T)\\
\Leftrightarrow \operatorname{minimize}_{A}&amp;amp;(\mathbf{y}^T - A \mathbf{X}^T) \cdot (\mathbf{y} - \mathbf{X} A^T)\\
\Leftrightarrow \operatorname{minimize}_{A}&amp;amp;\mathbf{y}^T \mathbf{y} - A \mathbf{X}^T \mathbf{y} - \mathbf{y}^T \mathbf{X} A^T + A\mathbf{X}^T \mathbf{X} A^T\\
\end{align}
$$&lt;/div&gt;
&lt;p&gt;You need to know that&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
A \mathbf{X}^T \mathbf{y} &amp;amp;= ((A \mathbf{X}^T \mathbf{y})^T)^T\\
&amp;amp;= (\mathbf{y}^T \mathbf{X} A^T)^T\\
&amp;amp;= \mathbf{y}^T \mathbf{X} A^T\\
\end{align}
$$&lt;/div&gt;
&lt;p&gt;You can get rid of the last transposing operation, because
&lt;/p&gt;
&lt;div class="math"&gt;$$(\mathbf{y}^T \mathbf{X} A^T) \in \mathbb{R}^{1 \times 1}$$&lt;/div&gt;
&lt;p&gt;This simplifies the optimization problem to&lt;/p&gt;
&lt;div class="math"&gt;$$\operatorname{minimize}_{A}\underbrace{\mathbf{y}^T \mathbf{y} - 2 A \mathbf{X}^T \mathbf{y} + A\mathbf{X}^T \mathbf{X} A^T}_{E_{X, y}(A)}$$&lt;/div&gt;
&lt;p&gt;Now you can calculate the gradient of this term with respect to &lt;span class="math"&gt;\(A\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$\nabla E_{X, y}(A) = 2 X^T X A^T - X^T y - X^T y = 2 X^T (X A^T - y)$$&lt;/div&gt;
&lt;p&gt;A necessary condition of a minimum is the gradient to be 0:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
\nabla E_{X, y}(A) &amp;amp;\overset{!}{=} 0\\
\Leftrightarrow 0 &amp;amp;\overset{!}{=} 2 X^T (X A^T - y)\\
\Leftrightarrow 0 &amp;amp;\overset{!}{=} X^T X A^T - X^T y\\
\Leftrightarrow A &amp;amp;\overset{!}{=} ((X^T X)^{-1} X^T y)^T\\
\end{align}
$$&lt;/div&gt;
&lt;p&gt;As &lt;span class="math"&gt;\(H_{E_{X, y}} = \nabla^2 E_{X, y}(A) = 2 X^T X\)&lt;/span&gt; is positive definite, this is a minimum. Hence, the optimal solution to this problem is:&lt;/p&gt;
&lt;div class="math"&gt;$$A = ((X^T X)^{-1} X^T y)^T$$&lt;/div&gt;
&lt;h2 id="lagrange-multipliers"&gt;Lagrange multipliers&lt;/h2&gt;
&lt;p&gt;Lagrange multipliers are a trick in optimization problems with constraints.
They can be used to get rid of the constraints.&lt;/p&gt;
&lt;p&gt;The Lagrange function has the form&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{L} (x, \lambda_1, \dots, \lambda_n) = f(x) + \sum_{j=1}^n \lambda_j h_j(x)$$&lt;/div&gt;
&lt;p&gt;
with the &lt;em&gt;Lagrange multipliers&lt;/em&gt; &lt;span class="math"&gt;\(\lambda_j \in \mathbb{R}\)&lt;/span&gt; and &lt;span class="math"&gt;\(h_j\)&lt;/span&gt; are equality constraints.&lt;br/&gt;
&lt;br/&gt;
Necessary conditions for a minimum &lt;span class="math"&gt;\(x^*\)&lt;/span&gt; is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\nabla_x \mathcal{L} = \nabla_x f(x^*) + \sum_{j=1}^n \lambda_j \nabla_x h_j(x^*) \overset{!}{=} 0$&lt;/li&gt;
&lt;li&gt;$\frac{\partial}{\partial \lambda_j} \mathcal{L} = h_j(x^*) \overset{!}{=} 0, \quad j=1, \dots, $&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See [&lt;a href="#ref-smi04" name="ref-smi04-anchor"&gt;Smi04&lt;/a&gt;] for many examples.&lt;/p&gt;
&lt;h2 id="optimization-problem-characteristics"&gt;Optimization Problem characteristics&lt;/h2&gt;
&lt;p&gt;There are some properties of optimization problems which make it easier / harder
to solve:&lt;/p&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;th&gt;Property&lt;/th&gt;
&lt;th&gt;Easy&lt;/th&gt;
&lt;th&gt;Hard&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Objective&lt;/td&gt;
&lt;td&gt;linear&lt;/td&gt;
&lt;td&gt;non-linear&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Optimization Variable&lt;/td&gt;
&lt;td&gt;small discrete, continuous&lt;/td&gt;
&lt;td&gt;large discrete&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Constraints&lt;/td&gt;
&lt;td&gt;No Constraints&lt;/td&gt;
&lt;td&gt;Constraints&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h2 id="resources"&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Reddit: &lt;a href="https://www.reddit.com/r/MachineLearning/comments/4582s0/overview_of_optimization_algorithms/"&gt;Overview of Optimization Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a href="#ref-smi04-anchor" name="ref-smi04"&gt;Smi04&lt;/a&gt;] B. T. Smith, &amp;ldquo;Lagrange multipliers tutorial in the context of support
  vector machines,&amp;rdquo; Memorial University of Newfoundland St. John&amp;rsquo;s,
  Newfoundland, Canada, Jun. 2004.&lt;/li&gt;
&lt;/ul&gt;</content><category term="Machine Learning"></category><category term="optimization"></category><category term="gradient descent"></category></entry><entry><title>Linear Classification</title><link href="https://martin-thoma.com/linear-classification/" rel="alternate"></link><published>2016-06-22T20:00:00+02:00</published><updated>2016-06-22T20:00:00+02:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2016-06-22:/linear-classification/</id><summary type="html">&lt;p&gt;In classification problems you have data points &lt;span class="math"&gt;\(x \in \mathbb{R}^m\)&lt;/span&gt; which you want to classify into one of &lt;span class="math"&gt;\(k \in \mathbb{N}_{\geq 2}\)&lt;/span&gt; classes. This is a supervised task. This means you have &lt;span class="math"&gt;\(n\)&lt;/span&gt; data points for training in a matrix &lt;span class="math"&gt;\(X \in \mathbb{R}^{n …&lt;/span&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;In classification problems you have data points &lt;span class="math"&gt;\(x \in \mathbb{R}^m\)&lt;/span&gt; which you want to classify into one of &lt;span class="math"&gt;\(k \in \mathbb{N}_{\geq 2}\)&lt;/span&gt; classes. This is a supervised task. This means you have &lt;span class="math"&gt;\(n\)&lt;/span&gt; data points for training in a matrix &lt;span class="math"&gt;\(X \in \mathbb{R}^{n \times m}\)&lt;/span&gt; with their labels.&lt;sup id="fnref-1"&gt;&lt;a class="footnote-ref" href="#fn-1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Initially, the label might be something like "cat" or "dog". But the machine learning algorithms can't deal with those directly, so you need to encode the labels. The simplest way of encoding them is to use integers &lt;span class="math"&gt;\(0, 1, \dots, k - 1\)&lt;/span&gt;. However, in many cases it is handy to use a &lt;em&gt;one-hot encoding&lt;/em&gt;. This means you make a &lt;span class="math"&gt;\(k\)&lt;/span&gt;-dimensional vector for each label. &lt;span class="math"&gt;\((1, 0)\)&lt;/span&gt; might encode "dog" and &lt;span class="math"&gt;\((0, 1)\)&lt;/span&gt; might encode "cat". &lt;code&gt;sklearn&lt;/code&gt; supports this encoding in a convenient way
(&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"&gt;docs&lt;/a&gt;). Another common encoding is &lt;span class="math"&gt;\(-1\)&lt;/span&gt; and &lt;span class="math"&gt;\(1\)&lt;/span&gt; for binary classification problems.&lt;sup id="fnref-2"&gt;&lt;a class="footnote-ref" href="#fn-2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;A linear model is one which applies only linear operations to the features. So basically the model may only be a matrix. Typically the elements of the matrix are called &lt;em&gt;weights&lt;/em&gt;, because they weight the importance of each feature.&lt;/p&gt;
&lt;p&gt;The objective of such a classifier&lt;sup id="fnref-4"&gt;&lt;a class="footnote-ref" href="#fn-4"&gt;4&lt;/a&gt;&lt;/sup&gt; is to find a matrix &lt;span class="math"&gt;\(W^*\)&lt;/span&gt; such that the MSE is as small as possible on the test set &lt;span class="math"&gt;\(D\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$W^* = \arg \min_{W} E_{MSE} (f_W, D)$$&lt;/div&gt;
&lt;h2 id="one-hot-encoding"&gt;One-hot encoding&lt;/h2&gt;
&lt;p&gt;If one-hot encoded labels are used,&lt;sup id="fnref-3"&gt;&lt;a class="footnote-ref" href="#fn-3"&gt;3&lt;/a&gt;&lt;/sup&gt; a linear classifier
&lt;/p&gt;
&lt;div class="math"&gt;$$f: \text{feature space} \rightarrow \text{class space}$$&lt;/div&gt;
&lt;p&gt;
usually works like this:
&lt;/p&gt;
&lt;div class="math"&gt;$$f(x) = {\arg \max}_{i \in 1, \dots, k} (W \cdot x)^{(i)}$$&lt;/div&gt;
&lt;p&gt;
with &lt;span class="math"&gt;\(W \in \mathbb{R}^{k \times m}\)&lt;/span&gt; and &lt;span class="math"&gt;\(^{(i)}\)&lt;/span&gt; denoting the &lt;span class="math"&gt;\(i\)&lt;/span&gt;-th element of the vector. Given a test set &lt;/p&gt;
&lt;div class="math"&gt;$$D = \{(x_i, y_i) \text{ with } i \in \{1, \dots, n_t\}, x_i \in \mathbb{R}^{m}, y_i \in \mathbb{R}_+^{k}\}$$&lt;/div&gt;
&lt;p&gt;
you can calculate the &lt;em&gt;mean squared error&lt;/em&gt; (MSE) of the classifier &lt;span class="math"&gt;\(f\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$E_{MSE}(f, D) = \frac{1}{n_t}\sum_{i=1}^{n_t} (t_i - W \cdot x_i)^T (t_i - W \cdot x_i)$$&lt;/div&gt;
&lt;p&gt;Please note that the MSE is always non-negative for every single data point.&lt;/p&gt;
&lt;h3 id="normalizing-the-output"&gt;Normalizing the output&lt;/h3&gt;
&lt;p&gt;The nice thing about the MSE is that it is simple. It can easily be calculated and is used in regression problems very often. What is not so nice is the fact that it punishes several good solutions, too. For example, say we have a data point &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; which has the target &lt;span class="math"&gt;\((1, 0)^T\)&lt;/span&gt;. The classification output&lt;/p&gt;
&lt;div class="math"&gt;$$c_1 = \begin{pmatrix}101\\0\end{pmatrix}\quad E_{MSE}(f, x_1) = 100^2\tag{1.1}$$&lt;/div&gt;
&lt;p&gt;while the classification output of another point &lt;span class="math"&gt;\(x_2\)&lt;/span&gt; is&lt;/p&gt;
&lt;div class="math"&gt;$$c_2 = \begin{pmatrix}0\\0.1\end{pmatrix}\quad E_{MSE}(f, x_2) = 1 + 0.1^2\tag{1.2}$$&lt;/div&gt;
&lt;p&gt;This is problematic, as the actual classification in &lt;span class="math"&gt;\((1.1)\)&lt;/span&gt; is correct whereas the classification in &lt;span class="math"&gt;\((1.2)\)&lt;/span&gt; is wrong. However, this can easily be fixed by normalizing the result.&lt;/p&gt;
&lt;h4 id="simple-normalization"&gt;Simple Normalization&lt;/h4&gt;
&lt;p&gt;The simplest way to normalize the result of the classifier would be to divide each entry by the sum of all entries, e.g. for &lt;span class="math"&gt;\((1.1)\)&lt;/span&gt; we get&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
c_1' &amp;amp;= \begin{pmatrix}1\\0\end{pmatrix}\quad &amp;amp;E_{MSE}(f'', x_1) &amp;amp;= 0\tag{2.1}\\
c_2' &amp;amp;= \begin{pmatrix}0\\1\end{pmatrix}\quad &amp;amp;E_{MSE}(f', x_2) &amp;amp;= 1\tag{2.2}
\end{align}
$$&lt;/div&gt;
&lt;h4 id="standardization"&gt;Standardization&lt;/h4&gt;
&lt;p&gt;You might want to interpret the output of your classifier as a probability of the data point belonging to the different classes. Then you may not have negative values and you also want to avoid a probability of 0. A common normalization then is the &lt;em&gt;softmax function&lt;/em&gt; &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;. It first exponentiates the single values and then normalizes:&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
c_1'' &amp;amp;= \begin{pmatrix}\frac{e^{101}}{e^{101} + e^0}\\\frac{e^{0}}{e^{101} + e^0}\end{pmatrix} \approx \begin{pmatrix}1\\0\end{pmatrix}\quad &amp;amp;E_{MSE}(f'', x_1) &amp;amp;\approx 0\tag{3.1}\\
c_2'' &amp;amp;= \begin{pmatrix}\frac{e^{0}}{e^{0} + e^{0.1}}\\\frac{e^{0.1}}{e^{0} + e^{0.1}}\end{pmatrix} \approx \begin{pmatrix}0.475\\0.525\end{pmatrix}\quad &amp;amp;E_{MSE}(f'', x_2) &amp;amp;\approx 0.551\tag{3.2}
\end{align}$$&lt;/div&gt;
&lt;p&gt;Note that this is the same as a neural network with only an input layer and a softmax output layer.&lt;/p&gt;
&lt;p&gt;Now the optimization problem is:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
W^* &amp;amp;= \arg \min_{W \in \mathbb{R}^{k \times m}} E(f'', D)\\
  &amp;amp;= \arg \min_{W \in \mathbb{R}^{k \times m}} \frac{1}{n} \sum_{i=1}^n (t_i - \sigma(W x_i))(t_i - \sigma(W x_i))^T \\
  &amp;amp;= \arg \min_{W \in \mathbb{R}^{k \times m}} \sum_{i=1}^n (t_i - \sigma(W x_i))(t_i - \sigma(W x_i))^T \\
\end{align}
$$&lt;/div&gt;
&lt;p&gt;This is a differentiable function. This means to optimize we can calculate the gradient and apply gradient descent:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
&amp;amp;\frac{\partial}{\partial W} \sum_{i=1}^n (t_i - \sigma(W x_i))(t_i - \sigma(W x_i))^T\\
=&amp;amp; \sum_{i=1}^n \frac{\partial}{\partial W} (t_i - \sigma(W x_i))(t_i - \sigma(W x_i))^T\\
=&amp;amp; \sum_{i=1}^n \left (\frac{\partial}{\partial W} (t_i - \sigma(W x_i)) \right ) \left(t_i - \sigma(W x_i) \right)^T + (t_i - \sigma(W x_i)) \left (\frac{\partial}{\partial W} (t_i - \sigma(W x_i))\right )\\
=&amp;amp; \sum_{i=1}^n \left (\frac{\partial}{\partial W} \sigma(W x_i) \right ) \left(t_i - \sigma(W x_i) \right)^T + (t_i - \sigma(W x_i)) \left (\frac{\partial}{\partial W} \sigma(W x_i)\right )^T\\
\end{align}$$&lt;/div&gt;
&lt;p&gt;as you can see it gets quite ugly. I don't want to continue this calculation here.  But I hope you can see that this is possible. &lt;a href="http://stats.stackexchange.com/q/79454/25741"&gt;stats.stackexchange.com&lt;/a&gt; gives some hints on how to continue.&lt;/p&gt;
&lt;h3 id="decision-boundary"&gt;Decision boundary&lt;/h3&gt;
&lt;p&gt;The decision boundary for a two-class problem with 2-dimensional data vectors (and one bias) can be calculated as&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
&amp;amp;W^{(0, 0)} \cdot 1 + W^{(0, 1)} \cdot x_1 + W^{(0, 2)} \cdot x_2 = W^{(1, 0)} \cdot 1 + W^{(1, 1)} \cdot x_1 + W^{(1, 2)} \cdot x_2\tag{DB}\\
\Leftrightarrow x_2&amp;amp;= \frac{W^{(0, 0)} \cdot 1 + W^{(0, 1)} \cdot x_1 - (W^{(1, 0)} \cdot 1 + W^{(1, 1)} \cdot x_1)}{W^{(1, 2)} - W^{(0, 2)}}\\
\Leftrightarrow x_2 &amp;amp;= \frac{(W^{(0, 1)} - W^{(1, 1)})}{W^{(1, 2)} - W^{(0, 2)}} \cdot x_1 + \frac{W^{(0, 0)} - W^{(1, 0)}}{W^{(1, 2)} - W^{(0, 2)}}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;As you can see, the decision boundary of the non-normalized form is a line:&lt;/p&gt;
&lt;div class="math"&gt;$$y = a \cdot x + b \qquad a, b \in \mathbb{R}$$&lt;/div&gt;
&lt;p&gt;Please also note that &lt;span class="math"&gt;\(W^{(1, 2)} = W^{(0, 2)}\)&lt;/span&gt; is possible, which would mean that the line is parallel to the &lt;span class="math"&gt;\(x_2\)&lt;/span&gt; axis. So &lt;strong&gt;this model basically only has 2&amp;nbsp;parameter combinations which matter, although it has 6&amp;nbsp;values which can be adjusted. But many combinations are equivalent.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;What does normalization change?&lt;/p&gt;
&lt;p&gt;For given &lt;span class="math"&gt;\(W, x\)&lt;/span&gt; it only divides both sides of the equation by the same constant. Hence it doesn't change the decision boundary.&lt;/p&gt;
&lt;p&gt;What does standardization with softmax change?&lt;/p&gt;
&lt;p&gt;Just like with normalization, softmax makes equation &lt;span class="math"&gt;\((DB)\)&lt;/span&gt; to be divided by a constant. This can be ignored. The exponentiation can also be ignored as we can simply take the logartihm of both sides of &lt;span class="math"&gt;\((DB)\)&lt;/span&gt;. Or in other words: &lt;strong&gt;A neural network with only one input layer and one softmax output layer also has a linear decision boundary!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="-11-encoding_1"&gt;-1/+1 encoding&lt;/h2&gt;
&lt;p&gt;In the 2-class case one might consider to use -1 for one class and +1 for the other class as targets. Then the classifcation is&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{cases}
\text{class with label 1}&amp;amp;\text{if } f(x) \geq 0\\
\text{class with label -1}&amp;amp;\text{if } f(x) &amp;lt; 0
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;The matrix &lt;span class="math"&gt;\(W\)&lt;/span&gt; is now in &lt;span class="math"&gt;\(\mathbb{R}^{1 \times 3}\)&lt;/span&gt; and the classifier is&lt;/p&gt;
&lt;div class="math"&gt;$$f(x) = W x$$&lt;/div&gt;
&lt;p&gt;To make sure that this is either &lt;span class="math"&gt;\(-1\)&lt;/span&gt; or &lt;span class="math"&gt;\(+1\)&lt;/span&gt;, one can modify it to&lt;/p&gt;
&lt;div class="math"&gt;$$f'(x) = \frac{W x}{|W x|}$$&lt;/div&gt;
&lt;p&gt;Note that this ignores the case &lt;span class="math"&gt;\(Wx = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now we can easily calculate the MSE:&lt;/p&gt;
&lt;div class="math"&gt;$$E_{MSE}(f, D) = \sum_{i=1}^{n_t} {(t_i - f'(x))}^2$$&lt;/div&gt;
&lt;h2 id="differences-from-target-encoding"&gt;Differences from target encoding&lt;/h2&gt;
&lt;p&gt;You might wonder if it makes a difference which type of encoding you use for your target. There are some things which come to my mind:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(+) One-hot encoding can easily be expanded to more than two classes, in contrast to &lt;span class="math"&gt;\(-1/+1\)&lt;/span&gt; encoding.&lt;/li&gt;
&lt;li&gt;(+) With one-hot encoding, you can easily get a probability distribution for the classes you are interested in. This is certainly also possible with &lt;span class="math"&gt;\(-1/+1\)&lt;/span&gt; encoding, but it doesn't strike my eye as clearly.&lt;/li&gt;
&lt;li&gt;(-) One-hot encoding needs more storage space.&lt;/li&gt;
&lt;li&gt;(?) &lt;span class="math"&gt;\(-1/+1\)&lt;/span&gt; encoding is used in SVMs (see &lt;a href="https://martin-thoma.com/svm-with-sklearn/"&gt;SVM article&lt;/a&gt;), so it might have advantages in maximum margin classification.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="implementation"&gt;Implementation&lt;/h2&gt;
&lt;p&gt;If I had to implement a linear binary classifier, I would use the delta rule
and a perceptron unit:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env python&lt;/span&gt;

&lt;span class="sd"&gt;"""Example for a linear classifier using a perceptron and the delta rule."""&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets.samples_generator&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;make_blobs&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Perceptron&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eta&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;        Single perceptron unit.&lt;/span&gt;

&lt;span class="sd"&gt;        Credit to Sebastian Raschka:&lt;/span&gt;
&lt;span class="sd"&gt;        http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html&lt;/span&gt;
&lt;span class="sd"&gt;        This was slightly modified.&lt;/span&gt;
&lt;span class="sd"&gt;        """&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;eta&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;w_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;errors_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;errors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;xi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;update&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eta&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xi&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;w_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;update&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;xi&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;w_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;update&lt;/span&gt;
                &lt;span class="n"&gt;errors&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;errors_&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;net_input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;w_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:])&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;w_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;net_input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Generate data&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;make_blobs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;centers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cluster_std&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Fit perceptron&lt;/span&gt;
&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Perceptron&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Plot decision boundary&lt;/span&gt;
&lt;span class="n"&gt;xs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;stop&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;w_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;w_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;xi&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;w_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;xi&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;'r--'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Plot data&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gray&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;which gives:&lt;/p&gt;
&lt;figure class="wp-caption aligncenter img-thumbnail"&gt;
&lt;img alt="Classification with a Perceptron." src="../images/2016/06/perceptron-classification.png"/&gt;
&lt;figcaption class="text-center"&gt;Classification with a Perceptron.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Looks about right. You can also see that this probably minimizes the MSE, but
it does not maximize the margin between the classes. This would be done by
SVMs.&lt;/p&gt;
&lt;h2 id="footnotes"&gt;Footnotes&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn-1"&gt;
&lt;p&gt;A label and a class are two different things. The class is a set which
  contains all data points which belong to this class. The label is only
  a pointer to this class.&amp;nbsp;&lt;a class="footnote-backref" href="#fnref-1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-2"&gt;
&lt;p&gt;Classification problems with only two classes.&amp;nbsp;&lt;a class="footnote-backref" href="#fnref-2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-3"&gt;
&lt;p&gt;Also called "targets" sometimes, as we want our classifier to output those values.&amp;nbsp;&lt;a class="footnote-backref" href="#fnref-3" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-4"&gt;
&lt;p&gt;The objective function defines what is to be done during fitting / training / learning.&amp;nbsp;&lt;a class="footnote-backref" href="#fnref-4" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/hr&gt;&lt;/div&gt;</content><category term="Algorithms"></category><category term="Machine Learning"></category><category term="optimization"></category><category term="Python"></category></entry><entry><title>Collaborative Filtering</title><link href="https://martin-thoma.com/collaborative-filtering/" rel="alternate"></link><published>2016-02-10T21:35:00+01:00</published><updated>2016-02-10T21:35:00+01:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2016-02-10:/collaborative-filtering/</id><summary type="html">&lt;p&gt;Suppose you are in the Netflix setting: You have &lt;span markdown="0"&gt;&lt;span class="math"&gt;\(M\)&lt;/span&gt;&lt;/span&gt;
movies, &lt;span markdown="0"&gt;&lt;span class="math"&gt;\(N\)&lt;/span&gt;&lt;/span&gt; users and integer ratings
&lt;span markdown="0"&gt;&lt;span class="math"&gt;\(1, \dots, K\)&lt;/span&gt;&lt;/span&gt; for some movies by some users.&lt;/p&gt;
&lt;p&gt;You want to predict all missing values. This means you want to say how the
users would rate movies they have not actually rated.&lt;/p&gt;
&lt;p&gt;Please …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Suppose you are in the Netflix setting: You have &lt;span markdown="0"&gt;&lt;span class="math"&gt;\(M\)&lt;/span&gt;&lt;/span&gt;
movies, &lt;span markdown="0"&gt;&lt;span class="math"&gt;\(N\)&lt;/span&gt;&lt;/span&gt; users and integer ratings
&lt;span markdown="0"&gt;&lt;span class="math"&gt;\(1, \dots, K\)&lt;/span&gt;&lt;/span&gt; for some movies by some users.&lt;/p&gt;
&lt;p&gt;You want to predict all missing values. This means you want to say how the
users would rate movies they have not actually rated.&lt;/p&gt;
&lt;p&gt;Please note that ratings for products on Amazon might be a very similar
situation. It might also be similar to the StumbleUpon rating.&lt;/p&gt;
&lt;h2 id="the-problem"&gt;The Problem&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Much Data&lt;/strong&gt;: You have 17&amp;thinsp;000 movies, 480&amp;thinsp;000 users and
  100&amp;thinsp;000&amp;thinsp;000 ratings of movies by those users.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Missing Data&lt;/strong&gt;: Although you have a lot of ratings, a complete dataset
  would be &lt;span markdown="0"&gt;&lt;span class="math"&gt;\(17\cdot 10^3 \cdot 480 \cdot 10^3 = 8160 \cdot 10^6\)&lt;/span&gt;&lt;/span&gt;
  ratings. This means you only have about 12% of all possible ratings. There
  is a lot of data missing.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="a-solution"&gt;A Solution&lt;/h2&gt;
&lt;p&gt;Train one RBM per user, but share weights amongst the RBMs. This simply means
the weights are averaged.&lt;/p&gt;
&lt;p&gt;The visible units are movies. But instead of having binary visible units, the
units have &lt;span markdown="0"&gt;&lt;span class="math"&gt;\(K=5\)&lt;/span&gt;&lt;/span&gt; states on which softmax is applied.&lt;/p&gt;
&lt;p&gt;The hidden units (about 100) model dependencies between movie ratings.&lt;/p&gt;
&lt;p&gt;When you now want to predict the missing ratings, you can just perform a
sampling in the user-specific RBM. You calculate the values of the hidden units,
then you have a vector for this user which describes the users preferences.
You add the missing movies with the weights from the other users and sample
back.&lt;/p&gt;
&lt;h2 id="material"&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Salakhutdinov, Mnih and Hinton: &lt;a href="http://www.cs.toronto.edu/~rsalakhu/papers/rbmcf.pdf"&gt;Restricted Boltzmann machines for collaborative filtering&lt;/a&gt;. In Proceedings of the 24th international conference on Machine learning, 2007.&lt;/li&gt;
&lt;li&gt;Hinton: &lt;a href="https://www.youtube.com/watch?v=fzAuXMg_7n4"&gt;5. RBMs for Collaborative Filtering
&lt;/a&gt; on YouTube. 9th of November 2013.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://academictorrents.com/details/9b13183dc4d60676b773c9e2cd6de5e5542cee9a"&gt;Netflix Prize Data Set&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Rating"></category></entry><entry><title>Softmax</title><link href="https://martin-thoma.com/softmax/" rel="alternate"></link><published>2016-02-09T18:09:00+01:00</published><updated>2016-02-09T18:09:00+01:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2016-02-09:/softmax/</id><summary type="html">&lt;p&gt;Softmax is an activation function for multi-layer perceptrons (MLPs). It is
a function which gets applied to a vector in &lt;span markdown="0"&gt;&lt;span class="math"&gt;\(\mathbb{x} \in \mathbb{R}^K\)&lt;/span&gt;&lt;/span&gt;
and returns a vector in &lt;span markdown="0"&gt;&lt;span class="math"&gt;\([0, 1]^K\)&lt;/span&gt;&lt;/span&gt; with the
property that the sum of all elements is 1:&lt;/p&gt;
&lt;div&gt;$$\varphi(\mathbb{x})_j = \frac …&lt;/div&gt;</summary><content type="html">&lt;p&gt;Softmax is an activation function for multi-layer perceptrons (MLPs). It is
a function which gets applied to a vector in &lt;span markdown="0"&gt;&lt;span class="math"&gt;\(\mathbb{x} \in \mathbb{R}^K\)&lt;/span&gt;&lt;/span&gt;
and returns a vector in &lt;span markdown="0"&gt;&lt;span class="math"&gt;\([0, 1]^K\)&lt;/span&gt;&lt;/span&gt; with the
property that the sum of all elements is 1:&lt;/p&gt;
&lt;div&gt;$$\varphi(\mathbb{x})_j = \frac{e^{x_j}}{\sum_{k=1}^K e^{x_k}} \;\;\;\text{ for } j=1, \dots, K$$&lt;/div&gt;
&lt;h2 id="python-implementation"&gt;Python implementation&lt;/h2&gt;
&lt;p&gt;The implementation is straight forward:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#! /usr/bin/env python&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Calculate the softmax of a list of numbers w.&lt;/span&gt;

&lt;span class="sd"&gt;    Parameters&lt;/span&gt;
&lt;span class="sd"&gt;    ----------&lt;/span&gt;
&lt;span class="sd"&gt;    w : list of numbers&lt;/span&gt;

&lt;span class="sd"&gt;    Return&lt;/span&gt;
&lt;span class="sd"&gt;    ------&lt;/span&gt;
&lt;span class="sd"&gt;    a list of the same length as w of non-negative numbers&lt;/span&gt;

&lt;span class="sd"&gt;    Examples&lt;/span&gt;
&lt;span class="sd"&gt;    --------&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;gt;&amp;gt;&amp;gt; softmax([0.1, 0.2])&lt;/span&gt;
&lt;span class="sd"&gt;    array([ 0.47502081,  0.52497919])&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;gt;&amp;gt;&amp;gt; softmax([-0.1, 0.2])&lt;/span&gt;
&lt;span class="sd"&gt;    array([ 0.42555748,  0.57444252])&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;gt;&amp;gt;&amp;gt; softmax([0.9, -10])&lt;/span&gt;
&lt;span class="sd"&gt;    array([  9.99981542e-01,   1.84578933e-05])&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;gt;&amp;gt;&amp;gt; softmax([0, 10])&lt;/span&gt;
&lt;span class="sd"&gt;    array([  4.53978687e-05,   9.99954602e-01])&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;dist&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;"__main__"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;doctest&lt;/span&gt;
    &lt;span class="n"&gt;doctest&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;testmod&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="short-analysis"&gt;Short analysis&lt;/h2&gt;
&lt;p&gt;One obvious property of the softmax function is that the sum of all elements
is one due to the normalization in the denominator.&lt;/p&gt;
&lt;p&gt;By printing the following you can see that values below 1 get closer together
and elements above 1 get farer away.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;percentage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;before&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;before&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;before&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;after&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;before&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Before: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;before&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;before&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"After:  &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;after&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;after&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"-"&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;experiments&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;experiments&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;experiments&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;experiments&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;experiments&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reverse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;percentage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;gives&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Before: [ 1.1  1.1  1. ]
After:  [ 1.10517092  1.10517092  1.        ]
------------------------------------------------------------
Before: [3 2 1]
After:  [ 7.3890561   2.71828183  1.        ]
------------------------------------------------------------
Before: [ 1.33333333  1.16666667  1.        ]
After:  [ 1.22140276  1.10517092  1.        ]
------------------------------------------------------------
&lt;/pre&gt;&lt;/div&gt;</content><category term="Machine Learning"></category><category term="Neural Networks"></category><category term="Activation Functions"></category></entry><entry><title>Comparing Classifiers</title><link href="https://martin-thoma.com/comparing-classifiers/" rel="alternate"></link><published>2016-01-19T20:13:00+01:00</published><updated>2016-01-19T20:13:00+01:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2016-01-19:/comparing-classifiers/</id><summary type="html">&lt;p&gt;Classification problems occur quite often and many different classification
algorithms have been described and implemented. But what is the best algorithm
for a given error function and dataset?&lt;/p&gt;
&lt;p&gt;I read questions like "I have problem X. What is the best classifier?" quite
often and my first impulse is always to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Classification problems occur quite often and many different classification
algorithms have been described and implemented. But what is the best algorithm
for a given error function and dataset?&lt;/p&gt;
&lt;p&gt;I read questions like "I have problem X. What is the best classifier?" quite
often and my first impulse is always to write: Just try them!&lt;/p&gt;
&lt;p&gt;I guess people asking this question might think that it is super difficult to
do so. However, the sklearn tutorial contains a very nice example where
many classifiers are compared (&lt;a href="http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html"&gt;source&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;This article gives you an overview over some classifiers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"&gt;SVM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"&gt;k-nearest neighbors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"&gt;Random Forest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html"&gt;AdaBoost Classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"&gt;Gradient Boosting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html"&gt;Naive Bayes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/0.16/modules/generated/sklearn.lda.LDA.html"&gt;LDA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/0.16/modules/generated/sklearn.qda.QDA.html"&gt;QDA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html"&gt;RBMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html"&gt;RBM&lt;/a&gt; + Logistic Regression Classifier&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course, neural networks are also one very powerful ML classifier I may not
forget. As sklearn does not have neural networks, I've installed
&lt;a href="https://github.com/tensorflow/skflow"&gt;&lt;code&gt;skflow&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="tutorial-example"&gt;Tutorial example&lt;/h2&gt;
&lt;p&gt;The sklearn tutorial creates three datasets with 100&amp;nbsp;points per dataset and
2&amp;nbsp;dimensions per point:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Moons&lt;/strong&gt;: Two interleaving half-circles&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Circles&lt;/strong&gt;: A larger circle containing the smaller one&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Linear&lt;/strong&gt;: A linearly seperable dataset&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each of those three datasets has added noise. This means for some points there
might be no way of classifying them correclty.&lt;/p&gt;
&lt;p&gt;Here are the results&lt;/p&gt;
&lt;figure class="aligncenter"&gt;
&lt;a href="../images/2016/01/ml-classifiers-1.png"&gt;&lt;img alt="k nearest neighbors, linear and RBFSVM" class="" src="../images/2016/01/ml-classifiers-1.png" style="max-width:500px;"/&gt;&lt;/a&gt;
&lt;figcaption class="text-center"&gt;k nearest neighbors, linear and RBFSVM&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;One can see that k nearest neighbors gives arbitrary decision boundaries.
Overall, they look reasonable. However, there are often strange zig-zag
patterns.&lt;/p&gt;
&lt;p&gt;The linear SVM in contrast has a very easy decision boundary: a line. It is no
suprise that it can't deal with the moons dataset. Note that a random guess
would be right in 50% of the cases.&lt;/p&gt;
&lt;p&gt;The RBF SVM has very nice decision boundary. It is smooth, matches the pattern
and is able to adjust to all three examles.&lt;/p&gt;
&lt;figure class="aligncenter"&gt;
&lt;a href="../images/2016/01/ml-classifiers-2.png"&gt;&lt;img alt="Decision Tree, Random Forest, AdaBoost" class="" src="../images/2016/01/ml-classifiers-2.png" style="max-width:500px;"/&gt;&lt;/a&gt;
&lt;figcaption class="text-center"&gt;Decision Tree, Random Forest, AdaBoost&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Decision Trees, Decision Forests and AdaBoost all show very similar
patterns. The boundaries change in parallel to the coordinate axes which looks
very unnatural.&lt;/p&gt;
&lt;figure class="aligncenter"&gt;
&lt;a href="../images/2016/01/ml-classifiers-3.png"&gt;&lt;img alt="Naive Bayes, LDA, QDA" class="" src="../images/2016/01/ml-classifiers-3.png" style="max-width:500px;"/&gt;&lt;/a&gt;
&lt;figcaption class="text-center"&gt;Naive Bayes, LDA, QDA&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Naive Bayes shows nice, smooth patterns. However, those patterns seem to be
a bit too simple. LDA is again linear (see linear SVM). Comparing QDA to
Naive Bayes is interesting. Although they get similar performance for the first
dataset, I would argue that the naive bayes classifier is much better as it is
much more confident for its classification. Even more extrem is the last example.
I'm astonished that the QDA gets 93% with that boundary; Naive Bayes seems to
find a much better boundary.&lt;/p&gt;
&lt;h2 id="the-hardware"&gt;The hardware&lt;/h2&gt;
&lt;p&gt;The following comparison is done on a PC with an &lt;a href="http://ark.intel.com/de/products/77781/Intel-Core-i7-4820K-Processor-10M-Cache-up-to-3_90-GHz"&gt;Intel i7-4820K CPU&lt;/a&gt; and a NVIDIA GeForce GTX Titan Black
GPU.&lt;/p&gt;
&lt;h2 id="mnist"&gt;MNIST&lt;/h2&gt;
&lt;p&gt;MNIST is a dataset of &lt;span markdown="0"&gt;&lt;span class="math"&gt;\(28\text{px} \times 28\text{px}\)&lt;/span&gt;&lt;/span&gt; greyscale images.
Each of the images contains a digit (0, 1, 2, 3, 4, 5, 6, 7, 8, 9). The
task is to classify the image into one of the 10 digit classes.&lt;/p&gt;
&lt;p&gt;Guessing randomly will give an accuracy of &lt;span markdown="0"&gt;&lt;span class="math"&gt;\(\frac{1}{10} = 0.1\)&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id="neural-networks"&gt;Neural Networks&lt;/h3&gt;
&lt;p&gt;Please note that there are neural networks which get much better accuracy.
Most notably the &lt;a href="https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html#deep-mnist-for-experts"&gt;MNIST Expert tutorial&lt;/a&gt; with 99.2% accuracy.&lt;/p&gt;
&lt;h4 id="simple-network"&gt;Simple Network&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Classifier&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;NN&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;
&lt;span class="n"&gt;Training&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;79.5696&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Testing&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.3480&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Confusion&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="o"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;2248&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;2565&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;7&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="mi"&gt;2258&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt; &lt;span class="mi"&gt;2294&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;23&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;2161&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;16&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;2014&lt;/span&gt;   &lt;span class="mi"&gt;17&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;11&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt; &lt;span class="mi"&gt;2237&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;2355&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;   &lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;   &lt;span class="mi"&gt;17&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;2161&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;   &lt;span class="mi"&gt;16&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt; &lt;span class="mi"&gt;2340&lt;/span&gt;&lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.9798&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4 id="dropout-network"&gt;Dropout Network&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Classifier&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;NN&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt; &lt;span class="n"&gt;dropout&lt;/span&gt;
&lt;span class="n"&gt;Training&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;118.2654&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Testing&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.3918&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Confusion&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="o"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;2250&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;2567&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt; &lt;span class="mi"&gt;2272&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;26&lt;/span&gt; &lt;span class="mi"&gt;2260&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;24&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;   &lt;span class="mi"&gt;19&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;2152&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="mi"&gt;1983&lt;/span&gt;   &lt;span class="mi"&gt;20&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;21&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;11&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;2237&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;   &lt;span class="mi"&gt;13&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;2363&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;7&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="mi"&gt;2170&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;13&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;19&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="mi"&gt;2337&lt;/span&gt;&lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.9780&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4 id="cnn"&gt;CNN&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Classifier&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;CNN&lt;/span&gt;
&lt;span class="n"&gt;Training&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;391.8810&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Testing&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.2035&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Confusion&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="o"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;2243&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;2548&lt;/span&gt;   &lt;span class="mi"&gt;20&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="mi"&gt;2253&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;   &lt;span class="mi"&gt;17&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;   &lt;span class="mi"&gt;13&lt;/span&gt; &lt;span class="mi"&gt;2290&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;2164&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;15&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;2016&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;12&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt; &lt;span class="mi"&gt;2227&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;2374&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;   &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;15&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;13&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="mi"&gt;2145&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;   &lt;span class="mi"&gt;16&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;24&lt;/span&gt;   &lt;span class="mi"&gt;13&lt;/span&gt; &lt;span class="mi"&gt;2306&lt;/span&gt;&lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.9769&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="svm"&gt;SVM&lt;/h3&gt;
&lt;p&gt;There is a ton of literature / papers about &lt;abbr title="Support Vector Machines"&gt;SVMs&lt;/abbr&gt;.
I've summed up the basics on &lt;a href="https://martin-thoma.com/svm-with-sklearn/"&gt;Using SVMs with sklearn&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I've trained two SVMs: A simple, linear one and one with an RBF kernel as I
found it online (I'm sorry, I don't remember where I found those parameters :-/).&lt;/p&gt;
&lt;p&gt;Please note the the SVM implementation of sklearn does not use the GPU.
However, there are &lt;a href="http://fastml.com/running-things-on-a-gpu/"&gt;GPU implmentations of SVMs&lt;/a&gt;
around.&lt;/p&gt;
&lt;h4 id="linear-svm"&gt;Linear SVM&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Classifier&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;linear&lt;/span&gt; &lt;span class="n"&gt;SVM&lt;/span&gt;
&lt;span class="n"&gt;Training&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;168.6950&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Testing&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;158.0101&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Confusion&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="o"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;2226&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;2537&lt;/span&gt;   &lt;span class="mi"&gt;18&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;   &lt;span class="mi"&gt;17&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;12&lt;/span&gt;   &lt;span class="mi"&gt;16&lt;/span&gt; &lt;span class="mi"&gt;2158&lt;/span&gt;   &lt;span class="mi"&gt;25&lt;/span&gt;   &lt;span class="mi"&gt;24&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;27&lt;/span&gt;   &lt;span class="mi"&gt;19&lt;/span&gt;   &lt;span class="mi"&gt;25&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;   &lt;span class="mi"&gt;46&lt;/span&gt; &lt;span class="mi"&gt;2188&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;   &lt;span class="mi"&gt;47&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;18&lt;/span&gt;   &lt;span class="mi"&gt;27&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;19&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;2117&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;49&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;18&lt;/span&gt;   &lt;span class="mi"&gt;13&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;   &lt;span class="mi"&gt;73&lt;/span&gt;   &lt;span class="mi"&gt;20&lt;/span&gt; &lt;span class="mi"&gt;1872&lt;/span&gt;   &lt;span class="mi"&gt;31&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;26&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;20&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;22&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;   &lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="mi"&gt;2179&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;   &lt;span class="mi"&gt;32&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;   &lt;span class="mi"&gt;30&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;2268&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;51&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;11&lt;/span&gt;   &lt;span class="mi"&gt;39&lt;/span&gt;   &lt;span class="mi"&gt;26&lt;/span&gt;   &lt;span class="mi"&gt;47&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;   &lt;span class="mi"&gt;40&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="mi"&gt;2018&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;11&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;   &lt;span class="mi"&gt;24&lt;/span&gt;   &lt;span class="mi"&gt;64&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;61&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt; &lt;span class="mi"&gt;2189&lt;/span&gt;&lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.9416&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4 id="adjusted-svm"&gt;Adjusted SVM&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Classifier&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;adj&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;SVM&lt;/span&gt;
&lt;span class="n"&gt;Training&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;347.1539&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Testing&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;234.5724&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Confusion&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="o"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;2258&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;2566&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;2280&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt; &lt;span class="mi"&gt;2304&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;13&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;2183&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;16&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;2026&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;7&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="mi"&gt;2245&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;2373&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;2166&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;19&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="mi"&gt;2329&lt;/span&gt;&lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.9840&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="random-forest"&gt;Random Forest&lt;/h3&gt;
&lt;p&gt;Data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;n_estimators=50&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;n_jobs=10&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Classifier&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Random&lt;/span&gt; &lt;span class="n"&gt;Forest&lt;/span&gt;
&lt;span class="n"&gt;Training&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;2.1359&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Testing&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;26.0763&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Confusion&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="o"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;2246&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;2543&lt;/span&gt;   &lt;span class="mi"&gt;18&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;7&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="mi"&gt;2233&lt;/span&gt;   &lt;span class="mi"&gt;20&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;   &lt;span class="mi"&gt;16&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;36&lt;/span&gt; &lt;span class="mi"&gt;2240&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;20&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;16&lt;/span&gt;   &lt;span class="mi"&gt;19&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;2142&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;   &lt;span class="mi"&gt;38&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;7&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;   &lt;span class="mi"&gt;30&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt; &lt;span class="mi"&gt;1977&lt;/span&gt;   &lt;span class="mi"&gt;16&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;13&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;   &lt;span class="mi"&gt;15&lt;/span&gt; &lt;span class="mi"&gt;2210&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;   &lt;span class="mi"&gt;29&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;   &lt;span class="mi"&gt;19&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;2315&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;   &lt;span class="mi"&gt;34&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;   &lt;span class="mi"&gt;18&lt;/span&gt;   &lt;span class="mi"&gt;17&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;   &lt;span class="mi"&gt;26&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="mi"&gt;2103&lt;/span&gt;   &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;10&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;24&lt;/span&gt;   &lt;span class="mi"&gt;27&lt;/span&gt;   &lt;span class="mi"&gt;13&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;20&lt;/span&gt;   &lt;span class="mi"&gt;18&lt;/span&gt; &lt;span class="mi"&gt;2262&lt;/span&gt;&lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.9641&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Alternatively:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;max_depth=5&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;n_estimators=10&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;max_features=1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Classifier&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Random&lt;/span&gt; &lt;span class="n"&gt;Forest&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;Training&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.2077&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Testing&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;22.2770&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Confusion&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="o"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;1955&lt;/span&gt;   &lt;span class="mi"&gt;32&lt;/span&gt;   &lt;span class="mi"&gt;63&lt;/span&gt;   &lt;span class="mi"&gt;64&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;  &lt;span class="mi"&gt;109&lt;/span&gt;   &lt;span class="mi"&gt;21&lt;/span&gt;   &lt;span class="mi"&gt;13&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;2524&lt;/span&gt;   &lt;span class="mi"&gt;20&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;252&lt;/span&gt;  &lt;span class="mi"&gt;425&lt;/span&gt; &lt;span class="mi"&gt;1198&lt;/span&gt;  &lt;span class="mi"&gt;151&lt;/span&gt;   &lt;span class="mi"&gt;64&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="mi"&gt;145&lt;/span&gt;   &lt;span class="mi"&gt;15&lt;/span&gt;   &lt;span class="mi"&gt;55&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;136&lt;/span&gt;  &lt;span class="mi"&gt;195&lt;/span&gt;  &lt;span class="mi"&gt;140&lt;/span&gt; &lt;span class="mi"&gt;1641&lt;/span&gt;   &lt;span class="mi"&gt;28&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;   &lt;span class="mi"&gt;22&lt;/span&gt;   &lt;span class="mi"&gt;95&lt;/span&gt;   &lt;span class="mi"&gt;65&lt;/span&gt;   &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;92&lt;/span&gt;  &lt;span class="mi"&gt;320&lt;/span&gt;   &lt;span class="mi"&gt;21&lt;/span&gt;   &lt;span class="mi"&gt;45&lt;/span&gt; &lt;span class="mi"&gt;1199&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;   &lt;span class="mi"&gt;76&lt;/span&gt;  &lt;span class="mi"&gt;153&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;  &lt;span class="mi"&gt;288&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;312&lt;/span&gt;  &lt;span class="mi"&gt;383&lt;/span&gt;   &lt;span class="mi"&gt;67&lt;/span&gt;  &lt;span class="mi"&gt;655&lt;/span&gt;   &lt;span class="mi"&gt;78&lt;/span&gt;  &lt;span class="mi"&gt;268&lt;/span&gt;   &lt;span class="mi"&gt;47&lt;/span&gt;   &lt;span class="mi"&gt;94&lt;/span&gt;  &lt;span class="mi"&gt;134&lt;/span&gt;   &lt;span class="mi"&gt;31&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;199&lt;/span&gt;  &lt;span class="mi"&gt;364&lt;/span&gt;  &lt;span class="mi"&gt;125&lt;/span&gt;   &lt;span class="mi"&gt;58&lt;/span&gt;   &lt;span class="mi"&gt;96&lt;/span&gt;   &lt;span class="mi"&gt;13&lt;/span&gt; &lt;span class="mi"&gt;1408&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;83&lt;/span&gt;  &lt;span class="mi"&gt;424&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;   &lt;span class="mi"&gt;70&lt;/span&gt;  &lt;span class="mi"&gt;101&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;19&lt;/span&gt; &lt;span class="mi"&gt;1555&lt;/span&gt;   &lt;span class="mi"&gt;56&lt;/span&gt;   &lt;span class="mi"&gt;98&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;392&lt;/span&gt;  &lt;span class="mi"&gt;574&lt;/span&gt;   &lt;span class="mi"&gt;44&lt;/span&gt;  &lt;span class="mi"&gt;147&lt;/span&gt;   &lt;span class="mi"&gt;52&lt;/span&gt;   &lt;span class="mi"&gt;17&lt;/span&gt;   &lt;span class="mi"&gt;71&lt;/span&gt;  &lt;span class="mi"&gt;106&lt;/span&gt;  &lt;span class="mi"&gt;773&lt;/span&gt;   &lt;span class="mi"&gt;39&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;71&lt;/span&gt;  &lt;span class="mi"&gt;338&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;   &lt;span class="mi"&gt;43&lt;/span&gt;  &lt;span class="mi"&gt;579&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;  &lt;span class="mi"&gt;632&lt;/span&gt;   &lt;span class="mi"&gt;24&lt;/span&gt;  &lt;span class="mi"&gt;681&lt;/span&gt;&lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.5715&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="k-nearest-neightbors"&gt;k nearest neightbors&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Classifier&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;
&lt;span class="n"&gt;Training&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;4.6439&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Testing&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1261.7815&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Confusion&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="o"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;2260&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;2572&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;16&lt;/span&gt;   &lt;span class="mi"&gt;15&lt;/span&gt; &lt;span class="mi"&gt;2235&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;26&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt; &lt;span class="mi"&gt;2276&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;27&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;4&lt;/span&gt;   &lt;span class="mi"&gt;19&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;2131&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;45&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;10&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;28&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="mi"&gt;1977&lt;/span&gt;   &lt;span class="mi"&gt;25&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;12&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="mi"&gt;2239&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;18&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;2349&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;29&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;11&lt;/span&gt;   &lt;span class="mi"&gt;32&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;   &lt;span class="mi"&gt;36&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;   &lt;span class="mi"&gt;34&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="mi"&gt;2053&lt;/span&gt;   &lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt;   &lt;span class="mi"&gt;26&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;19&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="mi"&gt;2303&lt;/span&gt;&lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.9695&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="decision-tree"&gt;Decision Tree&lt;/h3&gt;
&lt;p&gt;Data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;max_depth=5&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Classifier&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Decision&lt;/span&gt; &lt;span class="n"&gt;Tree&lt;/span&gt;
&lt;span class="n"&gt;Training&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;3.1346&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Testing&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0313&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Confusion&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="o"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;1767&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;   &lt;span class="mi"&gt;25&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;  &lt;span class="mi"&gt;120&lt;/span&gt;  &lt;span class="mi"&gt;137&lt;/span&gt;   &lt;span class="mi"&gt;71&lt;/span&gt;  &lt;span class="mi"&gt;114&lt;/span&gt;   &lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;2065&lt;/span&gt;  &lt;span class="mi"&gt;128&lt;/span&gt;  &lt;span class="mi"&gt;108&lt;/span&gt;   &lt;span class="mi"&gt;13&lt;/span&gt;   &lt;span class="mi"&gt;17&lt;/span&gt;   &lt;span class="mi"&gt;41&lt;/span&gt;   &lt;span class="mi"&gt;66&lt;/span&gt;  &lt;span class="mi"&gt;131&lt;/span&gt;   &lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;42&lt;/span&gt;   &lt;span class="mi"&gt;44&lt;/span&gt; &lt;span class="mi"&gt;1248&lt;/span&gt;   &lt;span class="mi"&gt;37&lt;/span&gt;  &lt;span class="mi"&gt;121&lt;/span&gt;   &lt;span class="mi"&gt;21&lt;/span&gt;  &lt;span class="mi"&gt;227&lt;/span&gt;   &lt;span class="mi"&gt;76&lt;/span&gt;  &lt;span class="mi"&gt;339&lt;/span&gt;  &lt;span class="mi"&gt;159&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;33&lt;/span&gt;   &lt;span class="mi"&gt;22&lt;/span&gt;   &lt;span class="mi"&gt;32&lt;/span&gt; &lt;span class="mi"&gt;1484&lt;/span&gt;   &lt;span class="mi"&gt;33&lt;/span&gt;  &lt;span class="mi"&gt;107&lt;/span&gt;   &lt;span class="mi"&gt;52&lt;/span&gt;   &lt;span class="mi"&gt;81&lt;/span&gt;  &lt;span class="mi"&gt;266&lt;/span&gt;  &lt;span class="mi"&gt;238&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;15&lt;/span&gt;   &lt;span class="mi"&gt;45&lt;/span&gt;   &lt;span class="mi"&gt;33&lt;/span&gt; &lt;span class="mi"&gt;1284&lt;/span&gt;   &lt;span class="mi"&gt;42&lt;/span&gt;   &lt;span class="mi"&gt;42&lt;/span&gt;   &lt;span class="mi"&gt;45&lt;/span&gt;  &lt;span class="mi"&gt;213&lt;/span&gt;  &lt;span class="mi"&gt;492&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;42&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;   &lt;span class="mi"&gt;21&lt;/span&gt;  &lt;span class="mi"&gt;229&lt;/span&gt;  &lt;span class="mi"&gt;166&lt;/span&gt;  &lt;span class="mi"&gt;577&lt;/span&gt;  &lt;span class="mi"&gt;137&lt;/span&gt;  &lt;span class="mi"&gt;123&lt;/span&gt;  &lt;span class="mi"&gt;254&lt;/span&gt;  &lt;span class="mi"&gt;510&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;34&lt;/span&gt;   &lt;span class="mi"&gt;33&lt;/span&gt;   &lt;span class="mi"&gt;66&lt;/span&gt;   &lt;span class="mi"&gt;24&lt;/span&gt;  &lt;span class="mi"&gt;103&lt;/span&gt;   &lt;span class="mi"&gt;65&lt;/span&gt; &lt;span class="mi"&gt;1734&lt;/span&gt;   &lt;span class="mi"&gt;24&lt;/span&gt;  &lt;span class="mi"&gt;102&lt;/span&gt;   &lt;span class="mi"&gt;86&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;10&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt;  &lt;span class="mi"&gt;179&lt;/span&gt;   &lt;span class="mi"&gt;57&lt;/span&gt;   &lt;span class="mi"&gt;53&lt;/span&gt;   &lt;span class="mi"&gt;21&lt;/span&gt;   &lt;span class="mi"&gt;19&lt;/span&gt; &lt;span class="mi"&gt;1775&lt;/span&gt;   &lt;span class="mi"&gt;79&lt;/span&gt;  &lt;span class="mi"&gt;210&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;98&lt;/span&gt;  &lt;span class="mi"&gt;129&lt;/span&gt;   &lt;span class="mi"&gt;43&lt;/span&gt;   &lt;span class="mi"&gt;43&lt;/span&gt;   &lt;span class="mi"&gt;42&lt;/span&gt;  &lt;span class="mi"&gt;160&lt;/span&gt;   &lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="mi"&gt;1439&lt;/span&gt;  &lt;span class="mi"&gt;231&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;   &lt;span class="mi"&gt;86&lt;/span&gt;   &lt;span class="mi"&gt;59&lt;/span&gt;  &lt;span class="mi"&gt;125&lt;/span&gt;   &lt;span class="mi"&gt;95&lt;/span&gt;   &lt;span class="mi"&gt;36&lt;/span&gt;   &lt;span class="mi"&gt;75&lt;/span&gt;  &lt;span class="mi"&gt;167&lt;/span&gt; &lt;span class="mi"&gt;1734&lt;/span&gt;&lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.6540&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="adaboost"&gt;Adaboost&lt;/h3&gt;
&lt;p&gt;You should note that you can use arbitrary base classifiers with Adaboost.
The default ones of &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html"&gt;&lt;code&gt;sklearn.ensemble.AdaBoostClassifier&lt;/code&gt;&lt;/a&gt; is &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"&gt;&lt;code&gt;sklearn.tree.DecisionTreeClassifies&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Classifier&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;AdaBoost&lt;/span&gt;
&lt;span class="n"&gt;Training&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;37.6443&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Testing&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.5815&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Confusion&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="o"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;1994&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;75&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;  &lt;span class="mi"&gt;113&lt;/span&gt;   &lt;span class="mi"&gt;51&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;15&lt;/span&gt;   &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;2435&lt;/span&gt;   &lt;span class="mi"&gt;27&lt;/span&gt;   &lt;span class="mi"&gt;22&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;   &lt;span class="mi"&gt;37&lt;/span&gt;   &lt;span class="mi"&gt;42&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;97&lt;/span&gt;   &lt;span class="mi"&gt;39&lt;/span&gt; &lt;span class="mi"&gt;1341&lt;/span&gt;   &lt;span class="mi"&gt;85&lt;/span&gt;   &lt;span class="mi"&gt;39&lt;/span&gt;   &lt;span class="mi"&gt;38&lt;/span&gt;  &lt;span class="mi"&gt;416&lt;/span&gt;   &lt;span class="mi"&gt;39&lt;/span&gt;  &lt;span class="mi"&gt;196&lt;/span&gt;   &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;108&lt;/span&gt;   &lt;span class="mi"&gt;52&lt;/span&gt;   &lt;span class="mi"&gt;37&lt;/span&gt; &lt;span class="mi"&gt;1508&lt;/span&gt;   &lt;span class="mi"&gt;13&lt;/span&gt;  &lt;span class="mi"&gt;313&lt;/span&gt;   &lt;span class="mi"&gt;66&lt;/span&gt;   &lt;span class="mi"&gt;64&lt;/span&gt;  &lt;span class="mi"&gt;122&lt;/span&gt;   &lt;span class="mi"&gt;65&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;11&lt;/span&gt;   &lt;span class="mi"&gt;16&lt;/span&gt;   &lt;span class="mi"&gt;48&lt;/span&gt;   &lt;span class="mi"&gt;23&lt;/span&gt; &lt;span class="mi"&gt;1662&lt;/span&gt;   &lt;span class="mi"&gt;49&lt;/span&gt;   &lt;span class="mi"&gt;23&lt;/span&gt;  &lt;span class="mi"&gt;134&lt;/span&gt;   &lt;span class="mi"&gt;90&lt;/span&gt;  &lt;span class="mi"&gt;155&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;81&lt;/span&gt;   &lt;span class="mi"&gt;56&lt;/span&gt;   &lt;span class="mi"&gt;30&lt;/span&gt;  &lt;span class="mi"&gt;309&lt;/span&gt;   &lt;span class="mi"&gt;51&lt;/span&gt; &lt;span class="mi"&gt;1255&lt;/span&gt;   &lt;span class="mi"&gt;57&lt;/span&gt;   &lt;span class="mi"&gt;17&lt;/span&gt;  &lt;span class="mi"&gt;129&lt;/span&gt;   &lt;span class="mi"&gt;84&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;29&lt;/span&gt;   &lt;span class="mi"&gt;28&lt;/span&gt;  &lt;span class="mi"&gt;151&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;   &lt;span class="mi"&gt;80&lt;/span&gt;   &lt;span class="mi"&gt;43&lt;/span&gt; &lt;span class="mi"&gt;1914&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;   &lt;span class="mi"&gt;17&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;25&lt;/span&gt;   &lt;span class="mi"&gt;37&lt;/span&gt;   &lt;span class="mi"&gt;33&lt;/span&gt;   &lt;span class="mi"&gt;36&lt;/span&gt;   &lt;span class="mi"&gt;70&lt;/span&gt;   &lt;span class="mi"&gt;30&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;1761&lt;/span&gt;   &lt;span class="mi"&gt;37&lt;/span&gt;  &lt;span class="mi"&gt;388&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;30&lt;/span&gt;   &lt;span class="mi"&gt;80&lt;/span&gt;   &lt;span class="mi"&gt;48&lt;/span&gt;  &lt;span class="mi"&gt;215&lt;/span&gt;   &lt;span class="mi"&gt;16&lt;/span&gt;   &lt;span class="mi"&gt;85&lt;/span&gt;   &lt;span class="mi"&gt;30&lt;/span&gt;   &lt;span class="mi"&gt;19&lt;/span&gt; &lt;span class="mi"&gt;1615&lt;/span&gt;   &lt;span class="mi"&gt;77&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;13&lt;/span&gt;   &lt;span class="mi"&gt;29&lt;/span&gt;   &lt;span class="mi"&gt;68&lt;/span&gt;   &lt;span class="mi"&gt;66&lt;/span&gt;  &lt;span class="mi"&gt;356&lt;/span&gt;   &lt;span class="mi"&gt;74&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="mi"&gt;171&lt;/span&gt;   &lt;span class="mi"&gt;78&lt;/span&gt; &lt;span class="mi"&gt;1533&lt;/span&gt;&lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.7367&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="gradient-boosting"&gt;Gradient Boosting&lt;/h3&gt;
&lt;p&gt;Gradient boosting with &lt;code&gt;xgboost&lt;/code&gt; has won in the Rossmann Store Sales prediction
(&lt;a href="http://blog.kaggle.com/2015/12/21/rossmann-store-sales-winners-interview-1st-place-gert/"&gt;source&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;See also:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://blog.kaggle.com/2015/09/22/caterpillar-winners-interview-1st-place-gilberto-josef-leustagos-mario/"&gt;Caterpillar Winners' Interview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.kaggle.com/2015/10/20/caterpillar-winners-interview-3rd-place-team-shift-workers/"&gt;Caterpillar Winners' Interview: 3rd place&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.kaggle.com/2015/09/28/liberty-mutual-property-inspection-winners-interview-qingchen-wang/"&gt;Liberty Mutual Property Inspection, Winner's Interview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.kaggle.com/2015/10/21/recruit-coupon-purchase-winners-interview-2nd-place-halla-yang/"&gt;Recruit Coupon Purchase Winner's Interview: 2nd place&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.kaggle.com/2015/10/30/dato-winners-interview-2nd-place-mortehu/"&gt;Dato Truly Native? Winner's Interview: 2nd place&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Classifier&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Gradient&lt;/span&gt; &lt;span class="n"&gt;Boosting&lt;/span&gt;
&lt;span class="n"&gt;Training&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;2409.8094&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Testing&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4159&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Confusion&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="o"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;2214&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;24&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;2528&lt;/span&gt;   &lt;span class="mi"&gt;16&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="mi"&gt;2165&lt;/span&gt;   &lt;span class="mi"&gt;34&lt;/span&gt;   &lt;span class="mi"&gt;16&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;   &lt;span class="mi"&gt;22&lt;/span&gt;   &lt;span class="mi"&gt;37&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;   &lt;span class="mi"&gt;27&lt;/span&gt; &lt;span class="mi"&gt;2182&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;   &lt;span class="mi"&gt;42&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;22&lt;/span&gt;   &lt;span class="mi"&gt;37&lt;/span&gt;   &lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;   &lt;span class="mi"&gt;16&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;2088&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;   &lt;span class="mi"&gt;65&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;   &lt;span class="mi"&gt;41&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="mi"&gt;1928&lt;/span&gt;   &lt;span class="mi"&gt;27&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;18&lt;/span&gt;   &lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;15&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;19&lt;/span&gt;   &lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="mi"&gt;2181&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;16&lt;/span&gt;   &lt;span class="mi"&gt;27&lt;/span&gt;   &lt;span class="mi"&gt;15&lt;/span&gt;   &lt;span class="mi"&gt;22&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;2246&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;   &lt;span class="mi"&gt;71&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;20&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt;   &lt;span class="mi"&gt;25&lt;/span&gt;   &lt;span class="mi"&gt;15&lt;/span&gt;   &lt;span class="mi"&gt;29&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt; &lt;span class="mi"&gt;2057&lt;/span&gt;   &lt;span class="mi"&gt;38&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;   &lt;span class="mi"&gt;24&lt;/span&gt;   &lt;span class="mi"&gt;49&lt;/span&gt;   &lt;span class="mi"&gt;15&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;54&lt;/span&gt;   &lt;span class="mi"&gt;17&lt;/span&gt; &lt;span class="mi"&gt;2205&lt;/span&gt;&lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.9435&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="naive-bayes"&gt;Naive Bayes&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Classifier&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Naive&lt;/span&gt; &lt;span class="n"&gt;Bayes&lt;/span&gt;
&lt;span class="n"&gt;Training&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.3814&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Testing&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.8863&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Confusion&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="o"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;2094&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;   &lt;span class="mi"&gt;56&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;69&lt;/span&gt;   &lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="mi"&gt;2432&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;   &lt;span class="mi"&gt;28&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;77&lt;/span&gt;   &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;278&lt;/span&gt;   &lt;span class="mi"&gt;64&lt;/span&gt;  &lt;span class="mi"&gt;703&lt;/span&gt;  &lt;span class="mi"&gt;143&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;  &lt;span class="mi"&gt;558&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;  &lt;span class="mi"&gt;528&lt;/span&gt;   &lt;span class="mi"&gt;26&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;202&lt;/span&gt;  &lt;span class="mi"&gt;136&lt;/span&gt;   &lt;span class="mi"&gt;18&lt;/span&gt;  &lt;span class="mi"&gt;791&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;  &lt;span class="mi"&gt;106&lt;/span&gt;   &lt;span class="mi"&gt;21&lt;/span&gt;  &lt;span class="mi"&gt;886&lt;/span&gt;  &lt;span class="mi"&gt;178&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;96&lt;/span&gt;   &lt;span class="mi"&gt;26&lt;/span&gt;   &lt;span class="mi"&gt;16&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt;  &lt;span class="mi"&gt;296&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;  &lt;span class="mi"&gt;169&lt;/span&gt;   &lt;span class="mi"&gt;13&lt;/span&gt;  &lt;span class="mi"&gt;535&lt;/span&gt; &lt;span class="mi"&gt;1038&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;327&lt;/span&gt;   &lt;span class="mi"&gt;63&lt;/span&gt;   &lt;span class="mi"&gt;15&lt;/span&gt;   &lt;span class="mi"&gt;39&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt;   &lt;span class="mi"&gt;87&lt;/span&gt;  &lt;span class="mi"&gt;100&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="mi"&gt;1253&lt;/span&gt;  &lt;span class="mi"&gt;166&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;34&lt;/span&gt;   &lt;span class="mi"&gt;51&lt;/span&gt;   &lt;span class="mi"&gt;17&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="mi"&gt;2109&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;52&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;19&lt;/span&gt;   &lt;span class="mi"&gt;21&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;23&lt;/span&gt;   &lt;span class="mi"&gt;20&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;  &lt;span class="mi"&gt;737&lt;/span&gt;  &lt;span class="mi"&gt;123&lt;/span&gt; &lt;span class="mi"&gt;1462&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;39&lt;/span&gt;  &lt;span class="mi"&gt;326&lt;/span&gt;   &lt;span class="mi"&gt;13&lt;/span&gt;   &lt;span class="mi"&gt;16&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;   &lt;span class="mi"&gt;18&lt;/span&gt;   &lt;span class="mi"&gt;25&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="mi"&gt;1482&lt;/span&gt;  &lt;span class="mi"&gt;281&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;15&lt;/span&gt;   &lt;span class="mi"&gt;26&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;41&lt;/span&gt;   &lt;span class="mi"&gt;40&lt;/span&gt; &lt;span class="mi"&gt;2240&lt;/span&gt;&lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.5615&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="lda"&gt;LDA&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Classifier&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;LDA&lt;/span&gt;
&lt;span class="n"&gt;Training&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;20.6464&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Testing&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0910&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Confusion&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="o"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;2131&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;   &lt;span class="mi"&gt;47&lt;/span&gt;   &lt;span class="mi"&gt;20&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;   &lt;span class="mi"&gt;36&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;2454&lt;/span&gt;   &lt;span class="mi"&gt;20&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;16&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;71&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;22&lt;/span&gt;   &lt;span class="mi"&gt;71&lt;/span&gt; &lt;span class="mi"&gt;1873&lt;/span&gt;   &lt;span class="mi"&gt;77&lt;/span&gt;   &lt;span class="mi"&gt;51&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;   &lt;span class="mi"&gt;82&lt;/span&gt;   &lt;span class="mi"&gt;20&lt;/span&gt;  &lt;span class="mi"&gt;101&lt;/span&gt;    &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;32&lt;/span&gt;   &lt;span class="mi"&gt;56&lt;/span&gt; &lt;span class="mi"&gt;1992&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;   &lt;span class="mi"&gt;77&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;   &lt;span class="mi"&gt;40&lt;/span&gt;   &lt;span class="mi"&gt;80&lt;/span&gt;   &lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;21&lt;/span&gt;   &lt;span class="mi"&gt;17&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;1983&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;   &lt;span class="mi"&gt;21&lt;/span&gt;  &lt;span class="mi"&gt;142&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;19&lt;/span&gt;   &lt;span class="mi"&gt;18&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;  &lt;span class="mi"&gt;112&lt;/span&gt;   &lt;span class="mi"&gt;18&lt;/span&gt; &lt;span class="mi"&gt;1682&lt;/span&gt;   &lt;span class="mi"&gt;37&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;  &lt;span class="mi"&gt;103&lt;/span&gt;   &lt;span class="mi"&gt;58&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;28&lt;/span&gt;   &lt;span class="mi"&gt;30&lt;/span&gt;   &lt;span class="mi"&gt;32&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;43&lt;/span&gt;   &lt;span class="mi"&gt;51&lt;/span&gt; &lt;span class="mi"&gt;2046&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;37&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;16&lt;/span&gt;   &lt;span class="mi"&gt;57&lt;/span&gt;   &lt;span class="mi"&gt;25&lt;/span&gt;   &lt;span class="mi"&gt;20&lt;/span&gt;   &lt;span class="mi"&gt;70&lt;/span&gt;    &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;1990&lt;/span&gt;   &lt;span class="mi"&gt;11&lt;/span&gt;  &lt;span class="mi"&gt;220&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;   &lt;span class="mi"&gt;9&lt;/span&gt;  &lt;span class="mi"&gt;113&lt;/span&gt;   &lt;span class="mi"&gt;16&lt;/span&gt;   &lt;span class="mi"&gt;64&lt;/span&gt;   &lt;span class="mi"&gt;33&lt;/span&gt;  &lt;span class="mi"&gt;115&lt;/span&gt;   &lt;span class="mi"&gt;13&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="mi"&gt;1781&lt;/span&gt;   &lt;span class="mi"&gt;61&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;15&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;35&lt;/span&gt;  &lt;span class="mi"&gt;133&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;  &lt;span class="mi"&gt;122&lt;/span&gt;   &lt;span class="mi"&gt;22&lt;/span&gt; &lt;span class="mi"&gt;2032&lt;/span&gt;&lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.8642&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="qda"&gt;QDA&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Classifier&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;QDA&lt;/span&gt;
&lt;span class="n"&gt;Training&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;23.0527&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Testing&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;6.2259&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Confusion&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="o"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;2212&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;   &lt;span class="mi"&gt;20&lt;/span&gt;    &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;66&lt;/span&gt; &lt;span class="mi"&gt;2409&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;32&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;   &lt;span class="mi"&gt;39&lt;/span&gt;   &lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;961&lt;/span&gt;   &lt;span class="mi"&gt;25&lt;/span&gt;  &lt;span class="mi"&gt;689&lt;/span&gt;  &lt;span class="mi"&gt;143&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="mi"&gt;310&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;  &lt;span class="mi"&gt;166&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1231&lt;/span&gt;   &lt;span class="mi"&gt;48&lt;/span&gt;   &lt;span class="mi"&gt;29&lt;/span&gt;  &lt;span class="mi"&gt;606&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;13&lt;/span&gt;   &lt;span class="mi"&gt;66&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;  &lt;span class="mi"&gt;232&lt;/span&gt;  &lt;span class="mi"&gt;110&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;810&lt;/span&gt;   &lt;span class="mi"&gt;22&lt;/span&gt;   &lt;span class="mi"&gt;25&lt;/span&gt;   &lt;span class="mi"&gt;27&lt;/span&gt;  &lt;span class="mi"&gt;250&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;  &lt;span class="mi"&gt;143&lt;/span&gt;   &lt;span class="mi"&gt;17&lt;/span&gt;  &lt;span class="mi"&gt;345&lt;/span&gt;  &lt;span class="mi"&gt;568&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;909&lt;/span&gt;   &lt;span class="mi"&gt;15&lt;/span&gt;   &lt;span class="mi"&gt;13&lt;/span&gt;   &lt;span class="mi"&gt;33&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="mi"&gt;214&lt;/span&gt;  &lt;span class="mi"&gt;140&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;  &lt;span class="mi"&gt;666&lt;/span&gt;   &lt;span class="mi"&gt;74&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;83&lt;/span&gt;   &lt;span class="mi"&gt;18&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="mi"&gt;2146&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;81&lt;/span&gt;   &lt;span class="mi"&gt;13&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;52&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="mi"&gt;776&lt;/span&gt;  &lt;span class="mi"&gt;120&lt;/span&gt; &lt;span class="mi"&gt;1352&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;487&lt;/span&gt;  &lt;span class="mi"&gt;181&lt;/span&gt;   &lt;span class="mi"&gt;18&lt;/span&gt;   &lt;span class="mi"&gt;20&lt;/span&gt;    &lt;span class="mi"&gt;6&lt;/span&gt;   &lt;span class="mi"&gt;17&lt;/span&gt;   &lt;span class="mi"&gt;58&lt;/span&gt;    &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;1320&lt;/span&gt;  &lt;span class="mi"&gt;105&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 &lt;span class="o"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;65&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;    &lt;span class="mi"&gt;7&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;23&lt;/span&gt;   &lt;span class="mi"&gt;33&lt;/span&gt; &lt;span class="mi"&gt;2225&lt;/span&gt;&lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;Accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.5561&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="mnist-summary_1"&gt;MNIST Summary&lt;/h2&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Classifier&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;th&gt;Training Time&lt;/th&gt;
&lt;th&gt;Testing Time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;MLP (500:200)&lt;/td&gt;
&lt;td style="text-align: right"&gt;97.98%&lt;/td&gt;
&lt;td style="text-align: right"&gt;79.5696s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.3480s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dropout NN (500:200)&lt;/td&gt;
&lt;td style="text-align: right"&gt;97.80%&lt;/td&gt;
&lt;td style="text-align: right"&gt;118.2654s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.3918s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CNN&lt;br/&gt;(32 5&amp;times;5 filters : 2&amp;times;2 max pool : 64 5&amp;times;5 filters : 2&amp;times;2 max pool : 1024)&lt;/td&gt;
&lt;td style="text-align: right"&gt;97.69%&lt;/td&gt;
&lt;td style="text-align: right"&gt;391.8810s&lt;/td&gt;
&lt;td style="text-align: right"&gt;1.2035s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Adjusted SVM&lt;/td&gt;
&lt;td style="text-align: right"&gt;&lt;b&gt;98.40%&lt;/b&gt;&lt;/td&gt;
&lt;td style="text-align: right"&gt;347.1539s&lt;/td&gt;
&lt;td class="danger" style="text-align: right"&gt;234.5724s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Linear SVM&lt;/td&gt;
&lt;td style="text-align: right"&gt;94.16%&lt;/td&gt;
&lt;td style="text-align: right"&gt;168.6950s&lt;/td&gt;
&lt;td class="danger" style="text-align: right"&gt;158.0101s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Random Forest (n_estimators=50, n_jobs=10)&lt;/td&gt;
&lt;td style="text-align: right"&gt;96.41%&lt;/td&gt;
&lt;td style="text-align: right"&gt;2.1359s&lt;/td&gt;
&lt;td class="danger" style="text-align: right"&gt;26.0763s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Random Forest (n_estimators=10, max_features=1, max_depth=5)&lt;/td&gt;
&lt;td class="danger" style="text-align: right"&gt;57.15%&lt;/td&gt;
&lt;td style="text-align: right"&gt;&lt;b&gt;0.2077s&lt;/b&gt;&lt;/td&gt;
&lt;td class="danger" style="text-align: right"&gt;22.2770s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;k nearest neightbors (k=3)&lt;/td&gt;
&lt;td style="text-align: right"&gt;96.95%&lt;/td&gt;
&lt;td style="text-align: right"&gt;4.6439s&lt;/td&gt;
&lt;td class="danger" style="text-align: right"&gt;1261.7815s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Decision Tree(max_depth=5)&lt;/td&gt;
&lt;td class="danger" style="text-align: right"&gt;65.40%&lt;/td&gt;
&lt;td style="text-align: right"&gt;3.1346s&lt;/td&gt;
&lt;td style="text-align: right"&gt;&lt;b&gt;0.0313s&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Adaboost&lt;/td&gt;
&lt;td class="danger" style="text-align: right"&gt;73.67%&lt;/td&gt;
&lt;td style="text-align: right"&gt;37.6443s&lt;/td&gt;
&lt;td style="text-align: right"&gt;1.5815s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Naive Bayes&lt;/td&gt;
&lt;td class="danger" style="text-align: right"&gt;56.15%&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.3814s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.8863s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LDA&lt;/td&gt;
&lt;td style="text-align: right"&gt;86.42%&lt;/td&gt;
&lt;td style="text-align: right"&gt;20.6464s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0910s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;QDA&lt;/td&gt;
&lt;td class="danger" style="text-align: right"&gt;55.61%&lt;/td&gt;
&lt;td style="text-align: right"&gt;23.0527s&lt;/td&gt;
&lt;td class="danger" style="text-align: right"&gt;6.2259s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gradient Boosting&lt;/td&gt;
&lt;td style="text-align: right"&gt;94.35%&lt;/td&gt;
&lt;td style="text-align: right"&gt;2409.8094s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.4159s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Logistic Regression (C=1)&lt;/td&gt;
&lt;td style="text-align: right"&gt;91.47%&lt;/td&gt;
&lt;td style="text-align: right"&gt;272.1309s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0531s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Logistic Regression (C=10000)&lt;/td&gt;
&lt;td style="text-align: right"&gt;91.23%&lt;/td&gt;
&lt;td style="text-align: right"&gt;1807.0624s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0529s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="iris-summary"&gt;IRIS summary&lt;/h2&gt;
&lt;p&gt;Just for fun, I tried the script from above with very minor adjustments to the
&lt;a href="https://en.wikipedia.org/wiki/Iris_flower_data_set"&gt;IRIS flower dataset&lt;/a&gt;:&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Classifier&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;th&gt;Training Time&lt;/th&gt;
&lt;th&gt;Testing Time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;AdaBoost&lt;/td&gt;
&lt;td style="text-align: right"&gt;92.00%&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.1203s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0101s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Decision Tree&lt;/td&gt;
&lt;td style="text-align: right"&gt;92.00%&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0005s&lt;/td&gt;
&lt;td style="text-align: right"&gt;&lt;b&gt;0.0001s&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gradient Boosting&lt;/td&gt;
&lt;td style="text-align: right"&gt;92.00%&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.2227s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0007s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LDA&lt;/td&gt;
&lt;td style="text-align: right"&gt;&lt;b&gt;96.00%&lt;/b&gt;&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0027s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0002s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NN 20:5&lt;/td&gt;
&lt;td style="text-align: right"&gt;90.00%&lt;/td&gt;
&lt;td style="text-align: right"&gt;1.6628s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0046s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Naive Bayes&lt;/td&gt;
&lt;td style="text-align: right"&gt;90.00%&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0010s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0004s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;QDA&lt;/td&gt;
&lt;td style="text-align: right"&gt;94.00%&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0009s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0003s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Random Forest&lt;/td&gt;
&lt;td style="text-align: right"&gt;90.00%&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.2147s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.1395s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Random Forest 2&lt;/td&gt;
&lt;td style="text-align: right"&gt;90.00%&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.1481s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.1249s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SVM, adj.&lt;/td&gt;
&lt;td style="text-align: right"&gt;90.00%&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0010s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0004s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SVM, linear&lt;/td&gt;
&lt;td class="danger" style="text-align: right"&gt;88.00%&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0006s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0002s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;k nn&lt;/td&gt;
&lt;td style="text-align: right"&gt;92.00%&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0007s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0009s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Logistic Regression (C=1)&lt;/td&gt;
&lt;td class="danger" style="text-align: right"&gt;88.00%&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0011s&lt;/td&gt;
&lt;td style="text-align: right"&gt;&lt;b&gt;0.0001s&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Logistic Regression (C=1000)&lt;/td&gt;
&lt;td style="text-align: right"&gt;92.00%&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0010s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0002s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RBM 100&lt;/td&gt;
&lt;td class="danger" style="text-align: right"&gt;78.00%&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0233s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0003s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RBM 100, n_iter=20&lt;/td&gt;
&lt;td class="danger" style="text-align: right"&gt;70.00%&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0427s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0003s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RBM 200, n_iter=40, LR=0.01, Reg: C=1&lt;/td&gt;
&lt;td class="danger" style="text-align: right"&gt;88.00%&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.2463s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0005s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RBM 200, n_iter=40, LR=0.01, Reg: C=10000&lt;/td&gt;
&lt;td style="text-align: right"&gt;90.00%&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.2437s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0005s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RBM 256&lt;/td&gt;
&lt;td class="danger" style="text-align: right"&gt;84.00%&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0424s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0006s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RBM 512, n_iter=100&lt;/td&gt;
&lt;td class="danger" style="text-align: right"&gt;84.00%&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0723s&lt;/td&gt;
&lt;td style="text-align: right"&gt;0.0010s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="tldr"&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;Neural networks take their time to train and a feeling for the topology, but
their classification results are nice and the testing time is good as well.&lt;/p&gt;
&lt;p&gt;Random Forests and SVMs are also a model a type of model one should think of.
However, the standard implementation is very slow compared to neural networks.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://scikit-learn.org/0.16/modules/generated/sklearn.lda.LDA.html"&gt;&lt;code&gt;sklearn.lda.LDA&lt;/code&gt;&lt;/a&gt;
might also be worth a try. The rest seems to be quite bad compared with those
classifiers.&lt;/p&gt;
&lt;p&gt;The code which generated the examples from above is &lt;a href="https://github.com/MartinThoma/algorithms/tree/master/ML/mnist/many-classifiers"&gt;here&lt;/a&gt;.&lt;/p&gt;</content><category term="Python"></category><category term="Machine Learning"></category><category term="Classification"></category></entry><entry><title>Function Approximation</title><link href="https://martin-thoma.com/function-approximation/" rel="alternate"></link><published>2016-01-18T20:00:00+01:00</published><updated>2016-01-18T20:00:00+01:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2016-01-18:/function-approximation/</id><summary type="html">&lt;p&gt;I was recently quite disappointed by how bad neural networks are for function
approximation (see &lt;a href="http://datascience.stackexchange.com/q/9495/8820"&gt;How should a neural network for unbound function approximation be structured?&lt;/a&gt;). However, I've just found that
Gaussian processes are great for function approximation!&lt;/p&gt;
&lt;p&gt;There are two important types of function approximation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Interpolation&lt;/strong&gt;: What values does …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;I was recently quite disappointed by how bad neural networks are for function
approximation (see &lt;a href="http://datascience.stackexchange.com/q/9495/8820"&gt;How should a neural network for unbound function approximation be structured?&lt;/a&gt;). However, I've just found that
Gaussian processes are great for function approximation!&lt;/p&gt;
&lt;p&gt;There are two important types of function approximation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Interpolation&lt;/strong&gt;: What values does the function have in between of known
  values?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Extrapolation&lt;/strong&gt;: What values does the function have outsive of the known
  values?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I did a couple of very quick examples which look promising.&lt;/p&gt;
&lt;h2 id="examples"&gt;Examples&lt;/h2&gt;
&lt;h3 id="square"&gt;Square&lt;/h3&gt;
&lt;p&gt;Approximating &lt;span class="math"&gt;\(f(x) = x^2\)&lt;/span&gt; worked very good:&lt;/p&gt;
&lt;figure class="aligncenter"&gt;
&lt;a href="../images/2016/01/gauss-x2.png"&gt;&lt;img alt="f(x) = x^2" class="" src="../images/2016/01/gauss-x2.png" style="max-width:500px;"/&gt;&lt;/a&gt;
&lt;figcaption class="text-center"&gt;f(x) = x^2&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;I've tried if with higher order polynomials, more complex polynomials. No
problem.&lt;/p&gt;
&lt;h3 id="sin"&gt;Sin&lt;/h3&gt;
&lt;p&gt;Approximating &lt;span class="math"&gt;\(f(x) = \sin(3x)\)&lt;/span&gt; seems to be more complicated:&lt;/p&gt;
&lt;figure class="aligncenter"&gt;
&lt;a href="../images/2016/01/gaussian-process-sin-3x.png"&gt;&lt;img alt="f(x) = sin(3x)" class="" src="../images/2016/01/gaussian-process-sin-3x.png" style="max-width:500px;"/&gt;&lt;/a&gt;
&lt;figcaption class="text-center"&gt;f(x) = sin(3x)&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;I guess a human would see the wave pattern and do a better job here.&lt;/p&gt;
&lt;h3 id="exp"&gt;exp&lt;/h3&gt;
&lt;p&gt;Approximating &lt;span class="math"&gt;\(f(x) = e^x\)&lt;/span&gt; works similar well as polynomials. One can see
that it does not perfectly fit it, but compared the the range of values seen
before and the distance from the last seen value I think this is absolutely
acceptable:&lt;/p&gt;
&lt;figure class="aligncenter"&gt;
&lt;a href="../images/2016/01/gauss-exponential.png"&gt;&lt;img alt="f(x) = e^x" class="" src="../images/2016/01/gauss-exponential.png" style="max-width:500px;"/&gt;&lt;/a&gt;
&lt;figcaption class="text-center"&gt;f(x) = e^x&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id="noise"&gt;noise&lt;/h3&gt;
&lt;p&gt;It is claimed that Gaussian processes implicitly model noise so that they can
easily deal with noise. However, in my experients this seems not to work so
great. The reason might be that I had points in &lt;span class="math"&gt;\([-3, 3]\)&lt;/span&gt; of the function&lt;/p&gt;
&lt;div class="math"&gt;$$f(x) = x^2$$&lt;/div&gt;
&lt;p&gt;with point-wise gaussian noise &lt;span class="math"&gt;\(N \sim \mathcal{N}(0, 1)\)&lt;/span&gt;. So the noise is
quite domintant on that intervall. One of the examples where it worked better
is:&lt;/p&gt;
&lt;figure class="aligncenter"&gt;
&lt;a href="../images/2016/01/gauss-noise.png"&gt;&lt;img alt="f(x) = x^2 with gaussian noise" class="" src="../images/2016/01/gauss-noise.png" style="max-width:500px;"/&gt;&lt;/a&gt;
&lt;figcaption class="text-center"&gt;f(x) = x^2 with gaussian noise&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id="make-it-brake"&gt;Make it brake&lt;/h3&gt;
&lt;p&gt;I was a bit suspicious if I had another mistake here. So I wanted it to break.
This was the reason why I created the following function&lt;/p&gt;
&lt;div class="math"&gt;$$f(x) = \begin{cases}x^2 &amp;amp;\text{if } x \geq 0\\\\-1 &amp;amp;\text{otherwise}\end{cases}$$&lt;/div&gt;
&lt;figure class="aligncenter"&gt;
&lt;a href="../images/2016/01/gauss-cases.png"&gt;&lt;img alt="Function with discontinuity" class="" src="../images/2016/01/gauss-cases.png" style="max-width:500px;"/&gt;&lt;/a&gt;
&lt;figcaption class="text-center"&gt;Function with discontinuity&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The predicted value is obviously not correct, but you should note that almost
all function values are within the 95% confidence intervall!&lt;/p&gt;
&lt;h2 id="code_1"&gt;Code&lt;/h2&gt;
&lt;p&gt;The following code needs &lt;a href="http://docs.scipy.org/doc/numpy-1.10.1/user/install.html"&gt;&lt;code&gt;numpy&lt;/code&gt;&lt;/a&gt;
and &lt;a href="http://scikit-learn.org/stable/install.html"&gt;&lt;code&gt;sklearn&lt;/code&gt;&lt;/a&gt;. For the plots,
you need &lt;a href="http://matplotlib.org/users/installing.html"&gt;&lt;code&gt;matplotlib&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env python&lt;/span&gt;

&lt;span class="sd"&gt;"""Example how to use gaussion processes for regression."""&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;gaussian_process&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="c1"&gt;# Create the dataset&lt;/span&gt;
    &lt;span class="n"&gt;x_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;atleast_2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;
    &lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;x_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;atleast_2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;

    &lt;span class="c1"&gt;# Define the Regression Modell and fit it&lt;/span&gt;
    &lt;span class="n"&gt;gp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gaussian_process&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GaussianProcess&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta0&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e-2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                          &lt;span class="n"&gt;thetaL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e-4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                          &lt;span class="n"&gt;thetaU&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e-3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;gp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Evaluate the result&lt;/span&gt;
    &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mse&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eval_MSE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"MSE: &lt;/span&gt;&lt;span class="si"&gt;%0.4f&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mse&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"max MSE: &lt;/span&gt;&lt;span class="si"&gt;%0.4f&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mse&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;plot_graph&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mse&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"x^2"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    Function which gets approximated&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;

    &lt;span class="n"&gt;noise&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))]&lt;/span&gt;
    &lt;span class="n"&gt;noise&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;atleast_2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noise&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;noise&lt;/span&gt;
    &lt;span class="c1"&gt;# Totally fails for that one:&lt;/span&gt;
    &lt;span class="c1"&gt;# y = []&lt;/span&gt;
    &lt;span class="c1"&gt;# for el in x:&lt;/span&gt;
    &lt;span class="c1"&gt;#     if el &amp;gt;= 0:&lt;/span&gt;
    &lt;span class="c1"&gt;#         y.append(el**2)&lt;/span&gt;
    &lt;span class="c1"&gt;#     else:&lt;/span&gt;
    &lt;span class="c1"&gt;#         y.append(-1)&lt;/span&gt;
    &lt;span class="c1"&gt;# return np.array(y)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_graph&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mse&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;function_tex&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# Plot the function, the prediction and the 95% confidence interval&lt;/span&gt;
    &lt;span class="c1"&gt;# based on the MSE&lt;/span&gt;
    &lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mse&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;matplotlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;pl&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s1"&gt;'r:'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;'$f(x) = &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;$'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;function_tex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'r.'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;markersize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;'Observations'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'b-'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;'Prediction'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fill&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[::&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt;
            &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;1.9600&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                           &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;1.9600&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;)[::&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt;
            &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'b'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ec&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'None'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'95&lt;/span&gt;&lt;span class="si"&gt;% c&lt;/span&gt;&lt;span class="s1"&gt;onfidence interval'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'$x$'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'$f(x)$'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y_min&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt;
    &lt;span class="n"&gt;y_max&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_min&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_max&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'upper left'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;'__main__'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="see-also"&gt;See also&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.gaussianprocess.org/"&gt;www.gaussianprocess.org&lt;/a&gt;: The definitive book about gaussian processes. It's freely available online!&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Kriging"&gt;Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;sklearn: &lt;a href="http://scikit-learn.org/stable/modules/gaussian_process.html"&gt;Gaussian Processes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;sklearn: &lt;a href="http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gp_regression.html"&gt;Gaussian Processes regression: basic introductory example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Machine Learning"></category><category term="Regression"></category></entry><entry><title>Using SVMs with sklearn</title><link href="https://martin-thoma.com/svm-with-sklearn/" rel="alternate"></link><published>2016-01-14T12:25:00+01:00</published><updated>2016-01-14T12:25:00+01:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2016-01-14:/svm-with-sklearn/</id><summary type="html">&lt;p&gt;Support Vector Machines (SVMs) is a group of powerful classifiers. In this
article, I will give a short impression of how they work. I continue
with an example how to use SVMs with sklearn.&lt;/p&gt;
&lt;h2 id="svm-theory"&gt;SVM theory&lt;/h2&gt;
&lt;p&gt;&lt;abbr title="Support Vector Machines"&gt;SVMs&lt;/abbr&gt; can be described with
5&amp;nbsp;ideas in mind:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;b&gt;Linear, binary classifiers&lt;/b&gt;: If data …&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;Support Vector Machines (SVMs) is a group of powerful classifiers. In this
article, I will give a short impression of how they work. I continue
with an example how to use SVMs with sklearn.&lt;/p&gt;
&lt;h2 id="svm-theory"&gt;SVM theory&lt;/h2&gt;
&lt;p&gt;&lt;abbr title="Support Vector Machines"&gt;SVMs&lt;/abbr&gt; can be described with
5&amp;nbsp;ideas in mind:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;b&gt;Linear, binary classifiers&lt;/b&gt;: If data is linearly separable, it
        can be separated by a hyperplane. There is one hyperplane which
        maximizes the distance to the next datapoints (support vectors). This
        hyperplane should be taken:&lt;br/&gt;
&lt;div&gt;
          $$
          \begin{aligned}
              \text{minimize}_{\mathbf{w}, b}\,&amp;amp;\frac{1}{2} \|\mathbf{w}\|^2\\
              \text{s.t. }&amp;amp; \forall_{i=1}^m y_i \cdot \underbrace{(\langle \mathbf{w}, \mathbf{x}_i\rangle + b)}_{\text{Classification}} \geq 1
          \end{aligned}$$&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Slack variables&lt;/b&gt;: Even if the underlying process which generates
          the features for the two classes is linearly separable, noise can
          make the data not separable. The introduction of &lt;i&gt;slack&amp;nbsp;variables&lt;/i&gt;
          to relax the requirement of linear separability solves
          this problem. The trade-off between accepting some errors and a more
          complex model is weighted by a parameter $C \in \mathbb{R}_0^+$. The
          bigger $C$, the more errors are accepted. The new optimization
          problem is:
          $$
          \begin{aligned}
              \text{minimize}_{\mathbf{w}, b}\,&amp;amp;\frac{1}{2} \|\mathbf{w}\|^2 + C \cdot \sum_{i=1}^m \xi_i\\
              \text{s.t. }&amp;amp; \forall_{i=1}^m y_i \cdot (\langle \mathbf{w}, \mathbf{x}_i\rangle + b) \geq 1 - \xi_i
          \end{aligned}$$

          Note that $0 \le \xi_i \le 1$ means that the data point is within
          the margin, whereas $\xi_i \ge 1$ means it is misclassified. An
          SVM with $C &amp;gt; 0$ is also called a &lt;i&gt;soft-margin SVM&lt;/i&gt;.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Dual Problem&lt;/b&gt;: The primal problem is to find the normal vector $\mathbf{w}$ and the
          bias $b$. The dual problem is to express $\mathbf{w}$ as a linear
          combination of the training data $\mathbf{x}_i$:
          $$\mathbf{w} = \sum_{i=1}^m \alpha_i y_i \mathbf{x}_i$$
          where $y_i \in \{-1, 1\}$ represents the class of the training
          example and $\alpha_i$ are Lagrange multipliers. The usage of
          Lagrange multipliers is explained with some examples
          in [&lt;a href="#ref-smi04" name="ref-smi04-anchor"&gt;Smi04&lt;/a&gt;]. The usage of the Lagrange multipliers
          $\alpha_i$ changes the optimization problem depend on the
          $\alpha_i$ which are weights for the feature vectors. It turns
          out that most $\alpha_i$ will be zero. The non-zero weighted vectors
          are called &lt;i&gt;support&amp;nbsp;vectors&lt;/i&gt;.

          The optimization problem is now, according to [&lt;a href="#ref-bur98" name="ref-bur98-anchor"&gt;Bur98&lt;/a&gt;] (a great read; if you really want to understand it I can recommend it!):
          $$
          \begin{aligned}
              \text{maximize}_{\alpha_i}\,&amp;amp; \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j \langle \mathbf{x}_i, \mathbf{x}_j \rangle\\
              \text{s.t. } &amp;amp; \forall_{i=1}^m 0 \leq \alpha_i \leq C\\
              \text{s.t. } &amp;amp; \sum_{i=1}^m \alpha_i y_i = 0
          \end{aligned}$$&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Kernel-Trick&lt;/b&gt;: Not every dataset is linearly separable. This problem is approached
          by transforming the feature vectors $\mathbf{x}$ with a non-linear
          mapping $\Phi$ into a higher dimensional (probably
          $\infty$-dimensional) space. As the feature vectors $\mathbf{x}$
          are only used within scalar product
          $\langle \mathbf{x}_i, \mathbf{x}_j \rangle$, it is not necessary to
          do the transformation. It is enough to do the calculation
          $$K(\mathbf{x}_i, \mathbf{x}_j) = \langle \mathbf{x}_i, \mathbf{x}_j \rangle$$

          This function $K$ is called a &lt;i&gt;kernel&lt;/i&gt;. The idea of never
          explicitly transforming the vectors $\mathbf{x}_i$ to the higher
          dimensional space is called the &lt;i&gt;kernel&amp;nbsp;trick&lt;/i&gt;. Common kernels
          include the polynomial kernel
          $$K_P(\mathbf{x}_i, \mathbf{x}_j) = (\langle \mathbf{x}_i, \mathbf{x}_j \rangle + r)^p$$
          of degree $p$ and coefficient $r$, the Gaussian &lt;abbr title="Radial Basis Function"&gt;RBF&lt;/abbr&gt; kernel
          $$K_{\text{Gauss}}(\mathbf{x}_i, \mathbf{x}_j) = e^{\frac{-\gamma\|\mathbf{x}_i - \mathbf{x}_j\|^2}{2 \sigma^2}}$$
          and the sigmoid kernel
          $$K_{\text{tanh}}(\mathbf{x}_i, \mathbf{x}_j) = \tanh(\gamma \langle \mathbf{x}_i, \mathbf{x}_j \rangle - r)$$
          where the parameter $\gamma$ determines how much influence single
          training examples have.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Multiple Classes&lt;/b&gt;: By using the &lt;i&gt;one-vs-all&lt;/i&gt; or the
        &lt;i&gt;one-vs-one&lt;/i&gt; strategy it is possible to get a classifying system
        which can distinguish many classes.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A nice visualization of the transformation of the data in a higher-dimensional
space was done by&lt;/p&gt;
&lt;p&gt;TeamGrizzly's channel: &lt;a href="https://youtu.be/9NrALgHFwTo"&gt;Performing nonlinear classification via linear separation in higher dimensional space&lt;/a&gt; on YouTube. 22.11.2010.&lt;/p&gt;
&lt;p&gt;See also:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://math.stackexchange.com/a/1620256/6876"&gt;What is an example of a SVM kernel, where one implicitly uses an infinity-dimensional space?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/a/4630731/562769"&gt;SVM - hard or soft margins?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="sklearn"&gt;sklearn&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;sklearn&lt;/code&gt; is the machine learning toolkit to get started for Python. It has
a very good documentation and many functions. You can find &lt;a href="http://scikit-learn.org/stable/install.html"&gt;installation
instructions&lt;/a&gt; on their website.&lt;/p&gt;
&lt;p&gt;It also includes &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC"&gt;&lt;code&gt;sklearn.svm.SVC&lt;/code&gt;&lt;/a&gt;.
SVC is short for &lt;em&gt;support vector classifier&lt;/em&gt; and this is how you use it for
the MNIST dataset.&lt;/p&gt;
&lt;p&gt;Parameters for which you might want a further explanation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;cache_size&lt;/code&gt;: &lt;a href="http://datascience.stackexchange.com/a/996/8820"&gt;datascience.stackexchange.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env python&lt;/span&gt;

&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;Train a SVM to categorize 28x28 pixel images into digits (MNIST dataset).&lt;/span&gt;
&lt;span class="sd"&gt;"""&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;"""Orchestrate the retrival of data, training and testing."""&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_data&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="c1"&gt;# Get classifier&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.svm&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SVC&lt;/span&gt;
    &lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SVC&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probability&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# cache_size=200,&lt;/span&gt;
              &lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"rbf"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;2.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gamma&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mo"&gt;0073&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Start fitting. This may take a while"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# take all of it - make that number lower for experiments&lt;/span&gt;
    &lt;span class="n"&gt;examples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'train'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;'X'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'train'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;'X'&lt;/span&gt;&lt;span class="p"&gt;][:&lt;/span&gt;&lt;span class="n"&gt;examples&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'train'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;'y'&lt;/span&gt;&lt;span class="p"&gt;][:&lt;/span&gt;&lt;span class="n"&gt;examples&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="n"&gt;analyze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;analyze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    Analyze how well a classifier performs on data.&lt;/span&gt;

&lt;span class="sd"&gt;    Parameters&lt;/span&gt;
&lt;span class="sd"&gt;    ----------&lt;/span&gt;
&lt;span class="sd"&gt;    clf : classifier object&lt;/span&gt;
&lt;span class="sd"&gt;    data : dict&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="c1"&gt;# Get confusion matrix&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;metrics&lt;/span&gt;
    &lt;span class="n"&gt;predicted&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'test'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;'X'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Confusion matrix:&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;
          &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;confusion_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'test'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;'y'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                   &lt;span class="n"&gt;predicted&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Accuracy: &lt;/span&gt;&lt;span class="si"&gt;%0.4f&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'test'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;'y'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                                     &lt;span class="n"&gt;predicted&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="c1"&gt;# Print example&lt;/span&gt;
    &lt;span class="n"&gt;try_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'test'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;'X'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;try_id&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# clf.predict_proba&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"out: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'test'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;'X'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;try_id&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;view_image&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'test'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;'X'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;try_id&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
               &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'test'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;'y'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;try_id&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;view_image&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;""&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    View a single image.&lt;/span&gt;

&lt;span class="sd"&gt;    Parameters&lt;/span&gt;
&lt;span class="sd"&gt;    ----------&lt;/span&gt;
&lt;span class="sd"&gt;    image : numpy array&lt;/span&gt;
&lt;span class="sd"&gt;        Make sure this is of the shape you want.&lt;/span&gt;
&lt;span class="sd"&gt;    label : str&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cm&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Label: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gray&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_data&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    Get data ready to learn with.&lt;/span&gt;

&lt;span class="sd"&gt;    Returns&lt;/span&gt;
&lt;span class="sd"&gt;    -------&lt;/span&gt;
&lt;span class="sd"&gt;    dict&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="c1"&gt;# Load the simple, but similar digits dataset&lt;/span&gt;
        &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_digits&lt;/span&gt;
        &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.utils&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;shuffle&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_digits&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;el&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;el&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;

        &lt;span class="c1"&gt;# Scale data to [-1, 1] - This is of mayor importance!!!&lt;/span&gt;
        &lt;span class="c1"&gt;# In this case, I know the range and thus I can (and should) scale&lt;/span&gt;
        &lt;span class="c1"&gt;# manually. However, this might not always be the case.&lt;/span&gt;
        &lt;span class="c1"&gt;# Then try sklearn.preprocessing.MinMaxScaler or&lt;/span&gt;
        &lt;span class="c1"&gt;# sklearn.preprocessing.StandardScaler&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;255.0&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

        &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
        &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                            &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.33&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                            &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'train'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'X'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                          &lt;span class="s1"&gt;'y'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
                &lt;span class="s1"&gt;'test'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'X'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="s1"&gt;'y'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;}}&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="c1"&gt;# Load the original dataset&lt;/span&gt;
        &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;fetch_mldata&lt;/span&gt;
        &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.utils&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;shuffle&lt;/span&gt;
        &lt;span class="n"&gt;mnist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fetch_mldata&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'MNIST original'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;

        &lt;span class="c1"&gt;# Scale data to [-1, 1] - This is of mayor importance!!!&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;255.0&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

        &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
        &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                            &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.33&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                            &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'train'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'X'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                          &lt;span class="s1"&gt;'y'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
                &lt;span class="s1"&gt;'test'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'X'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="s1"&gt;'y'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;}}&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;'__main__'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="results"&gt;Results&lt;/h2&gt;
&lt;p&gt;The script from above gives the following results:&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;Confusion matrix for an SVM classifier on the MNIST dataset&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;2258&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2566&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2280&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;2304&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;2183&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;2026&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2245&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;2373&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;2166&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;19&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;2329&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Accuracy: 98.40%&lt;/li&gt;
&lt;li&gt;Error: 1.60%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Looks pretty good to me. However, note that there are much better results.
The best on &lt;a href="http://yann.lecun.com/exdb/mnist/"&gt;the official website&lt;/a&gt; has an
error of 0.23% and is a committee of 35 convolutional neural networks.&lt;/p&gt;
&lt;p&gt;The best SVM I could find has an error of 0.56% and applies a polynomial kernel
of degree&amp;nbsp;9 as well as some preprocessing.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a href="#ref-smi04-anchor" name="ref-smi04"&gt;Smi04&lt;/a&gt;] B. T. Smith, &amp;ldquo;Lagrange multipliers tutorial in the context of support
  vector machines,&amp;rdquo; Memorial University of Newfoundland St. John&amp;rsquo;s,
  Newfoundland, Canada, Jun. 2004.&lt;/li&gt;
&lt;li&gt;[&lt;a href="#ref-bur98-anchor" name="ref-bur98"&gt;Bur98&lt;/a&gt;] C. J. Burges, &amp;ldquo;&lt;a href="http://research.microsoft.com/pubs/67119/svmtutorial.pdf"&gt;A tutorial on support vector machines for pattern recognition&lt;/a&gt;&amp;rdquo;, Data&amp;nbsp;mining and knowledge discovery, vol. 2, no. 2, pp.
  121&amp;ndash;167, 1998.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="see-also"&gt;See also&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html"&gt;Recognizing hand-written digits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Trung Huynh's tech blog: &lt;a href="http://www.trungh.com/2013/04/digit-recognition-using-svm-in-python/"&gt;Digit Recognition using SVM in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html"&gt;Classifier comparison&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/supervised_learning.html"&gt;Supervised learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stats.stackexchange.com/questions/80398/how-can-svm-find-an-infinite-feature-space-where-linear-separation-is-always-p"&gt;How can SVM 'find' an infinite feature space where linear separation is always possible?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Python"></category><category term="Machine Learning"></category><category term="SVM"></category><category term="Classification"></category><category term="sklearn"></category></entry><entry><title>Tensor Flow - A quick impression</title><link href="https://martin-thoma.com/tensor-flow-quick/" rel="alternate"></link><published>2015-11-11T22:33:00+01:00</published><updated>2015-11-11T22:33:00+01:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2015-11-11:/tensor-flow-quick/</id><summary type="html">&lt;p&gt;Tensor Flow is a machine learning toolkit which recently got published by
Google. They published it under &lt;a href="https://tldrlegal.com/license/apache-license-2.0-(apache-2.0)"&gt;Apache License 2.0&lt;/a&gt;. Looking at the source code overview, it seems to be mainly C++
with a significant bit of Python.&lt;/p&gt;
&lt;p&gt;I guess the abstract of the
&lt;a href="http://download.tensorflow.org/paper/whitepaper2015.pdf"&gt;Whitepaper&lt;/a&gt; is a good
description …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Tensor Flow is a machine learning toolkit which recently got published by
Google. They published it under &lt;a href="https://tldrlegal.com/license/apache-license-2.0-(apache-2.0)"&gt;Apache License 2.0&lt;/a&gt;. Looking at the source code overview, it seems to be mainly C++
with a significant bit of Python.&lt;/p&gt;
&lt;p&gt;I guess the abstract of the
&lt;a href="http://download.tensorflow.org/paper/whitepaper2015.pdf"&gt;Whitepaper&lt;/a&gt; is a good
description what TensorFlow is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;TensorFlow is an interface for expressing machine learning algorithms, and an
implementation for executing such algorithms. A computation expressed using
TensorFlow can be executed with little or no change on a wide variety of
heterogeneous systems, ranging from mobile devices such as phones and tablets
up to large-scale distributed systems of hundreds of machines and thousands
of computational devices such as GPU cards. The system is flexible and can be
used to express a wide variety of algorithms, including training and
inference algorithms for deep neural network models, and it has been used for
conducting research and for deploying machine learning systems into
production across more than a dozen areas of computer science and other
fields, including speech recognition, computer vision, robotics, information
retrieval, natural language processing, geographic information extraction,
and computational drug discovery. This paper describes the TensorFlow
interface and an implementation of that interface that we have built at
Google. The TensorFlow API and a reference implementation were released as an
open-source package under the Apache 2.0 license in November, 2015 and are
available at www.tensorflow.org.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The core seems to be written in C++, but it has a Python front end.&lt;/p&gt;
&lt;p&gt;By now, I couldn't test much because I just made my GPU machine unusable
(while trying to get the GPU General Computing practical software to run...).
I'll try to expand this article as soon as possible, but I guess it might
take several weeks until I have enough time. Lets see...&lt;/p&gt;
&lt;h2 id="installation"&gt;Installation&lt;/h2&gt;
&lt;p&gt;The documentation about the installation makes a VERY good impression. Better
than anything I can write in a few minutes, so ... &lt;a href="http://tensorflow.org/get_started/os_setup.md"&gt;RTFM&lt;/a&gt;
 😜&lt;/p&gt;
&lt;p&gt;For Linux systems with CUDA and without root privileges, you can install it
with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl --user
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;But remember you have to set the environment variable &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; and
&lt;code&gt;CUDA_HOME&lt;/code&gt;. For many configurations, adding the following lines to your
&lt;code&gt;.bashrc&lt;/code&gt; will work:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;LD_LIBRARY_PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="nv"&gt;$LD_LIBRARY_PATH&lt;/span&gt;&lt;span class="s2"&gt;:/usr/local/cuda/lib64"&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;CUDA_HOME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/usr/local/cuda
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="mnist"&gt;MNIST&lt;/h2&gt;
&lt;p&gt;The following code can be used to check if your Tensor Flow installation is
working. You have to have the &lt;a href="https://gist.github.com/MartinThoma/f37150d0c521f598b08a"&gt;&lt;code&gt;get_mnist_data_tf.py&lt;/code&gt;&lt;/a&gt;
in the same directory as the following script. I've - more or less - directly
copied it from &lt;a href="http://tensorflow.org/tutorials/mnist/pros/index.md"&gt;the tutorial&lt;/a&gt;.
Just execute the script below and see if it finishes without throwing errors.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env python&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;get_mnist_data_tf&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;read_data_sets&lt;/span&gt;
&lt;span class="n"&gt;mnist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;read_data_sets&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"MNIST_data/"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;one_hot&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;InteractiveSession&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"float"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"float"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;784&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize_all_variables&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cross_entropy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;train_step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cross_entropy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;train_step&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]})&lt;/span&gt;
&lt;span class="n"&gt;correct_prediction&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;equal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cast&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correct_prediction&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"float"&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                               &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;}))&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;weight_variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;initial&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;initial&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;bias_variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;initial&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;initial&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'SAME'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;max_pool_2x2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ksize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                        &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'SAME'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;W_conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weight_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;b_conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bias_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;x_image&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;h_conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W_conv1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b_conv1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;h_pool1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;max_pool_2x2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_conv1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;W_conv2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weight_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;b_conv2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bias_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;h_conv2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_pool1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W_conv2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b_conv2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;h_pool2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;max_pool_2x2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_conv2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;W_fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weight_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;b_fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bias_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;h_pool2_flat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_pool2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;h_fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_pool2_flat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W_fc1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b_fc1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;keep_prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"float"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;h_fc1_drop&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_fc1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;W_fc2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weight_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;b_fc2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bias_variable&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;y_conv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_fc1_drop&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W_fc2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b_fc2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;cross_entropy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_conv&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;train_step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AdamOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1e-4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cross_entropy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;correct_prediction&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;equal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_conv&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cast&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correct_prediction&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"float"&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize_all_variables&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;train_accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"step &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s2"&gt;, training accuracy &lt;/span&gt;&lt;span class="si"&gt;%g&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_accuracy&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
  &lt;span class="n"&gt;train_step&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"test accuracy &lt;/span&gt;&lt;span class="si"&gt;%g&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;}))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="observations"&gt;Observations&lt;/h2&gt;
&lt;p&gt;While looking at the MNIST example, I made a couple of observations. Let's
begin with the nice parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tensor Flow has a usable documentation (e.g. &lt;a href="http://tensorflow.org/api_docs/python/nn.md"&gt;The neural network part&lt;/a&gt;). Not great, as Lasagne where you have lots of details (e.g. &lt;a href="http://lasagne.readthedocs.org/en/latest/modules/nonlinearities.html#lasagne.nonlinearities.sigmoid"&gt;activation functions&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Seems to be quite easy to use.&lt;/li&gt;
&lt;li&gt;Seems to be well-tested by simply being used in many different projects by
  Google.&lt;/li&gt;
&lt;li&gt;Just like Theano (and thus Lasagne), Tensor flow has automatic
  differenciation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Not sure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How easy is it to share trained models? In which format would you do so?&lt;/li&gt;
&lt;li&gt;How easy is it to understand a shared model?&lt;/li&gt;
&lt;li&gt;How easy is it to get something new to Tensor Flow like recurrent layers?
  (Actually, this seems rather to show that either the Whitepaper is a bit
  misleading or the documentation / Google search is not that good. In the
  whitepaper they write something about LTSM models, but I couldn't find any docs
  about that. Only by manually going through the manual,
  &lt;a href="http://tensorflow.org/tutorials/recurrent/index.md"&gt;I found it&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Not so nice:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Doesn't work with Python&amp;nbsp;3.&lt;/li&gt;
&lt;li&gt;They don't follow &lt;a href="https://www.python.org/dev/peps/pep-0008/"&gt;PEP8&lt;/a&gt;. I know
  that there is a &lt;a href="https://google.github.io/styleguide/pyguide.html"&gt;Python style guide by Google&lt;/a&gt;,
  but it does not seem to follow that one either. See the next section for
  some more detailed feedback.&lt;/li&gt;
&lt;li&gt;Just like the other Toolkits, you need CUDA. It doesn't work with OpenCL.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="pep8"&gt;PEP8&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Whitespace&lt;/li&gt;
&lt;li&gt;&lt;code&gt;W = tf.Variable(tf.zeros([784, 10]))&lt;/code&gt; should be
    &lt;code&gt;W = tf.Variable(tf.zeros([784, 10]))&lt;/code&gt;.
    Missing whitespaces happened quite often.&lt;/li&gt;
&lt;li&gt;Indent with 2&amp;nbsp;spaces instead of 4&amp;nbsp;spaces. The Google guide seems
    also to use 4.&lt;/li&gt;
&lt;li&gt;Newlines between functions are missing.&lt;/li&gt;
&lt;li&gt;Print statement instead of a print function was used &amp;amp;rightarrow;
  only Python&amp;nbsp;2, not Python&amp;nbsp;3.&lt;/li&gt;
&lt;li&gt;I'm not sure why &lt;code&gt;y_&lt;/code&gt; has the trailing underscore. According to
  &lt;a href="https://www.python.org/dev/peps/pep-0008/#descriptive-naming-styles"&gt;PEP8&lt;/a&gt;,
  a single trailing underscore is used by convention to avoid conflicts with
  Python keyword.&lt;/li&gt;
&lt;li&gt;A mixture of different styles as pointed out on &lt;a href="http://beust.com/weblog/2015/11/09/tensorflows-rough-exterior/"&gt;Credric's Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="videos_1"&gt;Videos&lt;/h2&gt;
&lt;iframe allowfullscreen="" frameborder="0" height="288" src="https://www.youtube-nocookie.com/embed/oZikw5k_2FM?rel=0" width="512"&gt;&lt;/iframe&gt;
&lt;p&gt;Starting at 21m 2s:&lt;/p&gt;
&lt;iframe allowfullscreen="" frameborder="0" height="288" src="https://www.youtube-nocookie.com/embed/90-S1M7Ny_o?rel=0" width="512"&gt;&lt;/iframe&gt;
&lt;h2 id="alternatives-similar-software"&gt;Alternatives / Similar software&lt;/h2&gt;
&lt;p&gt;As I don't really know by now what Tensor Flow is doing, I can't pin-point
alternatives. But I have some educated guesses:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://deeplearning.net/software/theano/"&gt;Theano&lt;/a&gt; has been around for quite
  a while and seems to have a similar approach with its computational graph.
  Enhanced by &lt;a href="http://lasagne.readthedocs.org/en/latest/"&gt;Lasagne&lt;/a&gt;, it is a
  pretty good alternative when it comes to neural networks. Lasagne has an
  exceptionally good documentation, but parts of the tutorial could still be
  improved.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://caffe.berkeleyvision.org/"&gt;Caffe&lt;/a&gt; was something I recently tried.
  I didn't like it too much due to the lack of documentation, but it certainly
  is a big project. Especially when it comes to images.&lt;/li&gt;
&lt;li&gt;I haven't tried, but they look promising:&lt;/li&gt;
&lt;li&gt;&lt;a href="http://chainer.org/"&gt;Chainer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://mxnet.readthedocs.org/en/latest/"&gt;MXNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://rll.berkeley.edu/cgt/"&gt;CGT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://torch.ch/"&gt;Torch&lt;/a&gt; has a very nice example for &lt;a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"&gt;a character
    predictor&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="see-also"&gt;See also&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://tensorflow.org/"&gt;Official Website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/tensorflow"&gt;github.com/tensorflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://download.tensorflow.org/paper/whitepaper2015.pdf"&gt;TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://news.ycombinator.com/item?id=10532957"&gt;news.ycombinator.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.reddit.com/r/programming/comments/3s4vkn/google_brains_deep_learning_library_tensorflow_is/"&gt;reddit.com/r/programming&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Machine Learning"></category><category term="Python"></category><category term="Tensorflow"></category></entry><entry><title>Lasagne for Python Newbies</title><link href="https://martin-thoma.com/lasagne-for-python-newbies/" rel="alternate"></link><published>2015-04-17T19:26:00+02:00</published><updated>2015-04-17T19:26:00+02:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2015-04-17:/lasagne-for-python-newbies/</id><summary type="html">&lt;p&gt;Lasagne is a Python package for training neural networks. The nice thing about
Lasagne is that it is possible to write Python code and execute the training
on nVidea GPUs with automatically generated CUDA code.&lt;/p&gt;
&lt;p&gt;However, installing Lasagne is not that easy. Especially if you are not
familiar with Python …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Lasagne is a Python package for training neural networks. The nice thing about
Lasagne is that it is possible to write Python code and execute the training
on nVidea GPUs with automatically generated CUDA code.&lt;/p&gt;
&lt;p&gt;However, installing Lasagne is not that easy. Especially if you are not
familiar with Python. This article aims to guide you through the installation
process.&lt;/p&gt;
&lt;h2 id="python"&gt;Python&lt;/h2&gt;
&lt;p&gt;Ubuntu-based systems will have Python installed, but I'm not too sure about
pip. You can get it with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo apt-get install python-pip
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Make sure you have Python and &lt;code&gt;pip&lt;/code&gt;, the standard Python package installer.
Type the following commands to check if you have both:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ python --version
Python &lt;span class="m"&gt;2&lt;/span&gt;.7.8

$ pip --version
pip &lt;span class="m"&gt;6&lt;/span&gt;.1.1 from /usr/local/lib/python2.7/dist-packages &lt;span class="o"&gt;(&lt;/span&gt;python &lt;span class="m"&gt;2&lt;/span&gt;.7&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="sklearn"&gt;sklearn&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://scikit-learn.org/stable/"&gt;sklearn&lt;/a&gt; is a nice package for machine
learning. You can install it with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ pip install scikit-learn
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;(When I write commands like this you either have to execute
&lt;code&gt;sudo pip install scikit-learn&lt;/code&gt; or &lt;code&gt;pip install scikit-learn --user&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;This should work without problems.&lt;/p&gt;
&lt;p&gt;Each classifier has a &lt;code&gt;fit&lt;/code&gt; method and a &lt;code&gt;predict&lt;/code&gt; method. See
&lt;a href="http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html"&gt;iris example&lt;/a&gt;
to get a feeling how to use it. It provides a lot of useful functions like
&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html"&gt;&lt;code&gt;train_test_split&lt;/code&gt;&lt;/a&gt;
and has an awesome documentation.&lt;/p&gt;
&lt;p&gt;You don't need this for Lasagne, but it might be good to use sklearn and
Lasagne in combination.&lt;/p&gt;
&lt;h2 id="graphics-drivers-and-cuda"&gt;Graphics drivers and CUDA&lt;/h2&gt;
&lt;p&gt;Make sure CUDA runs on your system by the following commands.
If it doesn't run, you could try the following guides:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://askubuntu.com/q/451672/10425"&gt;Installing and testing CUDA in Ubuntu 14.04&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.r-tutor.com/gpu-computing/cuda-installation/cuda6.5-ubuntu"&gt;Installing CUDA Toolkit 6.5 on Ubuntu 14.04 Linux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-linux/#axzz3XaMVcNwV"&gt;NVIDIA CUDA Getting Started Guide for Linux&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Run&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ nvidia-smi -L
GPU &lt;span class="m"&gt;0&lt;/span&gt;: GeForce GTX TITAN Black &lt;span class="o"&gt;(&lt;/span&gt;UUID: GPU-abcdef12-abcd-1234-1234-01234567890a&lt;span class="o"&gt;)&lt;/span&gt;

$ nvcc --version
nvcc: NVIDIA &lt;span class="o"&gt;(&lt;/span&gt;R&lt;span class="o"&gt;)&lt;/span&gt; Cuda compiler driver
Copyright &lt;span class="o"&gt;(&lt;/span&gt;c&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="m"&gt;2005&lt;/span&gt;-2013 NVIDIA Corporation
Built on Thu_Mar_13_11:58:58_PDT_2014
Cuda compilation tools, release &lt;span class="m"&gt;6&lt;/span&gt;.0, V6.0.1

$ nvidia-smi -a

&lt;span class="o"&gt;==============&lt;/span&gt;NVSMI &lt;span class="nv"&gt;LOG&lt;/span&gt;&lt;span class="o"&gt;==============&lt;/span&gt;

Timestamp                           : Fri Apr &lt;span class="m"&gt;17&lt;/span&gt; &lt;span class="m"&gt;18&lt;/span&gt;:44:41 &lt;span class="m"&gt;2015&lt;/span&gt;
Driver Version                      : &lt;span class="m"&gt;331&lt;/span&gt;.79

Attached GPUs                       : &lt;span class="m"&gt;1&lt;/span&gt;
GPU &lt;span class="m"&gt;0000&lt;/span&gt;:01:00.0
    Product Name                    : GeForce GTX TITAN Black
    Display Mode                    : N/A
    Display Active                  : N/A
    Persistence Mode                : Disabled
    Accounting Mode                 : N/A
    Accounting Mode Buffer Size     : N/A
    Driver Model
        Current                     : N/A
        Pending                     : N/A
    Serial Number                   : N/A
    GPU UUID                        : GPU-fcff168f-a045-2f95-7a4f-8e1cf26a24eb
    Minor Number                    : &lt;span class="m"&gt;0&lt;/span&gt;
    VBIOS Version                   : &lt;span class="m"&gt;80&lt;/span&gt;.80.4E.00.01
    Inforom Version
        Image Version               : N/A
        OEM Object                  : N/A
        ECC Object                  : N/A
        Power Management Object     : N/A
    GPU Operation Mode
        Current                     : N/A
        Pending                     : N/A
    PCI
        Bus                         : 0x01
        Device                      : 0x00
        Domain                      : 0x0000
        Device Id                   : 0x100C10DE
        Bus Id                      : &lt;span class="m"&gt;0000&lt;/span&gt;:01:00.0
        Sub System Id               : 0x106610DE
        GPU Link Info
            PCIe Generation
                Max                 : N/A
                Current             : N/A
            Link Width
                Max                 : N/A
                Current             : N/A
        Bridge Chip
            Type                    : N/A
            Firmware                : N/A
    Fan Speed                       : &lt;span class="m"&gt;26&lt;/span&gt; %
    Performance State               : N/A
    Clocks Throttle Reasons         : N/A
    FB Memory Usage
        Total                       : &lt;span class="m"&gt;6143&lt;/span&gt; MiB
        Used                        : &lt;span class="m"&gt;39&lt;/span&gt; MiB
        Free                        : &lt;span class="m"&gt;6104&lt;/span&gt; MiB
    BAR1 Memory Usage
        Total                       : N/A
        Used                        : N/A
        Free                        : N/A
    Compute Mode                    : Default
    Utilization
        Gpu                         : N/A
        Memory                      : N/A
    Ecc Mode
        Current                     : N/A
        Pending                     : N/A
    ECC Errors
        Volatile
            Single Bit
                Device Memory       : N/A
                Register File       : N/A
                L1 Cache            : N/A
                L2 Cache            : N/A
                Texture Memory      : N/A
                Total               : N/A
            Double Bit
                Device Memory       : N/A
                Register File       : N/A
                L1 Cache            : N/A
                L2 Cache            : N/A
                Texture Memory      : N/A
                Total               : N/A
        Aggregate
            Single Bit
                Device Memory       : N/A
                Register File       : N/A
                L1 Cache            : N/A
                L2 Cache            : N/A
                Texture Memory      : N/A
                Total               : N/A
            Double Bit
                Device Memory       : N/A
                Register File       : N/A
                L1 Cache            : N/A
                L2 Cache            : N/A
                Texture Memory      : N/A
                Total               : N/A
    Retired Pages
        Single Bit ECC              : N/A
        Double Bit ECC              : N/A
        Pending                     : N/A
    Temperature
        Gpu                         : &lt;span class="m"&gt;29&lt;/span&gt; C
    Power Readings
        Power Management            : N/A
        Power Draw                  : N/A
        Power Limit                 : N/A
        Default Power Limit         : N/A
        Enforced Power Limit        : N/A
        Min Power Limit             : N/A
        Max Power Limit             : N/A
    Clocks
        Graphics                    : N/A
        SM                          : N/A
        Memory                      : N/A
    Applications Clocks
        Graphics                    : N/A
        Memory                      : N/A
    Default Applications Clocks
        Graphics                    : N/A
        Memory                      : N/A
    Max Clocks
        Graphics                    : N/A
        SM                          : N/A
        Memory                      : N/A
    Compute Processes               : N/A
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;to see if CUDA was installed correctly.&lt;/p&gt;
&lt;h2 id="theano"&gt;Theano&lt;/h2&gt;
&lt;p&gt;The installation of Theano is a bit tricky (see &lt;a href="http://deeplearning.net/software/theano/install.html"&gt;official page&lt;/a&gt;). I don't remember if I installed additional packages, but try&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo -H pip install theano
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Make sure that your &lt;code&gt;~/.theanorc&lt;/code&gt; exists and looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[global]
device=gpu
floatX=float32
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that &lt;code&gt;float32&lt;/code&gt; is required, even if you have a 64bit system.&lt;/p&gt;
&lt;p&gt;To test your installation, save the following as &lt;code&gt;theanotest.py&lt;/code&gt; and execute
it with &lt;code&gt;python theanotest.py&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env python&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;theano&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shared&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sandbox&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;theano.tensor&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;T&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;

&lt;span class="n"&gt;vlen&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;768&lt;/span&gt;  &lt;span class="c1"&gt;# 10 x #cores x # threads per core&lt;/span&gt;
&lt;span class="n"&gt;iters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;

&lt;span class="n"&gt;rng&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;RandomState&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;shared&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rng&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vlen&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;floatX&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;([],&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maker&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fgraph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toposort&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;t0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;xrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iters&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;t1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;'Looping &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt; times took'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;iters&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;t1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;t0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'seconds'&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;'Result is'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;any&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Elemwise&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maker&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fgraph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toposort&lt;/span&gt;&lt;span class="p"&gt;()]):&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Used the cpu'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Used the gpu'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It should print the following (well, something similar):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Using gpu device 0: GeForce GTX TITAN Black
[GpuElemwise{exp,no_inplace}(&amp;lt;CudaNdarrayType(float32, vector)&amp;gt;), HostFromGpu(GpuElemwise{exp,no_inplace}.0)]
Looping 1000 times took 0.38205909729 seconds
Result is [ 1.23178029  1.61879349  1.52278066 ...,  2.20771813  2.29967761
  1.62323296]
Used the gpu
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Especially "used the gpu" is important. Theano code work on both, CPU and GPU.
If you have a GPU and it does not currently work on a task and it is configured
correctly, then Theano should automatically use the GPU.&lt;/p&gt;
&lt;p&gt;(Don't try to run two Theano scripts at a time ... weird things could happen.)&lt;/p&gt;
&lt;h2 id="lasagne"&gt;Lasagne&lt;/h2&gt;
&lt;p&gt;Lasagne is hosted at Github: &lt;a href="https://github.com/Lasagne/Lasagne"&gt;https://github.com/Lasagne/Lasagne&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Currently, it is not on pip as Sander wants to wait until we get to version
1.0. So you have to install it manually:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git clone https://github.com/Lasagne/Lasagne.git
$ &lt;span class="nb"&gt;cd&lt;/span&gt; Lasagne
Lasagne$ sudo -H python setup.py install
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now you can test if it worked by executing the MNIST example in Lasagne
(&lt;a href="http://yann.lecun.com/exdb/mnist/"&gt;MNIST&lt;/a&gt; is a huge digit dataset). This
might first take some time to download, but should then run quite fast. If
your machine does not use the GPU it will take ages (e.g. on my laptop it takes
about a minute for one epoch)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Lasagne/examples$ python mnist.py
Loading data...
Downloading MNIST dataset
Building model and compiling functions...
/usr/local/lib/python2.7/dist-packages/Lasagne-0.1dev-py2.7.egg/lasagne/init.py:30: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.
  warnings.warn("The uniform initializer no longer uses Glorot et al.'s "
/usr/local/lib/python2.7/dist-packages/Lasagne-0.1dev-py2.7.egg/lasagne/layers/helper.py:55: UserWarning: get_all_layers() has been changed to return layers in topological order. The former implementation is still available as get_all_layers_old(), but will be removed before the first release of Lasagne. To ignore this warning, use `warnings.filterwarnings('ignore', '.*topo.*')`.
  warnings.warn("get_all_layers() has been changed to return layers in "
Starting training...
Epoch 1 of 500 took 72.593s
  training loss:        1.330231
  validation loss:        0.470251
  validation accuracy:        87.54 %%
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="nolearn"&gt;nolearn&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/dnouri/nolearn"&gt;nolearn&lt;/a&gt; is another Python package. It was
created to make using Lasagne even simpler.&lt;/p&gt;
&lt;p&gt;You can install it via&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git clone https://github.com/dnouri/nolearn.git
$ &lt;span class="nb"&gt;cd&lt;/span&gt; nolearn
$ python setup.py install --user
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For example, the following code downloads the MNIST dataset, trains a model on
it and evaluates the result for a single image:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env python&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;lasagne&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;lasagne&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;layers&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;lasagne.updates&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;nesterov_momentum&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nolearn.lasagne&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;NeuralNet&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;gzip&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pickle&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;


&lt;span class="n"&gt;PY2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;version_info&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;PY2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;urllib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;urlretrieve&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;pickle_load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoding&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;pickle&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;urllib.request&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;urlretrieve&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;pickle_load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoding&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;pickle&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;encoding&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;DATA_URL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'http://deeplearning.net/data/mnist/mnist.pkl.gz'&lt;/span&gt;
&lt;span class="n"&gt;DATA_FILENAME&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'mnist.pkl.gz'&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_load_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;DATA_URL&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;DATA_FILENAME&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Load data from `url` and store the result in `filename`."""&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exists&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Downloading MNIST dataset"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;urlretrieve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;gzip&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'rb'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;pickle_load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'latin-1'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;load_data&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;"""Get data with labels, split into training, validation and test set."""&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_load_data&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;X_valid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_valid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y_valid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_valid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;X_valid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X_valid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;y_valid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y_valid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;num_examples_train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="n"&gt;num_examples_valid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X_valid&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="n"&gt;num_examples_test&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;nn_example&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;net1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;NeuralNet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="s1"&gt;'input'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;InputLayer&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'hidden'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DenseLayer&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'output'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DenseLayer&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                &lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="c1"&gt;# layer parameters:&lt;/span&gt;
        &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;hidden_num_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# number of units in 'hidden' layer&lt;/span&gt;
        &lt;span class="n"&gt;output_nonlinearity&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;lasagne&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nonlinearities&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;output_num_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# 10 target values for the digits 0, 1, 2, ..., 9&lt;/span&gt;

        &lt;span class="c1"&gt;# optimization method:&lt;/span&gt;
        &lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;nesterov_momentum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;update_learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;update_momentum&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;

        &lt;span class="n"&gt;max_epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Train the network&lt;/span&gt;
    &lt;span class="n"&gt;net1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'X_train'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'y_train'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="c1"&gt;# Try the network on new data&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Feature vector (100-110): &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'X_test'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;110&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Label: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'y_test'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Predicted: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'X_test'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]])))&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_data&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Got &lt;/span&gt;&lt;span class="si"&gt;%i&lt;/span&gt;&lt;span class="s2"&gt; testing datasets."&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'X_train'&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
    &lt;span class="n"&gt;nn_example&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;'__main__'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The output is&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# Neural Network with 79510 learnable parameters

## Layer information

  #  name      size
---  ------  ------
  0  input      784
  1  hidden     100
  2  output      10

  epoch    train loss    valid loss    train/val    valid acc  dur
-------  ------------  ------------  -----------  -----------  -----
      1       0.59132       0.32314      1.82993      0.90988  1.70s
      2       0.30733       0.26644      1.15348      0.92623  1.96s
      3       0.25879       0.23606      1.09629      0.93363  2.09s
      4       0.22680       0.21424      1.05865      0.93897  2.13s
      5       0.20187       0.19633      1.02827      0.94313  2.21s
      6       0.18129       0.18187      0.99685      0.94758  1.81s
      7       0.16398       0.16992      0.96506      0.95074  2.14s
      8       0.14941       0.16020      0.93265      0.95262  1.88s
      9       0.13704       0.15189      0.90222      0.95460  2.15s
     10       0.12633       0.14464      0.87342      0.95707  2.21s
Feature vector (100-110): [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
Label: 7
Predicted: [7]
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="see-also"&gt;See also&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.pyimagesearch.com/2014/09/22/getting-started-deep-learning-python/"&gt;Getting Started with Deep Learning and Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Python"></category><category term="Machine Learning"></category></entry><entry><title>Gradient Descent, the Delta Rule and Backpropagation</title><link href="https://martin-thoma.com/gradient-descent-the-delta-rule-and-backpropagation/" rel="alternate"></link><published>2014-10-26T21:06:00+01:00</published><updated>2014-10-26T21:06:00+01:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2014-10-26:/gradient-descent-the-delta-rule-and-backpropagation/</id><summary type="html">&lt;p&gt;If you learn about machine learning you will stumble over three terms that are
related:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gradient descent,&lt;/li&gt;
&lt;li&gt;the Delta rule and&lt;/li&gt;
&lt;li&gt;backpropagation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Gradient descent is a way to find a minimum in a high-dimensional space. You
go in direction of the steepest descent.&lt;/p&gt;
&lt;p&gt;The delta rule is an update rule …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you learn about machine learning you will stumble over three terms that are
related:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gradient descent,&lt;/li&gt;
&lt;li&gt;the Delta rule and&lt;/li&gt;
&lt;li&gt;backpropagation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Gradient descent is a way to find a minimum in a high-dimensional space. You
go in direction of the steepest descent.&lt;/p&gt;
&lt;p&gt;The delta rule is an update rule for single layer perceptrons. It makes use
of gradient descent.&lt;/p&gt;
&lt;p&gt;Backpropagation is an efficient implementation of gradient descent, where a
rule can be formulated which has some recursively defined parts. Those parts
belong to neurons of different layers and get calculated from the output-layer
(last layer) to the first hidden layer.&lt;/p&gt;
&lt;h2 id="see-also"&gt;See also&lt;/h2&gt;
&lt;p&gt;Wikipedia pages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Gradient_descent"&gt;Gradient descent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Delta_rule"&gt;Delta rule&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Backpropagation"&gt;Backpropagation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Machine Learning"></category><category term="AI"></category></entry><entry><title>The Twiddle Algorithm</title><link href="https://martin-thoma.com/twiddle/" rel="alternate"></link><published>2014-09-06T13:49:00+02:00</published><updated>2014-09-06T13:49:00+02:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2014-09-06:/twiddle/</id><summary type="html">&lt;p&gt;Twiddle is an algorithm that tries to find a good choice of parameters &lt;span class="math"&gt;\(p\)&lt;/span&gt;
for an algorithm &lt;span class="math"&gt;\(\mathcal{A}\)&lt;/span&gt; that returns an error.&lt;/p&gt;
&lt;p&gt;The algorithm is quite simple to implement. I guess gradient descent might be
better for most cases, but Twiddle does not require any knowledge about the
algorithm …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Twiddle is an algorithm that tries to find a good choice of parameters &lt;span class="math"&gt;\(p\)&lt;/span&gt;
for an algorithm &lt;span class="math"&gt;\(\mathcal{A}\)&lt;/span&gt; that returns an error.&lt;/p&gt;
&lt;p&gt;The algorithm is quite simple to implement. I guess gradient descent might be
better for most cases, but Twiddle does not require any knowledge about the
algorithm &lt;span class="math"&gt;\(\mathcal{A}\)&lt;/span&gt; which might be a big advantage. And you don't have to
calculate the gradient of high dimensional functions, which is nice, too.&lt;/p&gt;
&lt;h2 id="the-algorithm"&gt;The algorithm&lt;/h2&gt;
&lt;p&gt;Here is some pythonic pseudo code. &lt;code&gt;A&lt;/code&gt; is an algorithm that returns an error.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Choose an initialization parameter vector&lt;/span&gt;
&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Define potential changes&lt;/span&gt;
&lt;span class="n"&gt;dp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Calculate the error&lt;/span&gt;
&lt;span class="n"&gt;best_err&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;threshold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.001&lt;/span&gt;

&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;threshold&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;dp&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;err&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;err&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;best_err&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="c1"&gt;# There was some improvement&lt;/span&gt;
            &lt;span class="n"&gt;best_err&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;err&lt;/span&gt;
            &lt;span class="n"&gt;dp&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="c1"&gt;# There was no improvement&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;dp&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  &lt;span class="c1"&gt;# Go into the other direction&lt;/span&gt;
            &lt;span class="n"&gt;err&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;err&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;best_err&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="c1"&gt;# There was an improvement&lt;/span&gt;
                &lt;span class="n"&gt;best_err&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;err&lt;/span&gt;
                &lt;span class="n"&gt;dp&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="mf"&gt;1.05&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;  &lt;span class="c1"&gt;# There was no improvement&lt;/span&gt;
                &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;dp&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="c1"&gt;# As there was no improvement, the step size in either&lt;/span&gt;
                &lt;span class="c1"&gt;# direction, the step size might simply be too big.&lt;/span&gt;
                &lt;span class="n"&gt;dp&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="mf"&gt;0.95&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="see-also"&gt;See also&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=2uQ2BSzDvXs"&gt;Twiddle - CS373 Unit 5 - Udacity&lt;/a&gt;:
  Explanation of Twiddle by Sebastian Thrun&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.htw-mechlab.de/index.php/numerische-optimierung-in-matlab-mit-twiddle-algorithmus/"&gt;Numerische Optimierung in Matlab mit Twiddle-Algorithmus&lt;/a&gt; (German)&lt;/li&gt;
&lt;/ul&gt;</content><category term="Python"></category><category term="AI"></category><category term="Machine Learning"></category></entry><entry><title>GPUs - Supercomputers for your home</title><link href="https://martin-thoma.com/gpus-supercomputers-for-your-home/" rel="alternate"></link><published>2014-08-20T23:26:00+02:00</published><updated>2014-08-20T23:26:00+02:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2014-08-20:/gpus-supercomputers-for-your-home/</id><summary type="html">&lt;p&gt;A few days ago I got some of my neural net code to work with a GPU.
The GPU is called "Tesla C2075". It is able to get 515 GFlops peak performance.
It has 448 CUDA cores that work with 1.15 GHz and it has 6GB GDDR5 memory.&lt;/p&gt;
&lt;p&gt;My …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A few days ago I got some of my neural net code to work with a GPU.
The GPU is called "Tesla C2075". It is able to get 515 GFlops peak performance.
It has 448 CUDA cores that work with 1.15 GHz and it has 6GB GDDR5 memory.&lt;/p&gt;
&lt;p&gt;My code needed about 10 hours to run before. After that, it only needed 10
minutes. That is 60 times faster! The library that did this miracle for me is
called &lt;a href="http://deeplearning.net/software/theano/"&gt;Theano&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Out of curiosity, I've searched for current high-end gamer graphic cards.
I found nVidia Titan Z:&lt;/p&gt;
&lt;iframe allowfullscreen="" frameborder="0" height="288" src="//www.youtube.com/embed/2JjxgJcXVE0" width="512"&gt;&lt;/iframe&gt;
&lt;p&gt;The Titan Z has 5760 CUDA cores. It can get 4061 GFLOPS x2 and has 12 GB of
memory. That technological wonder-work costs only 2802 Euro.&lt;/p&gt;
&lt;p&gt;To put that into perspective: In 2005, you would probably have been on place
68 of the TOP500 supercomputers world wide! (&lt;a href="http://www.top500.org/list/2005/06/?page=1"&gt;source&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Isn't that crazy?&lt;/p&gt;
&lt;h2 id="see-also"&gt;See also:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.geforce.com/hardware/desktop-gpus/geforce-gtx-titan-z/specifications"&gt;Titan Z Specification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://superuser.com/questions/805217/what-are-the-differences-between-scientific-gpus-and-gaming-gpus"&gt;What are the differences between &amp;ldquo;scientific GPUs&amp;rdquo; and &amp;ldquo;gaming GPUs&amp;rdquo;?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Python"></category><category term="Theano"></category><category term="GPU"></category><category term="nVidea"></category><category term="CUDA"></category><category term="AI"></category><category term="Machine Learning"></category></entry><entry><title>A.I. in Computer Games</title><link href="https://martin-thoma.com/ai-in-computer-games/" rel="alternate"></link><published>2014-07-01T23:52:00+02:00</published><updated>2014-07-01T23:52:00+02:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2014-07-01:/ai-in-computer-games/</id><summary type="html">&lt;p&gt;Artificial Intelligences (A.I.s) are computer programs that are able to adjust
their behaviour according to data they see. So A.I.s are able to adjust to
the data a human player generates.&lt;/p&gt;
&lt;figure&gt;
&lt;img alt="Game A.I.s" src="http://imgs.xkcd.com/comics/game_ais.png"&gt;
&lt;figcaption&gt;Game A.I.s&lt;/figcaption&gt;
&lt;/img&gt;&lt;/figure&gt;
&lt;h2 id="solved-games"&gt;Solved games&lt;/h2&gt;
&lt;p&gt;There is a number of games which are definitely …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Artificial Intelligences (A.I.s) are computer programs that are able to adjust
their behaviour according to data they see. So A.I.s are able to adjust to
the data a human player generates.&lt;/p&gt;
&lt;figure&gt;
&lt;img alt="Game A.I.s" src="http://imgs.xkcd.com/comics/game_ais.png"&gt;
&lt;figcaption&gt;Game A.I.s&lt;/figcaption&gt;
&lt;/img&gt;&lt;/figure&gt;
&lt;h2 id="solved-games"&gt;Solved games&lt;/h2&gt;
&lt;p&gt;There is a number of games which are definitely solved. That means the A.I.
plays perfectly:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tic-Tac-Toe&lt;/li&gt;
&lt;li&gt;Connect Four: &lt;a href="http://www.informatik.uni-trier.de/~fernau/DSL0607/Masterthesis-Viergewinnt.pdf"&gt;A Knowledge-based Approach of Connect-Four&lt;/a&gt;. Amsterdam, 1988. Victor Allis.&lt;/li&gt;
&lt;li&gt;Checkers:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See also: &lt;a href="https://en.wikipedia.org/wiki/Solved_game"&gt;Solved Game&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="computers-win-always"&gt;Computers win always&lt;/h2&gt;
&lt;p&gt;A second category are games in which A.I.s always win against human players, but
they don't have a perfect strategy. Or at least we have not proven that they
have a perfect strategy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chess&lt;/li&gt;
&lt;li&gt;Go on a 5&amp;times;5 board&lt;/li&gt;
&lt;li&gt;Reversi on a 4&amp;times;4 board&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Update: There are advances on the 19&amp;times;19 field:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://storage.googleapis.com/deepmind-data/assets/papers/deepmind-mastering-go.pdf"&gt;Paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Nature: &lt;a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html"&gt;Mastering the game of Go with deep neural networks and tree search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;YouTube by nature: &lt;a href="https://www.youtube.com/watch?v=g-dKXOlsf98"&gt;The computer that mastered Go&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Google Blog: &lt;a href="https://googleblog.blogspot.de/2016/01/alphago-machine-learning-game-go.html"&gt;AlphaGo: using machine learning to master the ancient game of Go&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="unspecialized-game-ais"&gt;Unspecialized Game A.I.s&lt;/h2&gt;
&lt;p&gt;The following video is an explanation and demo of software Tom Murphy VII wrote that learns how to play a Nintendo Entertainment System game and then automatically plays it.
It's called "learnfun" (for learn function).&lt;/p&gt;
&lt;p&gt;You might want to skip to 6:13 for the demo:&lt;/p&gt;
&lt;iframe allowfullscreen="" frameborder="0" height="288" src="//www.youtube.com/embed/xOCurBYI_gY" width="512"&gt;&lt;/iframe&gt;
&lt;p&gt;Research paper published in SIGBOVIK 2013: "&lt;a href="http://tom7.org/mario/mario.pdf"&gt;The first level of Super Mario Bros. is easy with lexicographic ordering a and time travel ...after that it gets a little tricky&lt;/a&gt;."&lt;/p&gt;
&lt;p&gt;There is a follow-up video with Zelda, Punch-Out, Dr. Mario, Contra, Wall Street Kid
and Russian Attack:&lt;/p&gt;
&lt;iframe allowfullscreen="" frameborder="0" height="288" src="//www.youtube.com/embed/YGJHR9Ovszs?list=UU3azLjQuz9s5qk76KEXaTvA" width="512"&gt;&lt;/iframe&gt;
&lt;p&gt;And a third episode with Super Mario, Gradius, Mega Man 2, Pro Wrestling, Color
a Dinosaur, Nintendo Pinball, Cliffhanger, Arkanoid, Double Dare, Ice hockey:&lt;/p&gt;
&lt;iframe allowfullscreen="" frameborder="0" height="288" src="//www.youtube.com/embed/Q-WgQcnessA" width="512"&gt;&lt;/iframe&gt;
&lt;h2 id="super-mario-ai-competition"&gt;Super Mario A.I. Competition&lt;/h2&gt;
&lt;iframe allowfullscreen="" frameborder="0" height="288" src="//www.youtube.com/embed/bBZ7kEphv3s?start=385" width="512"&gt;&lt;/iframe&gt;</content><category term="AI"></category><category term="games"></category><category term="Machine Learning"></category></entry><entry><title>Classification with PyBrain</title><link href="https://martin-thoma.com/classification-with-pybrain/" rel="alternate"></link><published>2014-06-16T01:02:00+02:00</published><updated>2014-06-16T01:02:00+02:00</updated><author><name>Martin Thoma</name></author><id>tag:martin-thoma.com,2014-06-16:/classification-with-pybrain/</id><summary type="html">&lt;p&gt;PyBrain is a Python library for machine learning. It's in version 0.31 and
the last change is 2 months ago (&lt;a href="https://github.com/pybrain/pybrain"&gt;source&lt;/a&gt;).
The source code is licensed under &lt;a href="https://tldrlegal.com/license/bsd-3-clause-license-(revised)"&gt;BSD 3 Clause License&lt;/a&gt;. The &lt;a href="http://pybrain.org/docs/"&gt;documentation&lt;/a&gt; is usable, but for
from perfect.&lt;/p&gt;
&lt;h2 id="classification-example"&gt;Classification example&lt;/h2&gt;
&lt;p&gt;The following is a slightly modified example from …&lt;/p&gt;</summary><content type="html">&lt;p&gt;PyBrain is a Python library for machine learning. It's in version 0.31 and
the last change is 2 months ago (&lt;a href="https://github.com/pybrain/pybrain"&gt;source&lt;/a&gt;).
The source code is licensed under &lt;a href="https://tldrlegal.com/license/bsd-3-clause-license-(revised)"&gt;BSD 3 Clause License&lt;/a&gt;. The &lt;a href="http://pybrain.org/docs/"&gt;documentation&lt;/a&gt; is usable, but for
from perfect.&lt;/p&gt;
&lt;h2 id="classification-example"&gt;Classification example&lt;/h2&gt;
&lt;p&gt;The following is a slightly modified example from the documentation. It shows
how PyBrain starts learning to classify 2-dimensional datapoints into 3 classes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env python&lt;/span&gt;
&lt;span class="c1"&gt;# -*- coding: utf-8 -*-&lt;/span&gt;

&lt;span class="c1"&gt;# Source: http://pybrain.org/docs/tutorial/fnn.html&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pybrain.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ClassificationDataSet&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pybrain.utilities&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;percentError&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pybrain.tools.shortcuts&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;buildNetwork&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pybrain.supervised.trainers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BackpropTrainer&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pybrain.structure.modules&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SoftmaxLayer&lt;/span&gt;

&lt;span class="c1"&gt;# Only needed for data generation and graphical output&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pylab&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ioff&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;draw&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;contourf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;plot&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;meshgrid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;where&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy.random&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;multivariate_normal&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;normalvariate&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;generate_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;400&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;INPUT_FEATURES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="n"&gt;CLASSES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
    &lt;span class="n"&gt;means&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="n"&gt;cov&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.2&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.7&lt;/span&gt;&lt;span class="p"&gt;])]&lt;/span&gt;
    &lt;span class="n"&gt;alldata&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ClassificationDataSet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;INPUT_FEATURES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nb_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;CLASSES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;minX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxX&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;minY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxY&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;klass&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;CLASSES&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;klass&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;klass&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;
            &lt;span class="n"&gt;minX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxX&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;minX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;maxX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;minY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxY&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;minY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;maxY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;alldata&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;addSample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;klass&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'minX'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;minX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'maxX'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;maxX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s1"&gt;'minY'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;minY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'maxY'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;maxY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'d'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;alldata&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;generate_data2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;400&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;alldata&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ClassificationDataSet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nb_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;minX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxX&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
    &lt;span class="n"&gt;minY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxY&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;normalvariate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;normalvariate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;minX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxX&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;minX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;maxX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;minY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxY&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;minY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;maxY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;alldata&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;addSample&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;normalvariate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;normalvariate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;alldata&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;addSample&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'minX'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;minX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'maxX'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;maxX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s1"&gt;'minY'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;minY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'maxY'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;maxY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'d'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;alldata&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;perceptron&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hidden_neurons&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weightdecay&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;INPUT_FEATURES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="n"&gt;CLASSES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
    &lt;span class="n"&gt;HIDDEN_NEURONS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hidden_neurons&lt;/span&gt;
    &lt;span class="n"&gt;WEIGHTDECAY&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weightdecay&lt;/span&gt;
    &lt;span class="n"&gt;MOMENTUM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;momentum&lt;/span&gt;

    &lt;span class="c1"&gt;# Generate the labeled set&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generate_data&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="c1"&gt;#g = generate_data2()&lt;/span&gt;
    &lt;span class="n"&gt;alldata&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'d'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;minX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;minY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxY&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'minX'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'maxX'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'minY'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'maxY'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="c1"&gt;# Split data into test and training dataset&lt;/span&gt;
    &lt;span class="n"&gt;tstdata&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trndata&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;alldata&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;splitWithProportion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.25&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;trndata&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_convertToOneOfMany&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# This is necessary, but I don't know why&lt;/span&gt;
    &lt;span class="n"&gt;tstdata&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_convertToOneOfMany&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# http://stackoverflow.com/q/8154674/562769&lt;/span&gt;

    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Number of training patterns: &lt;/span&gt;&lt;span class="si"&gt;%i&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trndata&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Input and output dimensions: &lt;/span&gt;&lt;span class="si"&gt;%i&lt;/span&gt;&lt;span class="s2"&gt;, &lt;/span&gt;&lt;span class="si"&gt;%i&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trndata&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;indim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                   &lt;span class="n"&gt;trndata&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outdim&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Hidden neurons: &lt;/span&gt;&lt;span class="si"&gt;%i&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;HIDDEN_NEURONS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"First sample (input, target, class):"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trndata&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'input'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;trndata&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'target'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;trndata&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'class'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="n"&gt;fnn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;buildNetwork&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trndata&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;indim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;HIDDEN_NEURONS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trndata&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outdim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                       &lt;span class="n"&gt;outclass&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;SoftmaxLayer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;trainer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BackpropTrainer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fnn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;trndata&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;MOMENTUM&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                              &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weightdecay&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;WEIGHTDECAY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Visualization&lt;/span&gt;
    &lt;span class="n"&gt;ticksX&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;minX&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxX&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ticksY&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;minY&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxY&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;meshgrid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ticksX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ticksY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# need column vectors in dataset, not arrays&lt;/span&gt;
    &lt;span class="n"&gt;griddata&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ClassificationDataSet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;INPUT_FEATURES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nb_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;CLASSES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;griddata&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;addSample&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;trainEpochs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;trnresult&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;percentError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;testOnClassData&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
                                 &lt;span class="n"&gt;trndata&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'class'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;tstresult&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;percentError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;testOnClassData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                                 &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tstdata&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;tstdata&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'class'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"epoch: &lt;/span&gt;&lt;span class="si"&gt;%4d&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;totalepochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"  train error: &lt;/span&gt;&lt;span class="si"&gt;%5.2f%%&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;trnresult&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"  test error: &lt;/span&gt;&lt;span class="si"&gt;%5.2f%%&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;tstresult&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;activateOnDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;griddata&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# the highest output activation gives the class&lt;/span&gt;
        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# always print on the same canvas&lt;/span&gt;
        &lt;span class="n"&gt;ioff&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# interactive graphics off&lt;/span&gt;
        &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;   &lt;span class="c1"&gt;# clear the plot&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
            &lt;span class="n"&gt;here&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tstdata&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'class'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tstdata&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'input'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;here&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;tstdata&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'input'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;here&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;'o'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;  &lt;span class="c1"&gt;# safety check against flat field&lt;/span&gt;
            &lt;span class="n"&gt;contourf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c1"&gt;# plot the contour&lt;/span&gt;
        &lt;span class="n"&gt;ion&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# interactive graphics on&lt;/span&gt;
        &lt;span class="n"&gt;draw&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# update the plot&lt;/span&gt;

    &lt;span class="n"&gt;ioff&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;'__main__'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;argparse&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ArgumentParser&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ArgumentDefaultsHelpFormatter&lt;/span&gt;

    &lt;span class="n"&gt;parser&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ArgumentParser&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;formatter_class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ArgumentDefaultsHelpFormatter&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Add more options if you like&lt;/span&gt;
    &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"-H"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;metavar&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"H"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dest&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"hidden_neurons"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"number of neurons in the hidden layer"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"-d"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;metavar&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"W"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dest&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"weightdecay"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"weightdecay"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"-m"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;metavar&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"M"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dest&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"momentum"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"momentum"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;args&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse_args&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;perceptron&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hidden_neurons&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weightdecay&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;See it in action:&lt;/p&gt;
&lt;iframe allowfullscreen="" frameborder="0" height="315" src="//www.youtube.com/embed/FjvO3zqVYSw" width="420"&gt;&lt;/iframe&gt;
&lt;h2 id="resources"&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://pybrain.org/"&gt;Official website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;StackExchange&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/tagged/pybrain"&gt;StackOverflow&lt;/a&gt;: 134 questions&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stats.stackexchange.com/search?q=pybrain"&gt;stats.SE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.reddit.com/r/MachineLearning/search?q=pybrain&amp;amp;restrict_sr=on"&gt;reddit.com/r/MachineLearning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Python"></category><category term="PyBrain"></category><category term="Neural Networks"></category><category term="Classification"></category><category term="AI"></category><category term="Machine Learning"></category></entry></feed>