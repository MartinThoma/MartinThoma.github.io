<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Martin Thoma - Machine Learning</title><link>https://martin-thoma.com/</link><description>A blog about Code, the Web and Cyberculture</description><lastBuildDate>Thu, 08 Jun 2017 20:00:00 +0200</lastBuildDate><item><title>Unsupervised Pretraining</title><link>https://martin-thoma.com/unsupervised-pretraining/</link><description>&lt;p&gt;Neural networks have thousands, often millions of parameters. They take
hundrets of features and predict thousands of classes. The features can often
not be seen independantly, but have to be taken as a whole into consideration.
Most parameters are not independant either. And still, we use only on the order …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Thu, 08 Jun 2017 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-06-08:/unsupervised-pretraining/</guid><category>Machine Learning</category><category>Neural Networks</category></item><item><title>ML Review 5</title><link>https://martin-thoma.com/ml-review-5/</link><description>&lt;p&gt;This Review gives an overview of intersting stuff I stumbled over which are
related to machine learning.&lt;/p&gt;
&lt;h2 id="news"&gt;News&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://canada.googleblog.com/2017/03/canadas-ai-moment.html"&gt;Canada&amp;rsquo;s AI Moment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.theverge.com/2017/4/3/15164490/alibaba-ai-starcraft-combat"&gt;AI is one step closer to mastering StarCraft&lt;/a&gt;: &lt;a href="https://arxiv.org/abs/1703.10069v1"&gt;Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Game&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://deepsense.io/deep-learning-for-satellite-imagery-via-image-segmentation/"&gt;Deep learning for satellite imagery via image segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hardware:&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.nvidia.com/en-us/data-center/volta-gpu-architecture/"&gt;Nvidia …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Thu, 25 May 2017 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-05-25:/ml-review-5/</guid><category>Machine Learning</category><category>matplotlib</category></item><item><title>Natural Language Processing</title><link>https://martin-thoma.com/nlp/</link><description>&lt;p&gt;Natural language processing (NLP) is a scientific field which deals with
language in textual form.&lt;/p&gt;
&lt;h2 id="tasks"&gt;Tasks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Classification&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Is an e-mail spam or not?&lt;/li&gt;
&lt;li&gt;Topic: Is it about sports, science or religion?&lt;/li&gt;
&lt;li&gt;Language: Is it English, German or French?&lt;/li&gt;
&lt;li&gt;Sentence boundary: Is a character the boundary of a sentence or not …&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 24 May 2017 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-05-24:/nlp/</guid><category>Machine Learning</category><category>NLP</category></item><item><title>How to silence TensorFlow</title><link>https://martin-thoma.com/silence-tf/</link><description>&lt;p&gt;Set the environment variable&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;TF_CPP_MIN_LOG_LEVEL=2
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;if you're also annoyed by messages like&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened …&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Thu, 30 Mar 2017 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-03-30:/silence-tf/</guid><category>Tensorflow</category></item><item><title>ZCA Whitening</title><link>https://martin-thoma.com/zca-whitening/</link><description>&lt;p&gt;Whitening is a transformation of data in such a way that its covariance matrix &lt;span class="math"&gt;\(\Sigma\)&lt;/span&gt;
is the identity matrix. Hence whitening decorrelates features. It is used as a
preprocessing method.&lt;/p&gt;
&lt;p&gt;When you have &lt;span class="math"&gt;\(N\)&lt;/span&gt; data points in &lt;span class="math"&gt;\(\mathbb{R}^n\)&lt;/span&gt;, then the covariance matrix
&lt;span class="math"&gt;\(\Sigma \in \mathbb{R}^{n \times …&lt;/span&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 29 Mar 2017 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-03-29:/zca-whitening/</guid><category>Computer Vision</category><category>Machine Learning</category></item><item><title>ML Review 4</title><link>https://martin-thoma.com/ml-review-4/</link><description>&lt;p&gt;This Review gives an overview of intersting stuff I stumbled over which are
related to machine learning.&lt;/p&gt;
&lt;h2 id="new-developments"&gt;New Developments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/MarvinTeichmann/KittiSeg"&gt;KittiSeg&lt;/a&gt; (&lt;a href="https://www.reddit.com/r/MachineLearning/comments/5y8c5w/p_kittiseg_a_toolkit_to_perform_semantic/"&gt;reddit&lt;/a&gt;): A toolkit for semantic segmentation based on &lt;a href="https://github.com/TensorVision/TensorVision"&gt;TensorVision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://research.google.com/audioset/"&gt;AudioSet&lt;/a&gt;: A dataset for accoustic events&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="publications"&gt;Publications&lt;/h2&gt;
&lt;!-- e.g. arXiv --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1703.03864"&gt;Evolution Strategies as a Scalable Alternative to Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1703.00955"&gt;Controllable Text Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1703.02528"&gt;Stopping GAN …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sat, 25 Mar 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-03-25:/ml-review-4/</guid><category>Machine Learning</category><category>matplotlib</category></item><item><title>Object Detection</title><link>https://martin-thoma.com/object-detection/</link><description>&lt;p&gt;Object detection is the following task: You have an image and you want
axis-aligned bounding boxes around every instance of a pre-defined set of
object classes. The set of object classes is finite and typically not bigger
than 1000.&lt;/p&gt;
&lt;p&gt;Here is an easy to use example&lt;/p&gt;
&lt;h2 id="prerequisites"&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/install/"&gt;Tensorflow&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="http://askubuntu.com/q/799184/10425"&gt;CUDA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://askubuntu.com/q/767269/10425"&gt;CuDNN …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sun, 19 Mar 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-03-19:/object-detection/</guid><category>machine learning</category><category>Computer Vision</category><category>Pascal VOC</category></item><item><title>Image Classification</title><link>https://martin-thoma.com/image-classification/</link><description>&lt;p&gt;Image classification is the following task: You have an image and you want to
assign it one label. The set of possible labels is finite and typically not
bigger than 1000.&lt;/p&gt;
&lt;p&gt;So for example, you might ask: What can you see in this image?&lt;/p&gt;
&lt;figure class="wp-caption aligncenter img-thumbnail"&gt;
&lt;img alt="A jellyfish" src="../images/2017/03/moon-jelly.jpg" style="width: 512px;"/&gt;
&lt;figcaption class="text-center"&gt;A jellyfish&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;It is one of …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 15 Mar 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-03-15:/image-classification/</guid><category>machine learning</category><category>Computer Vision</category><category>ImageNet</category></item><item><title>Ensembles</title><link>https://martin-thoma.com/ensembles/</link><description>&lt;p&gt;Models which are combinations of other models are called an &lt;strong&gt;ensemble&lt;/strong&gt;.
The simplest way to combine several classifiers is by averaging their predictions.&lt;/p&gt;
&lt;p&gt;For example, if you have three&amp;nbsp;models and four&amp;nbsp;classes, you might get
predictions like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model 1(x1) = [0.1, 0.5, 0.3, 0.1 …&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sat, 11 Mar 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-03-11:/ensembles/</guid><category>machine learning</category><category>ensembles</category><category>Computer Vision</category><category>CIFAR 100</category></item><item><title>How to download ImageNet</title><link>https://martin-thoma.com/download-data/</link><description>&lt;p&gt;Machine Learning algorithms for computer vision need huge amounts of data.
Here are a few remarks on how to download them.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make sure you have enough space (&lt;code&gt;df -h&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Get a download manager. I use aria2c (&lt;code&gt;sudo apt-get install aria2&lt;/code&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For ImageNet, you have to register at &lt;a href="http://image-net.org/"&gt;image-net.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Download …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Mon, 06 Mar 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-03-06:/download-data/</guid><category>download</category><category>machine learning</category></item><item><title>ML Review 3</title><link>https://martin-thoma.com/ml-review-3/</link><description>&lt;p&gt;This Review gives an overview of intersting stuff I stumbled over which are
related to machine learning.&lt;/p&gt;
&lt;h2 id="new-developments"&gt;New Developments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://developers.googleblog.com/2017/02/announcing-tensorflow-10.html"&gt;Tensorflow 1.0 is released&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="human-strenght-and-super-human-strength-programs"&gt;Human-strenght and Super-human strength programs&lt;/h3&gt;
&lt;p&gt;Super-human strength programs are programs, which surpass even the best human
(on the long run) in a specified task. Human-strength programs …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sat, 25 Feb 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-02-25:/ml-review-3/</guid><category>Machine Learning</category></item><item><title>Best of ML</title><link>https://martin-thoma.com/best-of-ml/</link><description>&lt;p&gt;This post is a summary of articles, websites and material in general about
machine learning.&lt;/p&gt;
&lt;h2 id="articles"&gt;Articles&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;RNNs&lt;ul&gt;
&lt;li&gt;Get an overview: &lt;a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Understand them: &lt;a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;Understanding LSTM Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/"&gt;Using convolutional neural nets to detect facial keypoints tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://hunch.net/?p=22"&gt;Clever Methods of Overfitting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scott.fortmann-roe.com/docs/BiasVariance.html"&gt;Understanding the Bias-Variance Tradeoff&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sebastianruder.com/optimizing-gradient-descent/"&gt;An …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Mon, 13 Feb 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-02-13:/best-of-ml/</guid><category>Machine Learning</category></item><item><title>State of the Art in ML</title><link>https://martin-thoma.com/sota/</link><description>&lt;p&gt;It is difficult to keep track of the current state of the art (SotA). Also, it
might not be directly clear which datasets are relevant. The following list
should help. If you think some datasets / problems / SotA results are missing,
let me know in the comments or via E-mail (&lt;code&gt;info …&lt;/code&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Mon, 06 Feb 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-02-06:/sota/</guid><category>Machine Learning</category><category>Datasets</category></item><item><title>skdata</title><link>https://martin-thoma.com/skdata/</link><description>&lt;p&gt;I really like Machine Learning. I like reading papers, understanding and
evaluating new ideas. But one part I always have to spend quite a bit of time
on is loading the data. It's always a mess to find the datasets, understand
where exactly I can download them and how they've …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Mon, 30 Jan 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-01-30:/skdata/</guid><category>Machine Learning</category><category>Python</category><category>dataset</category></item><item><title>Label Correction Algorithm</title><link>https://martin-thoma.com/label-correction-algorithm/</link><description>&lt;p&gt;The label-correction algorithm is a generalization which includes very common
graph search algorithms like breadth first search (BFS), depth first search (DFS),
&lt;a href="https://en.wikipedia.org/wiki/A*_search_algorithm"&gt;A*&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm"&gt;Dijkstra's algorithm&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Branch_and_bound"&gt;Branch and bound&lt;/a&gt; as special cases.&lt;/p&gt;
&lt;h2 id="pseudocode"&gt;Pseudocode&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter img-thumbnail"&gt;
&lt;img alt="Pseudocode for the Label correction algorithm" src="../images/2016/07/label-correction.png"/&gt;
&lt;figcaption class="text-center"&gt;Pseudocode for the Label correction algorithm&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Explanation:&lt;/p&gt;
&lt;p&gt;First &lt;code&gt;if&lt;/code&gt;: The left hand side is a lower bound …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 25 Jan 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-01-25:/label-correction-algorithm/</guid><category>A.I.</category><category>Algorithms</category><category>Programming</category><category>Python</category><category>Machine Learning</category></item><item><title>Reinforcement Learning</title><link>https://martin-thoma.com/reinforcement-learning/</link><description>&lt;p&gt;Reinforcement learning is a sub-field of mathematics and computer science. It
deals with the following kind of problems: You're given a set of states
&lt;span class="math"&gt;\(\mathcal{X} \subseteq \mathbb{R}^n\)&lt;/span&gt; and a starting state &lt;span class="math"&gt;\(x_0 \in \mathcal{X}\)&lt;/span&gt;.
For every time step &lt;span class="math"&gt;\(k = 0, 1, 2, \dots\)&lt;/span&gt; you have a …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Thu, 29 Dec 2016 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-12-29:/reinforcement-learning/</guid><category>Machine Learning</category><category>RL</category></item><item><title>ML Review 2</title><link>https://martin-thoma.com/ml-review-2/</link><description>&lt;p&gt;This Review gives an overview of intersting stuff I stumbled over which are
related to machine learning. Most of it was posted in &lt;a href="https://ml-ka.de/"&gt;KITs machine learning group&lt;/a&gt; (on Facebook).&lt;/p&gt;
&lt;p&gt;A lot of stuff can be found in my article about &lt;a href="https://martin-thoma.com/nips-2016/"&gt;NIPS 2016&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="new-developments"&gt;New Developments&lt;/h2&gt;
&lt;!-- Trends --&gt;
&lt;ul&gt;
&lt;li&gt;Random forests for courier detection: &lt;a href="https://www.theguardian.com/science/the-lay-scientist/2016/feb/18/has-a-rampaging-ai-algorithm-really-killed-thousands-in-pakistan"&gt;Has …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Tue, 27 Dec 2016 11:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-12-27:/ml-review-2/</guid><category>Machine Learning</category></item><item><title>NIPS 2016</title><link>https://martin-thoma.com/nips-2016/</link><description>&lt;p&gt;The Conference and Workshop on Neural Information Processing Systems (NIPS) is
probably the biggest conference with machine learning / deep learning as a
main topic. This year, about 6000 people attended it. My friend Marvin and me
were supported by &lt;a href="https://www.informatik.kit.edu/begabtenstiftung_informatik_karlsruhe.php"&gt;Begabtenstiftung Informatik Karlsruhe&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The complete program can be found in the …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sat, 24 Dec 2016 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-12-24:/nips-2016/</guid><category>Machine Learning</category><category>research</category></item><item><title>ML Review 1</title><link>https://martin-thoma.com/ml-review-1/</link><description>&lt;p&gt;This Review gives an overview of intersting stuff I stumbled over which are
related to machine learning. Most of it was posted in &lt;a href="https://ml-ka.de/"&gt;KITs machine learning group&lt;/a&gt; (on Facebook).&lt;/p&gt;
&lt;h2 id="new-developments"&gt;New Developments&lt;/h2&gt;
&lt;!-- Trends --&gt;
&lt;ul&gt;
&lt;li&gt;Random forests for courier detection: &lt;a href="https://www.theguardian.com/science/the-lay-scientist/2016/feb/18/has-a-rampaging-ai-algorithm-really-killed-thousands-in-pakistan"&gt;Has a rampaging AI algorithm called Skynet really killed thousands in Pakistan?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="live-demos-and-websites"&gt;Live Demos …&lt;/h2&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Tue, 22 Nov 2016 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-11-22:/ml-review-1/</guid><category>Machine Learning</category></item><item><title>Machine Learning Glossary</title><link>https://martin-thoma.com/ml-glossary/</link><description>&lt;p&gt;The following is a list of short explanations of different terms in machine
learning. The aim is to keep things simple and brief, not to explain the terms
in full detail.&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;dfn id="active-learning"&gt;Active Learning&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;The algorithm gives a pattern and asks for a label.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="backpropagation"&gt;Backpropagation&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;A clever implementation of gradient descent …&lt;/dd&gt;&lt;/dl&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Mon, 24 Oct 2016 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-10-24:/ml-glossary/</guid><category>Machine Learning</category></item><item><title>Average Distance of Random Points in a Unit Hypercube</title><link>https://martin-thoma.com/average-distance-of-points/</link><description>&lt;p&gt;In machine learning, the "curse of dimensionality" is often stated but much
less often explained. At least not in detail. One just gets told that points
are farer away from each other in high dimensional spaces.&lt;/p&gt;
&lt;h2 id="maximum-minimal-distance"&gt;Maximum minimal distance&lt;/h2&gt;
&lt;p&gt;One approach to this is to calculate the maximum minimal distance …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Thu, 20 Oct 2016 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-10-20:/average-distance-of-points/</guid><category>Machine Learning</category></item><item><title>Iterating over Graphs</title><link>https://martin-thoma.com/graph-iteration/</link><description>&lt;p&gt;Today I was thinking if one could iterate over all possible feed forward network
architectures possible. A feed forward network is essentially only a directed
acyclic graph.&lt;/p&gt;
&lt;p&gt;To make things simpler, lets just think about multilayer perceptrons. This
means we only have connections between neighboring layers (and we have layers …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 19 Oct 2016 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-10-19:/graph-iteration/</guid><category>Machine Learning</category></item><item><title>Diverging Gradient Descent</title><link>https://martin-thoma.com/diverging-gradient-descent/</link><description>&lt;p&gt;When you take the function&lt;/p&gt;
&lt;div class="math"&gt;$$f(x, y) = 3x^2 + 3y^2 + 2xy$$&lt;/div&gt;
&lt;p&gt;and start gradient descent at &lt;span class="math"&gt;\(x_0 = (6, 6)\)&lt;/span&gt; with learning rate &lt;span class="math"&gt;\(\eta = \frac{1}{2}\)&lt;/span&gt;
it diverges.&lt;/p&gt;
&lt;h2 id="gradient-descent"&gt;Gradient descent&lt;/h2&gt;
&lt;p&gt;Gradient descent is an optimization rule which starts at a point &lt;span class="math"&gt;\(x_0\)&lt;/span&gt; and
then applies the update rule …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Thu, 21 Jul 2016 16:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-07-21:/diverging-gradient-descent/</guid><category>Gradient Descent</category><category>Optimization</category></item><item><title>XOR tutorial with TensorFlow</title><link>https://martin-thoma.com/tf-xor-tutorial/</link><description>&lt;p&gt;The XOR-Problem is a classification problem, where you only have four data
points with two features. The training set and the test set are exactly
the same in this problem. So the interesting question is only if the model is
able to find a decision boundary which classifies all four …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Tue, 19 Jul 2016 14:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-07-19:/tf-xor-tutorial/</guid><category>Machine Learning</category><category>Python</category><category>Tensorflow</category><category>sklearn</category></item><item><title>Optimization Basics</title><link>https://martin-thoma.com/optimization-basics/</link><description>&lt;p&gt;Optimization is a subfield of mathematics / computer science which deals with finding the best solution. Typically, problems in optimization are stated like this:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
&amp;amp;\underset{x}{\operatorname{minimize}}&amp;amp; &amp;amp; f(x) \\
&amp;amp;\operatorname{subject\;to}
&amp;amp; &amp;amp;g_i(x) \leq 0, \quad i = 1,\dots,m \\
&amp;amp;&amp;amp;&amp;amp;h_i(x) = 0, \quad i = 1, \dots …&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 06 Jul 2016 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-07-06:/optimization-basics/</guid><category>Machine Learning</category><category>optimization</category><category>gradient descent</category></item><item><title>Linear Classification</title><link>https://martin-thoma.com/linear-classification/</link><description>&lt;p&gt;In classification problems you have data points &lt;span class="math"&gt;\(x \in \mathbb{R}^m\)&lt;/span&gt; which you want to classify into one of &lt;span class="math"&gt;\(k \in \mathbb{N}_{\geq 2}\)&lt;/span&gt; classes. This is a supervised task. This means you have &lt;span class="math"&gt;\(n\)&lt;/span&gt; data points for training in a matrix &lt;span class="math"&gt;\(X \in \mathbb{R}^{n …&lt;/span&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 22 Jun 2016 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-06-22:/linear-classification/</guid><category>Algorithms</category><category>Machine Learning</category><category>optimization</category><category>Python</category></item><item><title>Collaborative Filtering</title><link>https://martin-thoma.com/collaborative-filtering/</link><description>&lt;p&gt;Suppose you are in the Netflix setting: You have &lt;span markdown="0"&gt;&lt;span class="math"&gt;\(M\)&lt;/span&gt;&lt;/span&gt;
movies, &lt;span markdown="0"&gt;&lt;span class="math"&gt;\(N\)&lt;/span&gt;&lt;/span&gt; users and integer ratings
&lt;span markdown="0"&gt;&lt;span class="math"&gt;\(1, \dots, K\)&lt;/span&gt;&lt;/span&gt; for some movies by some users.&lt;/p&gt;
&lt;p&gt;You want to predict all missing values. This means you want to say how the
users would rate movies they have not actually rated.&lt;/p&gt;
&lt;p&gt;Please …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 10 Feb 2016 21:35:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-02-10:/collaborative-filtering/</guid><category>Rating</category></item><item><title>Softmax</title><link>https://martin-thoma.com/softmax/</link><description>&lt;p&gt;Softmax is an activation function for multi-layer perceptrons (MLPs). It is
a function which gets applied to a vector in &lt;span markdown="0"&gt;&lt;span class="math"&gt;\(\mathbb{x} \in \mathbb{R}^K\)&lt;/span&gt;&lt;/span&gt;
and returns a vector in &lt;span markdown="0"&gt;&lt;span class="math"&gt;\([0, 1]^K\)&lt;/span&gt;&lt;/span&gt; with the
property that the sum of all elements is 1:&lt;/p&gt;
&lt;div&gt;$$\varphi(\mathbb{x})_j = \frac …&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Tue, 09 Feb 2016 18:09:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-02-09:/softmax/</guid><category>Machine Learning</category><category>Neural Networks</category><category>Activation Functions</category></item><item><title>Comparing Classifiers</title><link>https://martin-thoma.com/comparing-classifiers/</link><description>&lt;p&gt;Classification problems occur quite often and many different classification
algorithms have been described and implemented. But what is the best algorithm
for a given error function and dataset?&lt;/p&gt;
&lt;p&gt;I read questions like "I have problem X. What is the best classifier?" quite
often and my first impulse is always to …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Tue, 19 Jan 2016 20:13:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-01-19:/comparing-classifiers/</guid><category>Python</category><category>Machine Learning</category><category>Classification</category></item><item><title>Function Approximation</title><link>https://martin-thoma.com/function-approximation/</link><description>&lt;p&gt;I was recently quite disappointed by how bad neural networks are for function
approximation (see &lt;a href="http://datascience.stackexchange.com/q/9495/8820"&gt;How should a neural network for unbound function approximation be structured?&lt;/a&gt;). However, I've just found that
Gaussian processes are great for function approximation!&lt;/p&gt;
&lt;p&gt;There are two important types of function approximation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Interpolation&lt;/strong&gt;: What values does …&lt;/li&gt;&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Mon, 18 Jan 2016 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-01-18:/function-approximation/</guid><category>Machine Learning</category><category>Regression</category></item><item><title>Using SVMs with sklearn</title><link>https://martin-thoma.com/svm-with-sklearn/</link><description>&lt;p&gt;Support Vector Machines (SVMs) is a group of powerful classifiers. In this
article, I will give a short impression of how they work. I continue
with an example how to use SVMs with sklearn.&lt;/p&gt;
&lt;h2 id="svm-theory"&gt;SVM theory&lt;/h2&gt;
&lt;p&gt;&lt;abbr title="Support Vector Machines"&gt;SVMs&lt;/abbr&gt; can be described with
5&amp;nbsp;ideas in mind:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;b&gt;Linear, binary classifiers&lt;/b&gt;: If data …&lt;/li&gt;&lt;/ol&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Thu, 14 Jan 2016 12:25:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-01-14:/svm-with-sklearn/</guid><category>Python</category><category>Machine Learning</category><category>SVM</category><category>Classification</category><category>sklearn</category></item><item><title>Tensor Flow - A quick impression</title><link>https://martin-thoma.com/tensor-flow-quick/</link><description>&lt;p&gt;Tensor Flow is a machine learning toolkit which recently got published by
Google. They published it under &lt;a href="https://tldrlegal.com/license/apache-license-2.0-(apache-2.0)"&gt;Apache License 2.0&lt;/a&gt;. Looking at the source code overview, it seems to be mainly C++
with a significant bit of Python.&lt;/p&gt;
&lt;p&gt;I guess the abstract of the
&lt;a href="http://download.tensorflow.org/paper/whitepaper2015.pdf"&gt;Whitepaper&lt;/a&gt; is a good
description …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 11 Nov 2015 22:33:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2015-11-11:/tensor-flow-quick/</guid><category>Machine Learning</category><category>Python</category><category>Tensorflow</category></item><item><title>Lasagne for Python Newbies</title><link>https://martin-thoma.com/lasagne-for-python-newbies/</link><description>&lt;p&gt;Lasagne is a Python package for training neural networks. The nice thing about
Lasagne is that it is possible to write Python code and execute the training
on nVidea GPUs with automatically generated CUDA code.&lt;/p&gt;
&lt;p&gt;However, installing Lasagne is not that easy. Especially if you are not
familiar with Python …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Fri, 17 Apr 2015 19:26:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2015-04-17:/lasagne-for-python-newbies/</guid><category>Python</category><category>Machine Learning</category></item><item><title>Gradient Descent, the Delta Rule and Backpropagation</title><link>https://martin-thoma.com/gradient-descent-the-delta-rule-and-backpropagation/</link><description>&lt;p&gt;If you learn about machine learning you will stumble over three terms that are
related:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gradient descent,&lt;/li&gt;
&lt;li&gt;the Delta rule and&lt;/li&gt;
&lt;li&gt;backpropagation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Gradient descent is a way to find a minimum in a high-dimensional space. You
go in direction of the steepest descent.&lt;/p&gt;
&lt;p&gt;The delta rule is an update rule …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sun, 26 Oct 2014 21:06:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2014-10-26:/gradient-descent-the-delta-rule-and-backpropagation/</guid><category>Machine Learning</category><category>AI</category></item><item><title>The Twiddle Algorithm</title><link>https://martin-thoma.com/twiddle/</link><description>&lt;p&gt;Twiddle is an algorithm that tries to find a good choice of parameters &lt;span class="math"&gt;\(p\)&lt;/span&gt;
for an algorithm &lt;span class="math"&gt;\(\mathcal{A}\)&lt;/span&gt; that returns an error.&lt;/p&gt;
&lt;p&gt;The algorithm is quite simple to implement. I guess gradient descent might be
better for most cases, but Twiddle does not require any knowledge about the
algorithm …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sat, 06 Sep 2014 13:49:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2014-09-06:/twiddle/</guid><category>Python</category><category>AI</category><category>Machine Learning</category></item><item><title>GPUs - Supercomputers for your home</title><link>https://martin-thoma.com/gpus-supercomputers-for-your-home/</link><description>&lt;p&gt;A few days ago I got some of my neural net code to work with a GPU.
The GPU is called "Tesla C2075". It is able to get 515 GFlops peak performance.
It has 448 CUDA cores that work with 1.15 GHz and it has 6GB GDDR5 memory.&lt;/p&gt;
&lt;p&gt;My …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 20 Aug 2014 23:26:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2014-08-20:/gpus-supercomputers-for-your-home/</guid><category>Python</category><category>Theano</category><category>GPU</category><category>nVidea</category><category>CUDA</category><category>AI</category><category>Machine Learning</category></item><item><title>A.I. in Computer Games</title><link>https://martin-thoma.com/ai-in-computer-games/</link><description>&lt;p&gt;Artificial Intelligences (A.I.s) are computer programs that are able to adjust
their behaviour according to data they see. So A.I.s are able to adjust to
the data a human player generates.&lt;/p&gt;
&lt;figure&gt;
&lt;img alt="Game A.I.s" src="http://imgs.xkcd.com/comics/game_ais.png"&gt;
&lt;figcaption&gt;Game A.I.s&lt;/figcaption&gt;
&lt;/img&gt;&lt;/figure&gt;
&lt;h2 id="solved-games"&gt;Solved games&lt;/h2&gt;
&lt;p&gt;There is a number of games which are definitely …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Tue, 01 Jul 2014 23:52:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2014-07-01:/ai-in-computer-games/</guid><category>AI</category><category>games</category><category>Machine Learning</category></item><item><title>Classification with PyBrain</title><link>https://martin-thoma.com/classification-with-pybrain/</link><description>&lt;p&gt;PyBrain is a Python library for machine learning. It's in version 0.31 and
the last change is 2 months ago (&lt;a href="https://github.com/pybrain/pybrain"&gt;source&lt;/a&gt;).
The source code is licensed under &lt;a href="https://tldrlegal.com/license/bsd-3-clause-license-(revised)"&gt;BSD 3 Clause License&lt;/a&gt;. The &lt;a href="http://pybrain.org/docs/"&gt;documentation&lt;/a&gt; is usable, but for
from perfect.&lt;/p&gt;
&lt;h2 id="classification-example"&gt;Classification example&lt;/h2&gt;
&lt;p&gt;The following is a slightly modified example from …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Mon, 16 Jun 2014 01:02:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2014-06-16:/classification-with-pybrain/</guid><category>Python</category><category>PyBrain</category><category>Neural Networks</category><category>Classification</category><category>AI</category><category>Machine Learning</category></item></channel></rss>