<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Martin Thoma - Machine Learning</title><link>https://martin-thoma.com/</link><description>A blog about Code, the Web and Cyberculture</description><lastBuildDate>Mon, 02 Dec 2019 20:00:00 +0100</lastBuildDate><item><title>Data Applications</title><link>https://martin-thoma.com/data-applications/</link><description>&lt;p&gt;"Data is the new oil", "we need to be data driven",
"we need to apply AI to keep being competitive" are some of the prashes I hear
a lot. As I haven't seen yet a clear article pointing out what is done with
the data ... here you are ðŸ™‚&lt;/p&gt;
&lt;h2 id="why-its-complicated"&gt;Why it's â€¦&lt;/h2&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Mon, 02 Dec 2019 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2019-12-02:/data-applications/</guid><category>Data</category><category>IaC</category><category>AWS</category><category>Kafka</category><category>Data Science</category><category>Data Engineering</category></item><item><title>Siamese Networks</title><link>https://martin-thoma.com/siamese-networks/</link><description>&lt;p&gt;Siamese Networks are feature extractors trained to learn an embedding in &lt;span class="math"&gt;\(\mathbb{R}^n\)&lt;/span&gt;
where not the absolute output is important, but the relative one.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter img-thumbnail"&gt;
&lt;a href="../images/2019/04/siamese-networks.png"&gt;&lt;img alt="Schema of a Siamese Network" src="../images/2019/04/siamese-networks.png" style="width: 512px;"/&gt;&lt;/a&gt;
&lt;figcaption class="text-center"&gt;Schema of a Siamese Network $m_1$.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The original paper&lt;sup id="fnref-1"&gt;&lt;a class="footnote-ref" href="#fn-1"&gt;1&lt;/a&gt;&lt;/sup&gt; was about signature verification. You have one original
signature and one that might be the â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sat, 27 Apr 2019 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2019-04-27:/siamese-networks/</guid><category>Machine Learning</category><category>Neural Networks</category></item><item><title>WiLI-2018</title><link>https://martin-thoma.com/wili/</link><description>&lt;p&gt;WiLI-2018, the Wikipedia Language Identification database, is a collection of
sentences from Wikipedia of different languages. It can be used to test how hard
it is to distinguish different languages.&lt;/p&gt;
&lt;p&gt;If you want to get to the data, go to &lt;a href="https://zenodo.org/record/841984"&gt;zenodo.org&lt;/a&gt;.
If you want to get to the publication â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sun, 13 Jan 2019 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2019-01-13:/wili/</guid><category>Machine Learning</category></item><item><title>Expert Systems</title><link>https://martin-thoma.com/expert-systems/</link><description>&lt;div class="info"&gt;This is an article I had for quite a while as a draft. As part of my yearly cleanup, I've published it without finishing it. It might not be finished or have other problems.&lt;/div&gt;
&lt;p&gt;Science fiction movies are full of advanced systems for medical analysis and
treatment:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stargate SG1: The â€¦&lt;/li&gt;&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sun, 30 Dec 2018 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2018-12-30:/expert-systems/</guid><category>Machine Learning</category><category>Expert Systems</category></item><item><title>Techniques for Analyzing ML models</title><link>https://martin-thoma.com/model-analysis/</link><description>&lt;div class="info"&gt;This is an article I had for quite a while as a draft. As part of my yearly cleanup, I've published it without finishing it. It might not be finished or have other problems.&lt;/div&gt;
&lt;p&gt;Techniques for model analysis:&lt;/p&gt;
&lt;p&gt;Prediction-Based:
&lt;em&gt; Decision boundaries
&lt;/em&gt; LIME
&lt;em&gt; &lt;a href="https://martin-thoma.com/feature-importance/"&gt;Feature importance&lt;/a&gt;
&lt;/em&gt; &lt;a href="https://www.kaggle.com/dansbecker/shap-values"&gt;SHAP values&lt;/a&gt;
&lt;em&gt; &lt;a href="https://www.kaggle.com/dansbecker/partial-plots"&gt;Partial Dependence Plots&lt;/a&gt;
&lt;/em&gt; Sensitivity â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sun, 30 Dec 2018 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2018-12-30:/model-analysis/</guid><category>Machine Learning</category></item><item><title>Code Challenges in ML</title><link>https://martin-thoma.com/code-challenges-in-ml/</link><description>&lt;p&gt;Having machines that can write software is the wet dreem of probably every
company. Instead of having years of development you just tell the machine what
to do and it automatically creates the software.&lt;/p&gt;
&lt;p&gt;As you might have guessed, we are not there yet. Not even close. But a friend â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Tue, 25 Dec 2018 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2018-12-25:/code-challenges-in-ml/</guid><category>Machine Learning</category></item><item><title>Perfect Models</title><link>https://martin-thoma.com/perfect-models/</link><description>&lt;p&gt;When you develop a model, you want the optimal model. The perfect one.&lt;/p&gt;
&lt;p&gt;The first problem with that desire are diagonal goals:&lt;/p&gt;
&lt;figure class="wp-caption aligncenter img-thumbnail"&gt;
&lt;a href="../images/2018/12/model-pick-2.png"&gt;&lt;img alt="A jellyfish" src="../images/2018/12/model-pick-2.png" style="width: 512px;"/&gt;&lt;/a&gt;
&lt;figcaption class="text-center"&gt;Diagonal goals in model development&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Typical goals when designing a model are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Quality: Have a high accuracy, low error, high &lt;span class="math"&gt;\(F_\beta\)&lt;/span&gt; score, ...&lt;/li&gt;
&lt;li&gt;Production&lt;ul&gt;
&lt;li&gt;Inference speed: The faster â€¦&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Fri, 14 Dec 2018 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2018-12-14:/perfect-models/</guid><category>Machine Learning</category></item><item><title>Recommender Systems</title><link>https://martin-thoma.com/recommender-systems/</link><description>&lt;p&gt;I recently became interested in recommender systems. You know, the thing on
Amazon that tells you which products you might be interested in. Or the stuff
on Spotify that gives you a song you might like. On YouTube the next videos
shown. On StumbleUpon, your next stumble. On a news â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sun, 07 Oct 2018 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2018-10-07:/recommender-systems/</guid><category>Machine Learning</category></item><item><title>Regression</title><link>https://martin-thoma.com/regression/</link><description>&lt;div class="info"&gt;A while ago, this link pointed to the content which is now in the &lt;a href="https://martin-thoma.com/forecasting/"&gt;Forecasting article&lt;/a&gt;.&lt;/div&gt;
&lt;p&gt;Regression is one of the core tasks in machine learning. In this task, you get
some input and your target variable is a single floating point number. For
example, predicting the price of a â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 18 Jul 2018 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2018-07-18:/regression/</guid><category>Machine Learning</category><category>Regression</category></item><item><title>Evaluation of binary classifiers</title><link>https://martin-thoma.com/binary-classifier-evaluation/</link><description>&lt;p&gt;Binary classification is likely the simplest task in machine learning. It is
typically solved with Random Forests, Neural Networks, SVMs or a
&lt;a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier"&gt;naive Bayes classifier&lt;/a&gt;.
For all of them, you have to measure how well you are doing. In this article,
I give an overview over the different metrics for â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sat, 23 Jun 2018 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2018-06-23:/binary-classifier-evaluation/</guid><category>Machine Learning</category></item><item><title>Data Scientist Interviews</title><link>https://martin-thoma.com/ds-interview/</link><description>&lt;p&gt;Interviews for Data Scientists - which traits and skills are important for a
Data Scientist? Which questions should you be able to answer as a Data
Scientist?&lt;/p&gt;
&lt;h2 id="skillset"&gt;Skillset&lt;/h2&gt;
&lt;p&gt;The following is a typical skillset I expect from a data scientist. It might be
that there are some data scientists with a â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Thu, 14 Jun 2018 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2018-06-14:/ds-interview/</guid><category>Machine Learning</category><category>Data Science</category></item><item><title>Feature Importance</title><link>https://martin-thoma.com/feature-importance/</link><description>&lt;p&gt;Trust is important for a Data Scientist. If you are in a position where you can
apply a classification / regression model where the company used rules before,
you have to be able to build trust why your model is better than the old
system. Stakeholders want to understand what the â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Tue, 05 Jun 2018 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2018-06-05:/feature-importance/</guid><category>Machine Learning</category><category>Data Science</category></item><item><title>Data Science Project Guide</title><link>https://martin-thoma.com/ds-project-guide/</link><description>&lt;p&gt;Data Science projects are either pure analytics projects, or Software projects,
or both.&lt;/p&gt;
&lt;p&gt;Three typical data science project phases are understanding the data, creating
a software component and then a story telling part:&lt;/p&gt;
&lt;figure class="wp-caption aligncenter img-thumbnail"&gt;
&lt;img alt="Typical Data Science project phases" src="../images/2018/06/ds-flowchart.png" style="width: 512px;"/&gt;
&lt;figcaption class="text-center"&gt;Typical Data Science project phases&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;There are two other steps which I left out as I don't â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sun, 03 Jun 2018 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2018-06-03:/ds-project-guide/</guid><category>Machine Learning</category><category>Data Science</category></item><item><title>Data Science - An Overview</title><link>https://martin-thoma.com/data-science/</link><description>&lt;p&gt;Data Science recently became popular. Currently are 154 open job positions on
Indeed.com for Data Scientists in Munich. To put it into context: There are 186
Android developer positions open, 527 Dev Ops, 753 frontend, 812 backend. So
it's still fairly small, but in the same ballpark.&lt;/p&gt;
&lt;p&gt;I wanted â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sat, 02 Jun 2018 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2018-06-02:/data-science/</guid><category>Machine Learning</category></item><item><title>Forecasting</title><link>https://martin-thoma.com/forecasting/</link><description>&lt;p&gt;Today, I played a bit with the &lt;a href="https://datamarket.com/data/set/22u3/international-airline-passengers-monthly-totals-in-thousands-jan-49-dec-60#!ds=22u3&amp;amp;display=line"&gt;International airline passengers dataset&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It's only feature is a date given by year and month. One should predict the
number of passengers (in thousands).&lt;/p&gt;
&lt;h2 id="code"&gt;Code&lt;/h2&gt;
&lt;p&gt;See &lt;a href="https://github.com/MartinThoma/algorithms/blob/master/ML/regression/airline-passengers/main.py"&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="results"&gt;Results&lt;/h2&gt;
&lt;p&gt;See the scoring functions are explained in the &lt;a href="https://martin-thoma.com/regression/"&gt;regression article&lt;/a&gt;.&lt;/p&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;th&gt;name&lt;/th&gt;
&lt;th&gt;training time&lt;/th&gt;
&lt;th&gt;testing time â€¦&lt;/th&gt;&lt;/tr&gt;&lt;/table&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Mon, 16 Apr 2018 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2018-04-16:/forecasting/</guid><category>Machine Learning</category><category>Regression</category><category>Time Series</category></item><item><title>SLAM</title><link>https://martin-thoma.com/slam/</link><description>&lt;div class="info"&gt;This is an article I had for quite a while as a draft. As part of my yearly cleanup, I've published it without finishing it. It might not be finished or have other problems.&lt;/div&gt;
&lt;p&gt;SLAM is short of simultaneous localization and mapping. It is a term used to
describe the â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Mon, 26 Mar 2018 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2018-03-26:/slam/</guid><category>Artificial Intelligence</category><category>Robotics</category></item><item><title>Sankey Diagrams</title><link>https://martin-thoma.com/sankey-diagrams/</link><description>&lt;p&gt;Sankey diagrams can be used to visualize the breakdown of money / electriciy.
You have a big base value like the energy used by an average single German in
December 2017. Then you build big clusters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transportation&lt;/li&gt;
&lt;li&gt;Household&lt;/li&gt;
&lt;li&gt;Other&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and you might be able to split household up again:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Heating&lt;/li&gt;
&lt;li&gt;Refrigerator â€¦&lt;/li&gt;&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Tue, 09 Jan 2018 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2018-01-09:/sankey-diagrams/</guid><category>Data Analysis</category><category>Data Visualization</category></item><item><title>Gradient Boosting</title><link>https://martin-thoma.com/gradient-boosting/</link><description>&lt;p&gt;Boosting is an ensemble learning technique. Gradient Boosting is a technique
for regressors.&lt;/p&gt;
&lt;p&gt;See the following video "Ensembles (3): Gradient Boosting" by Prof. Alexander
Ihler of the University of California for more information:&lt;/p&gt;
&lt;iframe allow="encrypted-media" allowfullscreen="" frameborder="0" gesture="media" height="315" src="https://www.youtube-nocookie.com/embed/sRktKszFmSk" width="560"&gt;&lt;/iframe&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Fri, 29 Dec 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-12-29:/gradient-boosting/</guid><category>Machine Learning</category></item><item><title>ML Showcases</title><link>https://martin-thoma.com/ml-showcases/</link><description>&lt;p&gt;There are many awesome examples out there where you can get a very direct
feeling for what Machine Learning is. I'll collect a couple of them here.&lt;/p&gt;
&lt;h2 id="image-input-data"&gt;Image Input Data&lt;/h2&gt;
&lt;h3 id="math-symbol-recognition"&gt;Math Symbol Recognition&lt;/h3&gt;
&lt;p&gt;The &lt;a href="http://write-math.com/"&gt;write-math.com&lt;/a&gt;
web service allows you to recognize mathematical symbols automatically. It is
described in &lt;a href="https://arxiv.org/abs/1511.09030"&gt;my â€¦&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sat, 23 Dec 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-12-23:/ml-showcases/</guid><category>Machine Learning</category><category>Demo</category></item><item><title>Reproducibility in Machine Learning</title><link>https://martin-thoma.com/ml-reproducibility/</link><description>&lt;p&gt;Getting reproducible results is important because of trust: Why should somebody
else trust you, if you can get the same results repeatedly? Why do you trust
your results in the first place? People make errors. Making sure you can repeat
what you did before eliminates possibilities for human error.&lt;/p&gt;
&lt;p&gt;Here â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 13 Dec 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-12-13:/ml-reproducibility/</guid><category>Machine Learning</category><category>Research</category></item><item><title>DQN - Deep Q Networks</title><link>https://martin-thoma.com/dqn/</link><description>&lt;p&gt;This blog post is a collection of experiments, not for explaining. If you want
to understand how DQNs work, have a look at &lt;a href="https://keon.io/deep-q-learning/"&gt;keon.io/deep-q-learning&lt;/a&gt;.
Another one is &lt;a href="http://blog.ironhead.ninja/2016/09/08/openai-cartpole.html"&gt;DQN and OpenAI Cartpole&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="results"&gt;Results&lt;/h2&gt;
&lt;p&gt;The code is &lt;a href="https://github.com/MartinThoma/algorithms/blob/master/ML/rl/dqn_agent.py"&gt;&lt;code&gt;dqn_agent.py&lt;/code&gt; on Github&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;th&gt;Environment&lt;/th&gt;
&lt;th&gt;Config File&lt;/th&gt;
&lt;th&gt;Time&lt;/th&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CartPole-v0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h2 id="see-also"&gt;See also&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/matthiasplappert/keras-rl"&gt;matthiasplappert â€¦&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Thu, 30 Nov 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-11-30:/dqn/</guid><category>Machine Learning</category><category>Reinforcement Learning</category></item><item><title>Q-Learning</title><link>https://martin-thoma.com/q-learning/</link><description>&lt;p&gt;Reinforcement Learning (RL) is about finding optimal actions automatically.
So you have an environment &lt;code&gt;env&lt;/code&gt; which has&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;env.reset() -&amp;gt; None&lt;/code&gt;: Start a new episode. This could be a new game in the
  case of chess.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;env.step(action) -&amp;gt; observation, reward, is_done, additional_information&lt;/code&gt;:
  Make a step in the environment. The &lt;code&gt;is_done â€¦&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sun, 26 Nov 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-11-26:/q-learning/</guid><category>Machine learning</category><category>RL</category><category>Reinforcement Learning</category></item><item><title>Best practice for Machine Learning Projects</title><link>https://martin-thoma.com/ml-best-practice/</link><description>&lt;p&gt;I did a couple of machine learning projects so far and there are some patterns
in the projects which turned out to be good ideas. In this post, I would like
to share those patterns with you.&lt;/p&gt;
&lt;h2 id="know-your-problem"&gt;Know your problem&lt;/h2&gt;
&lt;p&gt;For me, a machine learning project really starts when you â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 15 Nov 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-11-15:/ml-best-practice/</guid><category>Machine Learning</category></item><item><title>RL Agents</title><link>https://martin-thoma.com/rl-agents/</link><description>&lt;table class="table"&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Observation Space&lt;/th&gt;
&lt;th&gt;Action Space&lt;/th&gt;
&lt;th&gt;Paper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;abbr title="State-Action-Reward-State-Action"&gt;SARSA&lt;/abbr&gt;&lt;/td&gt;
&lt;td&gt;discrete or continuous&lt;/td&gt;
&lt;td&gt;discrete&lt;/td&gt;
&lt;td&gt;&lt;a href="http://incompleteideas.net/sutton/book/the-book-2nd.html"&gt;&lt;abbr title="Reinforcement learning: An introduction"&gt;Sutton and Barto, 2011&lt;/abbr&gt;&lt;/a&gt;, &lt;a href="https://martin-thoma.com/probabilistische-planung/#sarsa"&gt;Blog Post&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;abbr title="Deep Q-Networks"&gt;DQN&lt;/abbr&gt;&lt;/td&gt;
&lt;td&gt;discrete or continuous&lt;/td&gt;
&lt;td&gt;discrete&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1312.5602"&gt;&lt;abbr title="Playing Atari with Deep Reinforcement Learning"&gt;[MKSG+13]&lt;/abbr&gt;&lt;/a&gt;, &lt;a href="http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html"&gt;&lt;abbr title="Human-level control through deep reinforcement learning"&gt;[MKSR+15]&lt;/abbr&gt;&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1509.06461"&gt;&lt;abbr title="Deep Reinforcement Learning with Double Q-learning"&gt;[HGS15]&lt;/abbr&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;abbr title="Cross-Entropy Method"&gt;CEM&lt;/abbr&gt;&lt;/td&gt;
&lt;td&gt;discrete or continuous&lt;/td&gt;
&lt;td&gt;discrete&lt;/td&gt;
&lt;td&gt;&lt;abbr title="Learning Tetris Using the Noisy Cross-Entropy Method"&gt;&lt;a href="http://www.mitpressjournals.org/doi/abs/10.1162/neco.2006.18.12.2936?journalCode=neco&amp;amp;"&gt;Szita et al., 2006&lt;/a&gt;&lt;/abbr&gt;, &lt;abbr title="Deep Reinforcement Learning (MLSS lecture notes)"&gt;Schulman, 2016&lt;/abbr&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;abbr title="deep deterministic policy gradient"&gt;DDPG&lt;/abbr&gt;&lt;/td&gt;
&lt;td&gt;discrete or continuous&lt;/td&gt;
&lt;td&gt;continuous&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1509.02971"&gt;&lt;abbr title="Continuous control with deep reinforcement learning"&gt;[LHPH+15]&lt;/abbr&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;abbr title="normalized adantage functions"&gt;NAF&lt;/abbr&gt;&lt;/td&gt;
&lt;td&gt;discrete or continuous&lt;/td&gt;
&lt;td&gt;continuous â€¦&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Tue, 07 Nov 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-11-07:/rl-agents/</guid><category>Reinforcement Learning</category></item><item><title>Exploratory Data Analysis</title><link>https://martin-thoma.com/eda/</link><description>&lt;p&gt;Getting insights from data is exciting. So let's see how well I can cover this
topic in a single article.&lt;/p&gt;
&lt;p&gt;In this article, I assume you have data in a single CSV file. If you have
multiple CSV files, you can &lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html"&gt;merge&lt;/a&gt;
them similar to SQL &lt;code&gt;JOIN&lt;/code&gt; statements.&lt;/p&gt;
&lt;h2 id="prerequesites"&gt;Prerequesites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Python â€¦&lt;/li&gt;&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 18 Oct 2017 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-10-18:/eda/</guid><category>CSV</category><category>EDA</category><category>Data</category><category>Pandas</category></item><item><title>ASR Services</title><link>https://martin-thoma.com/asr-services/</link><description>&lt;p&gt;Automatic Speech Recognition (ASR) is really difficult to set up yourself.
There are some toolkits like &lt;a href="https://en.wikipedia.org/wiki/CMU_Sphinx"&gt;CMU Sphinx&lt;/a&gt;
and &lt;a href="https://en.wikipedia.org/wiki/List_of_speech_recognition_software"&gt;others&lt;/a&gt;,
but the last time I checked (some years ago) they either didn't really work or
I couldn't manage to get them running.&lt;/p&gt;
&lt;h2 id="how-does-asr-work"&gt;How does ASR work?&lt;/h2&gt;
&lt;p&gt;One way to do â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sun, 08 Oct 2017 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-10-08:/asr-services/</guid><category>Machine Learning</category><category>ASR</category><category>Service</category></item><item><title>1D Data Visualization</title><link>https://martin-thoma.com/1d-data-visualization/</link><description>&lt;p&gt;Once in a while I have to visualize simple 1D numerical data. So here is an
example script:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32 â€¦&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sun, 03 Sep 2017 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-09-03:/1d-data-visualization/</guid><category>Machine Learning</category><category>Data Visualization</category></item><item><title>ML Review 6</title><link>https://martin-thoma.com/ml-review-6/</link><description>&lt;p&gt;This Review gives an overview of intersting stuff I stumbled over which are
related to machine learning.&lt;/p&gt;
&lt;h2 id="news"&gt;News&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Hardware&lt;ul&gt;
&lt;li&gt;&lt;a href="https://devblogs.nvidia.com/parallelforall/inside-volta/"&gt;Inside Volta: The World&amp;rsquo;s Most Advanced Data Center GPU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.nvidia.com/en-us/data-center/dgx-systems/"&gt;NVIDIA DGX SYSTEMS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Core-Software&lt;ul&gt;
&lt;li&gt;&lt;a href="https://devblogs.nvidia.com/parallelforall/cuda-9-features-revealed/"&gt;CUDA 9&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://developer.nvidia.com/cudnn"&gt;cuDNN 7&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://devblogs.nvidia.com/parallelforall/scaling-keras-training-multiple-gpus/"&gt;Scaling Keras Model Training to Multiple GPUs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Applications:&lt;ul&gt;
&lt;li&gt;Facebook: &lt;a href="https://code.facebook.com/posts/289921871474277/transitioning-entirely-to-neural-machine-translation/"&gt;Transitioning entirely to neural â€¦&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Thu, 31 Aug 2017 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-08-31:/ml-review-6/</guid><category>Machine Learning</category><category>matplotlib</category></item><item><title>Data Visualization with Python</title><link>https://martin-thoma.com/python-data-visualization/</link><description>&lt;p&gt;Python has a lot of libraries for data visualization and I recently stumbled
over an awesome talk from PyCon 2017 by Jake VanderPlas titled "The Python
Visualization Landscape" which gives an overview over them:&lt;/p&gt;
&lt;iframe allow="autoplay; encrypted-media" allowfullscreen="" frameborder="0" height="315" src="https://www.youtube-nocookie.com/embed/FytuB8nFHPQ" width="560"&gt;&lt;/iframe&gt;
&lt;ul&gt;
&lt;li&gt;Matplotlib&lt;ul&gt;
&lt;li&gt;&lt;a href="https://seaborn.pydata.org/"&gt;seaborn&lt;/a&gt;: statistical data visualization&lt;/li&gt;
&lt;li&gt;&lt;a href="http://pandas.pydata.org/"&gt;Pandas&lt;/a&gt;: Dataframes&lt;/li&gt;
&lt;li&gt;&lt;a href="https://networkx.github.io/"&gt;networkx&lt;/a&gt;: Graphs&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/yhat/ggpy"&gt;ggpy&lt;/a&gt;: Python implementation of the grammar of â€¦&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 02 Aug 2017 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-08-02:/python-data-visualization/</guid><category>Data Visualization</category><category>Python</category><category>Vega</category><category>Matplotlib</category></item><item><title>The Reuters Dataset</title><link>https://martin-thoma.com/nlp-reuters/</link><description>&lt;p&gt;Reuters is a benchmark dataset for &lt;a href="https://martin-thoma.com/document-classification/"&gt;document classification&lt;/a&gt;.
To be more precise, it is a multi-class (e.g. there are multiple classes),
multi-label (e.g. each document can belong to many classes) dataset.
It has &lt;strong&gt;90 classes&lt;/strong&gt;, &lt;strong&gt;7769 training documents&lt;/strong&gt; and &lt;strong&gt;3019 testing documents&lt;/strong&gt;.
It is the ModApte (R(90 â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Thu, 27 Jul 2017 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-07-27:/nlp-reuters/</guid><category>NLP</category><category>Reuters</category><category>Classification</category><category>Machine Learning</category></item><item><title>Document Classification</title><link>https://martin-thoma.com/document-classification/</link><description>&lt;p&gt;This article explains how to classify texts.&lt;/p&gt;
&lt;p&gt;Suppose you have a text classification problem. For example, you want to
classify incoming emails as (C1) spam (C2) notifications (C3) personal. Hence each email belongs to exactly one of three classes.&lt;/p&gt;
&lt;h2 id="basic-setup"&gt;Basic Setup&lt;/h2&gt;
&lt;p&gt;Suppose &lt;strong&gt;you have corpus&lt;/strong&gt; of 1000 emails. You make â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 26 Jul 2017 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-07-26:/document-classification/</guid><category>NLP</category><category>Machine Learning</category><category>Classification</category></item><item><title>How to get Data for ML systems</title><link>https://martin-thoma.com/ml-get-data/</link><description>&lt;p&gt;Machine Learning is only possible with data. The more data, the better. For
many services this is a self-improving system. The more data the system gets,
the better it becomes. The better the system is, the more users use it. The
more users use the system, the more data the â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Thu, 15 Jun 2017 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-06-15:/ml-get-data/</guid><category>Machine Learning</category><category>data</category></item><item><title>Unsupervised Pretraining</title><link>https://martin-thoma.com/unsupervised-pretraining/</link><description>&lt;p&gt;Neural networks have thousands, often millions of parameters. They take
hundrets of features and predict thousands of classes. The features can often
not be seen independantly, but have to be taken as a whole into consideration.
Most parameters are not independant either. And still, we use only on the order â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Thu, 08 Jun 2017 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-06-08:/unsupervised-pretraining/</guid><category>Machine Learning</category><category>Neural Networks</category></item><item><title>ML Review 5</title><link>https://martin-thoma.com/ml-review-5/</link><description>&lt;p&gt;This Review gives an overview of intersting stuff I stumbled over which are
related to machine learning.&lt;/p&gt;
&lt;h2 id="news"&gt;News&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://canada.googleblog.com/2017/03/canadas-ai-moment.html"&gt;Canada&amp;rsquo;s AI Moment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.theverge.com/2017/4/3/15164490/alibaba-ai-starcraft-combat"&gt;AI is one step closer to mastering StarCraft&lt;/a&gt;: &lt;a href="https://arxiv.org/abs/1703.10069v1"&gt;Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Game&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://deepsense.io/deep-learning-for-satellite-imagery-via-image-segmentation/"&gt;Deep learning for satellite imagery via image segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hardware:&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.nvidia.com/en-us/data-center/volta-gpu-architecture/"&gt;Nvidia â€¦&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Thu, 25 May 2017 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-05-25:/ml-review-5/</guid><category>Machine Learning</category><category>matplotlib</category></item><item><title>Natural Language Processing</title><link>https://martin-thoma.com/nlp/</link><description>&lt;p&gt;Natural language processing (NLP) is a scientific field which deals with
language in textual form.&lt;/p&gt;
&lt;h2 id="tasks"&gt;Tasks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Classification&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Is an e-mail spam or not?&lt;/li&gt;
&lt;li&gt;Topic: Is it about sports, science or religion?&lt;/li&gt;
&lt;li&gt;Language: Is it English, German or French?&lt;/li&gt;
&lt;li&gt;Sentence boundary: Is a character the boundary of a sentence or not â€¦&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 24 May 2017 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-05-24:/nlp/</guid><category>Machine Learning</category><category>NLP</category></item><item><title>How to silence TensorFlow</title><link>https://martin-thoma.com/silence-tf/</link><description>&lt;p&gt;Set the environment variable&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;TF_CPP_MIN_LOG_LEVEL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;if you're also annoyed by messages like&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened â€¦&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Thu, 30 Mar 2017 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-03-30:/silence-tf/</guid><category>Tensorflow</category></item><item><title>ZCA Whitening</title><link>https://martin-thoma.com/zca-whitening/</link><description>&lt;p&gt;Whitening is a transformation of data in such a way that its covariance matrix &lt;span class="math"&gt;\(\Sigma\)&lt;/span&gt;
is the identity matrix. Hence whitening decorrelates features. It is used as a
preprocessing method.&lt;/p&gt;
&lt;p&gt;When you have &lt;span class="math"&gt;\(N\)&lt;/span&gt; data points in &lt;span class="math"&gt;\(\mathbb{R}^n\)&lt;/span&gt;, then the covariance matrix
&lt;span class="math"&gt;\(\Sigma \in \mathbb{R}^{n \times â€¦&lt;/span&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 29 Mar 2017 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-03-29:/zca-whitening/</guid><category>Computer Vision</category><category>Machine Learning</category></item><item><title>ML Review 4</title><link>https://martin-thoma.com/ml-review-4/</link><description>&lt;p&gt;This Review gives an overview of intersting stuff I stumbled over which are
related to machine learning.&lt;/p&gt;
&lt;h2 id="new-developments"&gt;New Developments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/MarvinTeichmann/KittiSeg"&gt;KittiSeg&lt;/a&gt; (&lt;a href="https://www.reddit.com/r/MachineLearning/comments/5y8c5w/p_kittiseg_a_toolkit_to_perform_semantic/"&gt;reddit&lt;/a&gt;): A toolkit for semantic segmentation based on &lt;a href="https://github.com/TensorVision/TensorVision"&gt;TensorVision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://research.google.com/audioset/"&gt;AudioSet&lt;/a&gt;: A dataset for accoustic events&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="publications"&gt;Publications&lt;/h2&gt;
&lt;!-- e.g. arXiv --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1703.03864"&gt;Evolution Strategies as a Scalable Alternative to Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1703.00955"&gt;Controllable Text Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1703.02528"&gt;Stopping GAN â€¦&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sat, 25 Mar 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-03-25:/ml-review-4/</guid><category>Machine Learning</category><category>matplotlib</category><category>Color</category></item><item><title>Object Detection</title><link>https://martin-thoma.com/object-detection/</link><description>&lt;p&gt;Object detection is the following task: You have an image and you want
axis-aligned bounding boxes around every instance of a pre-defined set of
object classes. The set of object classes is finite and typically not bigger
than 1000.&lt;/p&gt;
&lt;p&gt;Here is an easy to use example&lt;/p&gt;
&lt;h2 id="prerequisites"&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/install/"&gt;Tensorflow&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="http://askubuntu.com/q/799184/10425"&gt;CUDA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://askubuntu.com/q/767269/10425"&gt;CuDNN â€¦&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sun, 19 Mar 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-03-19:/object-detection/</guid><category>machine learning</category><category>Computer Vision</category><category>Pascal VOC</category></item><item><title>Image Classification</title><link>https://martin-thoma.com/image-classification/</link><description>&lt;p&gt;Image classification is the following task: You have an image and you want to
assign it one label. The set of possible labels is finite and typically not
bigger than 1000.&lt;/p&gt;
&lt;p&gt;So for example, you might ask: What can you see in this image?&lt;/p&gt;
&lt;figure class="wp-caption aligncenter img-thumbnail"&gt;
&lt;img alt="A jellyfish" src="../images/2017/03/moon-jelly.jpg" style="width: 512px;"/&gt;
&lt;figcaption class="text-center"&gt;A jellyfish&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;It is one of â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 15 Mar 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-03-15:/image-classification/</guid><category>machine learning</category><category>Computer Vision</category><category>ImageNet</category></item><item><title>Ensembles</title><link>https://martin-thoma.com/ensembles/</link><description>&lt;p&gt;Models which are combinations of other models are called an &lt;strong&gt;ensemble&lt;/strong&gt;.
The simplest way to combine several classifiers is by averaging their predictions.&lt;/p&gt;
&lt;p&gt;For example, if you have three&amp;nbsp;models and four&amp;nbsp;classes, you might get
predictions like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model 1(x1) = [0.1, 0.5, 0.3, 0.1 â€¦&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sat, 11 Mar 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-03-11:/ensembles/</guid><category>machine learning</category><category>ensembles</category><category>Computer Vision</category><category>CIFAR 100</category></item><item><title>How to download ImageNet</title><link>https://martin-thoma.com/download-data/</link><description>&lt;p&gt;Machine Learning algorithms for computer vision need huge amounts of data.
Here are a few remarks on how to download them.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make sure you have enough space (&lt;code&gt;df -h&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Get a download manager. I use aria2c (&lt;code&gt;sudo apt-get install aria2&lt;/code&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For ImageNet, you have to register at &lt;a href="http://image-net.org/"&gt;image-net.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Download â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Mon, 06 Mar 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-03-06:/download-data/</guid><category>download</category><category>machine learning</category></item><item><title>ML Review 3</title><link>https://martin-thoma.com/ml-review-3/</link><description>&lt;p&gt;This Review gives an overview of intersting stuff I stumbled over which are
related to machine learning.&lt;/p&gt;
&lt;h2 id="new-developments"&gt;New Developments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://developers.googleblog.com/2017/02/announcing-tensorflow-10.html"&gt;Tensorflow 1.0 is released&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="human-strenght-and-super-human-strength-programs"&gt;Human-strenght and Super-human strength programs&lt;/h3&gt;
&lt;p&gt;Super-human strength programs are programs, which surpass even the best human
(on the long run) in a specified task. Human-strength programs â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sat, 25 Feb 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-02-25:/ml-review-3/</guid><category>Machine Learning</category></item><item><title>Best of ML</title><link>https://martin-thoma.com/best-of-ml/</link><description>&lt;p&gt;This post is a summary of articles, websites and material in general about
machine learning.&lt;/p&gt;
&lt;h2 id="articles"&gt;Articles&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;RNNs&lt;ul&gt;
&lt;li&gt;Get an overview: &lt;a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Understand them: &lt;a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;Understanding LSTM Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/"&gt;Using convolutional neural nets to detect facial keypoints tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://hunch.net/?p=22"&gt;Clever Methods of Overfitting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scott.fortmann-roe.com/docs/BiasVariance.html"&gt;Understanding the Bias-Variance Tradeoff&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sebastianruder.com/optimizing-gradient-descent/"&gt;An â€¦&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Mon, 13 Feb 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-02-13:/best-of-ml/</guid><category>Machine Learning</category></item><item><title>State of the Art in ML</title><link>https://martin-thoma.com/sota/</link><description>&lt;p&gt;It is difficult to keep track of the current state of the art (SotA). Also, it
might not be directly clear which datasets are relevant. The following list
should help. If you think some datasets / problems / SotA results are missing,
let me know in the comments or via E-mail (&lt;code&gt;info â€¦&lt;/code&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Mon, 06 Feb 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-02-06:/sota/</guid><category>Machine Learning</category><category>Datasets</category></item><item><title>skdata</title><link>https://martin-thoma.com/skdata/</link><description>&lt;p&gt;I really like Machine Learning. I like reading papers, understanding and
evaluating new ideas. But one part I always have to spend quite a bit of time
on is loading the data. It's always a mess to find the datasets, understand
where exactly I can download them and how they've â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Mon, 30 Jan 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-01-30:/skdata/</guid><category>Machine Learning</category><category>Python</category><category>dataset</category></item><item><title>Label Correction Algorithm</title><link>https://martin-thoma.com/label-correction-algorithm/</link><description>&lt;p&gt;The label-correction algorithm is a generalization which includes very common
graph search algorithms like breadth first search (BFS), depth first search (DFS),
&lt;a href="https://en.wikipedia.org/wiki/A*_search_algorithm"&gt;A*&lt;/a&gt;,
&lt;a href="https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm"&gt;Dijkstra's algorithm&lt;/a&gt; and
&lt;a href="https://en.wikipedia.org/wiki/Branch_and_bound"&gt;Branch and bound&lt;/a&gt; as special cases.&lt;/p&gt;
&lt;h2 id="pseudocode"&gt;Pseudocode&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter img-thumbnail"&gt;
&lt;img alt="Pseudocode for the Label correction algorithm" src="../images/2016/07/label-correction.png"/&gt;
&lt;figcaption class="text-center"&gt;Pseudocode for the Label correction algorithm&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Explanation:&lt;/p&gt;
&lt;p&gt;First &lt;code&gt;if&lt;/code&gt;: The left hand side is a lower bound â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 25 Jan 2017 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2017-01-25:/label-correction-algorithm/</guid><category>A.I.</category><category>Algorithms</category><category>Programming</category><category>Python</category><category>Machine Learning</category><category>Branch-and-Bound</category></item><item><title>Reinforcement Learning</title><link>https://martin-thoma.com/reinforcement-learning/</link><description>&lt;p&gt;Reinforcement learning is a sub-field of mathematics and computer science. It
deals with the following kind of problems: You're given a set of states
&lt;span class="math"&gt;\(\mathcal{X} \subseteq \mathbb{R}^n\)&lt;/span&gt; and a starting state &lt;span class="math"&gt;\(x_0 \in \mathcal{X}\)&lt;/span&gt;.
For every time step &lt;span class="math"&gt;\(k = 0, 1, 2, \dots\)&lt;/span&gt; you have a â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Thu, 29 Dec 2016 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-12-29:/reinforcement-learning/</guid><category>Machine Learning</category><category>RL</category></item><item><title>ML Review 2</title><link>https://martin-thoma.com/ml-review-2/</link><description>&lt;p&gt;This Review gives an overview of intersting stuff I stumbled over which are
related to machine learning. Most of it was posted in &lt;a href="https://ml-ka.de/"&gt;KITs machine learning group&lt;/a&gt; (on Facebook).&lt;/p&gt;
&lt;p&gt;A lot of stuff can be found in my article about &lt;a href="https://martin-thoma.com/nips-2016/"&gt;NIPS 2016&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="new-developments"&gt;New Developments&lt;/h2&gt;
&lt;!-- Trends --&gt;
&lt;ul&gt;
&lt;li&gt;Random forests for courier detection: &lt;a href="https://www.theguardian.com/science/the-lay-scientist/2016/feb/18/has-a-rampaging-ai-algorithm-really-killed-thousands-in-pakistan"&gt;Has â€¦&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Tue, 27 Dec 2016 11:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-12-27:/ml-review-2/</guid><category>Machine Learning</category></item><item><title>NIPS 2016</title><link>https://martin-thoma.com/nips-2016/</link><description>&lt;p&gt;The Conference and Workshop on Neural Information Processing Systems (NIPS) is
probably the biggest conference with machine learning / deep learning as a
main topic. This year, about 6000 people attended it. My friend Marvin and me
were supported by &lt;a href="https://www.informatik.kit.edu/begabtenstiftung_informatik_karlsruhe.php"&gt;Begabtenstiftung Informatik Karlsruhe&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The complete program can be found in the â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sat, 24 Dec 2016 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-12-24:/nips-2016/</guid><category>Machine Learning</category><category>research</category></item><item><title>ML Review 1</title><link>https://martin-thoma.com/ml-review-1/</link><description>&lt;p&gt;This Review gives an overview of intersting stuff I stumbled over which are
related to machine learning. Most of it was posted in &lt;a href="https://ml-ka.de/"&gt;KITs machine learning group&lt;/a&gt; (on Facebook).&lt;/p&gt;
&lt;h2 id="new-developments"&gt;New Developments&lt;/h2&gt;
&lt;!-- Trends --&gt;
&lt;ul&gt;
&lt;li&gt;Random forests for courier detection: &lt;a href="https://www.theguardian.com/science/the-lay-scientist/2016/feb/18/has-a-rampaging-ai-algorithm-really-killed-thousands-in-pakistan"&gt;Has a rampaging AI algorithm called Skynet really killed thousands in Pakistan?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="live-demos-and-websites"&gt;Live Demos â€¦&lt;/h2&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Tue, 22 Nov 2016 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-11-22:/ml-review-1/</guid><category>Machine Learning</category></item><item><title>Machine Learning Glossary</title><link>https://martin-thoma.com/ml-glossary/</link><description>&lt;p&gt;The following is a list of short explanations of different terms in machine
learning. The aim is to keep things simple and brief, not to explain the terms
in full detail.&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;dfn id="active-learning"&gt;Active Learning&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;The algorithm gives a pattern and asks for a label.&lt;/dd&gt;
&lt;dt&gt;&lt;dfn id="backpropagation"&gt;Backpropagation&lt;/dfn&gt;&lt;/dt&gt;
&lt;dd&gt;A clever implementation of gradient descent â€¦&lt;/dd&gt;&lt;/dl&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Mon, 24 Oct 2016 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-10-24:/ml-glossary/</guid><category>Machine Learning</category></item><item><title>Average Distance of Random Points in a Unit Hypercube</title><link>https://martin-thoma.com/curse-of-dimensionality/</link><description>&lt;p&gt;In machine learning, the "curse of dimensionality" is often stated but much
less often explained. At least not in detail. One just gets told that points
are farer away from each other in high dimensional spaces.&lt;/p&gt;
&lt;h2 id="maximum-minimal-distance"&gt;Maximum minimal distance&lt;/h2&gt;
&lt;p&gt;One approach to this is to calculate the maximum minimal distance â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Thu, 20 Oct 2016 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-10-20:/curse-of-dimensionality/</guid><category>Machine Learning</category></item><item><title>Iterating over Graphs</title><link>https://martin-thoma.com/graph-iteration/</link><description>&lt;p&gt;Today I was thinking if one could iterate over all possible feed forward network
architectures possible. A feed forward network is essentially only a directed
acyclic graph.&lt;/p&gt;
&lt;p&gt;To make things simpler, lets just think about multilayer perceptrons. This
means we only have connections between neighboring layers (and we have layers â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 19 Oct 2016 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-10-19:/graph-iteration/</guid><category>Machine Learning</category></item><item><title>Diverging Gradient Descent</title><link>https://martin-thoma.com/diverging-gradient-descent/</link><description>&lt;p&gt;When you take the function&lt;/p&gt;
&lt;div class="math"&gt;$$f(x, y) = 3x^2 + 3y^2 + 2xy$$&lt;/div&gt;
&lt;p&gt;and start gradient descent at &lt;span class="math"&gt;\(x_0 = (6, 6)\)&lt;/span&gt; with learning rate &lt;span class="math"&gt;\(\eta = \frac{1}{2}\)&lt;/span&gt;
it diverges.&lt;/p&gt;
&lt;h2 id="gradient-descent"&gt;Gradient descent&lt;/h2&gt;
&lt;p&gt;Gradient descent is an optimization rule which starts at a point &lt;span class="math"&gt;\(x_0\)&lt;/span&gt; and
then applies the update rule â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Thu, 21 Jul 2016 16:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-07-21:/diverging-gradient-descent/</guid><category>Gradient Descent</category><category>Optimization</category></item><item><title>XOR tutorial with TensorFlow</title><link>https://martin-thoma.com/tf-xor-tutorial/</link><description>&lt;p&gt;The XOR-Problem is a classification problem, where you only have four data
points with two features. The training set and the test set are exactly
the same in this problem. So the interesting question is only if the model is
able to find a decision boundary which classifies all four â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Tue, 19 Jul 2016 14:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-07-19:/tf-xor-tutorial/</guid><category>Machine Learning</category><category>Python</category><category>Tensorflow</category><category>sklearn</category></item><item><title>Optimization Basics</title><link>https://martin-thoma.com/optimization-basics/</link><description>&lt;p&gt;Optimization is a subfield of mathematics / computer science which deals with finding the best solution. Typically, problems in optimization are stated like this:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
&amp;amp;\underset{x}{\operatorname{minimize}}&amp;amp; &amp;amp; f(x) \\
&amp;amp;\operatorname{subject\;to}
&amp;amp; &amp;amp;g_i(x) \leq 0, \quad i = 1,\dots,m \\
&amp;amp;&amp;amp;&amp;amp;h_i(x) = 0, \quad i = 1, \dots â€¦&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 06 Jul 2016 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-07-06:/optimization-basics/</guid><category>Machine Learning</category><category>optimization</category><category>gradient descent</category></item><item><title>Linear Classification</title><link>https://martin-thoma.com/linear-classification/</link><description>&lt;p&gt;In classification problems you have data points &lt;span class="math"&gt;\(x \in \mathbb{R}^m\)&lt;/span&gt; which you want to classify into one of &lt;span class="math"&gt;\(k \in \mathbb{N}_{\geq 2}\)&lt;/span&gt; classes. This is a supervised task. This means you have &lt;span class="math"&gt;\(n\)&lt;/span&gt; data points for training in a matrix &lt;span class="math"&gt;\(X \in \mathbb{R}^{n â€¦&lt;/span&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 22 Jun 2016 20:00:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-06-22:/linear-classification/</guid><category>Algorithms</category><category>Machine Learning</category><category>optimization</category><category>Python</category></item><item><title>Collaborative Filtering</title><link>https://martin-thoma.com/collaborative-filtering/</link><description>&lt;p&gt;Suppose you are in the Netflix setting: You have &lt;span markdown="0"&gt;&lt;span class="math"&gt;\(M\)&lt;/span&gt;&lt;/span&gt;
movies, &lt;span markdown="0"&gt;&lt;span class="math"&gt;\(N\)&lt;/span&gt;&lt;/span&gt; users and integer ratings
&lt;span markdown="0"&gt;&lt;span class="math"&gt;\(1, \dots, K\)&lt;/span&gt;&lt;/span&gt; for some movies by some users.&lt;/p&gt;
&lt;p&gt;You want to predict all missing values. This means you want to say how the
users would rate movies they have not actually rated.&lt;/p&gt;
&lt;p&gt;Please â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 10 Feb 2016 21:35:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-02-10:/collaborative-filtering/</guid><category>Rating</category></item><item><title>Softmax</title><link>https://martin-thoma.com/softmax/</link><description>&lt;p&gt;Softmax is an activation function for multi-layer perceptrons (MLPs). It is
a function which gets applied to a vector in &lt;span markdown="0"&gt;&lt;span class="math"&gt;\(\mathbb{x} \in \mathbb{R}^K\)&lt;/span&gt;&lt;/span&gt;
and returns a vector in &lt;span markdown="0"&gt;&lt;span class="math"&gt;\([0, 1]^K\)&lt;/span&gt;&lt;/span&gt; with the
property that the sum of all elements is 1:&lt;/p&gt;
&lt;div&gt;$$\varphi(\mathbb{x})_j = \frac â€¦&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Tue, 09 Feb 2016 18:09:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-02-09:/softmax/</guid><category>Machine Learning</category><category>Neural Networks</category><category>Activation Functions</category></item><item><title>Comparing Classifiers</title><link>https://martin-thoma.com/comparing-classifiers/</link><description>&lt;p&gt;Classification problems occur quite often and many different classification
algorithms have been described and implemented. But what is the best algorithm
for a given error function and dataset?&lt;/p&gt;
&lt;p&gt;I read questions like "I have problem X. What is the best classifier?" quite
often and my first impulse is always to â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Tue, 19 Jan 2016 20:13:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-01-19:/comparing-classifiers/</guid><category>Python</category><category>Machine Learning</category><category>Classification</category></item><item><title>Function Approximation</title><link>https://martin-thoma.com/function-approximation/</link><description>&lt;p&gt;I was recently quite disappointed by how bad neural networks are for function
approximation (see &lt;a href="http://datascience.stackexchange.com/q/9495/8820"&gt;How should a neural network for unbound function approximation be structured?&lt;/a&gt;). However, I've just found that
Gaussian processes are great for function approximation!&lt;/p&gt;
&lt;p&gt;There are two important types of function approximation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Interpolation&lt;/strong&gt;: What values does â€¦&lt;/li&gt;&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Mon, 18 Jan 2016 20:00:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-01-18:/function-approximation/</guid><category>Machine Learning</category><category>Regression</category></item><item><title>Using SVMs with sklearn</title><link>https://martin-thoma.com/svm-with-sklearn/</link><description>&lt;p&gt;Support Vector Machines (SVMs) is a group of powerful classifiers. In this
article, I will give a short impression of how they work. I continue
with an example how to use SVMs with sklearn.&lt;/p&gt;
&lt;h2 id="svm-theory"&gt;SVM theory&lt;/h2&gt;
&lt;p&gt;&lt;abbr title="Support Vector Machines"&gt;SVMs&lt;/abbr&gt; can be described with
5&amp;nbsp;ideas in mind:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;b&gt;Linear, binary classifiers&lt;/b&gt;: If data â€¦&lt;/li&gt;&lt;/ol&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Thu, 14 Jan 2016 12:25:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2016-01-14:/svm-with-sklearn/</guid><category>Python</category><category>Machine Learning</category><category>SVM</category><category>Classification</category><category>sklearn</category></item><item><title>Tensor Flow - A quick impression</title><link>https://martin-thoma.com/tensor-flow-quick/</link><description>&lt;p&gt;Tensor Flow is a machine learning toolkit which recently got published by
Google. They published it under &lt;a href="https://tldrlegal.com/license/apache-license-2.0-(apache-2.0)"&gt;Apache License 2.0&lt;/a&gt;. Looking at the source code overview, it seems to be mainly C++
with a significant bit of Python.&lt;/p&gt;
&lt;p&gt;I guess the abstract of the
&lt;a href="http://download.tensorflow.org/paper/whitepaper2015.pdf"&gt;Whitepaper&lt;/a&gt; is a good
description â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 11 Nov 2015 22:33:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2015-11-11:/tensor-flow-quick/</guid><category>Machine Learning</category><category>Python</category><category>Tensorflow</category></item><item><title>Lasagne for Python Newbies</title><link>https://martin-thoma.com/lasagne-for-python-newbies/</link><description>&lt;p&gt;Lasagne is a Python package for training neural networks. The nice thing about
Lasagne is that it is possible to write Python code and execute the training
on nVidea GPUs with automatically generated CUDA code.&lt;/p&gt;
&lt;p&gt;However, installing Lasagne is not that easy. Especially if you are not
familiar with Python â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Fri, 17 Apr 2015 19:26:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2015-04-17:/lasagne-for-python-newbies/</guid><category>Python</category><category>Machine Learning</category></item><item><title>Gradient Descent, the Delta Rule and Backpropagation</title><link>https://martin-thoma.com/gradient-descent-the-delta-rule-and-backpropagation/</link><description>&lt;p&gt;If you learn about machine learning you will stumble over three terms that are
related:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gradient descent,&lt;/li&gt;
&lt;li&gt;the Delta rule and&lt;/li&gt;
&lt;li&gt;backpropagation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Gradient descent is a way to find a minimum in a high-dimensional space. You
go in direction of the steepest descent.&lt;/p&gt;
&lt;p&gt;The delta rule is an update rule â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sun, 26 Oct 2014 21:06:00 +0100</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2014-10-26:/gradient-descent-the-delta-rule-and-backpropagation/</guid><category>Machine Learning</category><category>AI</category></item><item><title>The Twiddle Algorithm</title><link>https://martin-thoma.com/twiddle/</link><description>&lt;p&gt;Twiddle is an algorithm that tries to find a good choice of parameters &lt;span class="math"&gt;\(p\)&lt;/span&gt;
for an algorithm &lt;span class="math"&gt;\(\mathcal{A}\)&lt;/span&gt; that returns an error.&lt;/p&gt;
&lt;p&gt;The algorithm is quite simple to implement. I guess gradient descent might be
better for most cases, but Twiddle does not require any knowledge about the
algorithm â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Sat, 06 Sep 2014 13:49:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2014-09-06:/twiddle/</guid><category>Python</category><category>AI</category><category>Machine Learning</category></item><item><title>GPUs - Supercomputers for your home</title><link>https://martin-thoma.com/gpus-supercomputers-for-your-home/</link><description>&lt;p&gt;A few days ago I got some of my neural net code to work with a GPU.
The GPU is called "Tesla C2075". It is able to get 515 GFlops peak performance.
It has 448 CUDA cores that work with 1.15 GHz and it has 6GB GDDR5 memory.&lt;/p&gt;
&lt;p&gt;My â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Wed, 20 Aug 2014 23:26:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2014-08-20:/gpus-supercomputers-for-your-home/</guid><category>Python</category><category>Theano</category><category>GPU</category><category>nVidea</category><category>CUDA</category><category>AI</category><category>Machine Learning</category></item><item><title>A.I. in Computer Games</title><link>https://martin-thoma.com/ai-in-computer-games/</link><description>&lt;p&gt;Artificial Intelligences (A.I.s) are computer programs that are able to adjust
their behaviour according to data they see. So A.I.s are able to adjust to
the data a human player generates.&lt;/p&gt;
&lt;figure&gt;
&lt;img alt="Game A.I.s" src="http://imgs.xkcd.com/comics/game_ais.png"/&gt;
&lt;figcaption&gt;Game A.I.s&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id="solved-games"&gt;Solved games&lt;/h2&gt;
&lt;p&gt;There is a number of games which are definitely â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Tue, 01 Jul 2014 23:52:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2014-07-01:/ai-in-computer-games/</guid><category>AI</category><category>games</category><category>Machine Learning</category></item><item><title>Classification with PyBrain</title><link>https://martin-thoma.com/classification-with-pybrain/</link><description>&lt;p&gt;PyBrain is a Python library for machine learning. It's in version 0.31 and
the last change is 2 months ago (&lt;a href="https://github.com/pybrain/pybrain"&gt;source&lt;/a&gt;).
The source code is licensed under &lt;a href="https://tldrlegal.com/license/bsd-3-clause-license-(revised)"&gt;BSD 3 Clause License&lt;/a&gt;. The &lt;a href="http://pybrain.org/docs/"&gt;documentation&lt;/a&gt; is usable, but for
from perfect.&lt;/p&gt;
&lt;h2 id="classification-example"&gt;Classification example&lt;/h2&gt;
&lt;p&gt;The following is a slightly modified example from â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Thoma</dc:creator><pubDate>Mon, 16 Jun 2014 01:02:00 +0200</pubDate><guid isPermaLink="false">tag:martin-thoma.com,2014-06-16:/classification-with-pybrain/</guid><category>Python</category><category>PyBrain</category><category>Neural Networks</category><category>Classification</category><category>AI</category><category>Machine Learning</category></item></channel></rss>