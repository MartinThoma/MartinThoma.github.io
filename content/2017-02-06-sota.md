---
layout: post
title: State of the Art in ML
slug: sota
author: Martin Thoma
date: 2017-02-06 20:00
category: Machine Learning
tags: Machine Learning, Datasets
featured_image: logos/ml.png
---

It is difficult to keep track of the current state of the art (SotA). Also, it
might not be directly clear which datasets are relevant. The following list
should help. If you think some datasets / problems / SotA results are missing,
let me know in the comments or via E-mail (<code>info@martin-thoma.de</code>).
I will update it.

Papers and blog posts which summarize a topic or give a good introduction are
always welcome.

In the following, a <code>+</code> will indicate "higher is better" and a
<code>-</code> will indicate "lower is better".


## Computer Vision

### Image Classification

<table class="table" id="image-classification-table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td><a href="http://www.image-net.org/challenges/LSVRC/2012/nonpub-downloads">ImageNet 2012</a></td>
        <td>2015</td>
        <td style="text-align: right;">3.08 %</td>
        <td>Top-5 error <span title="lower is better">-</span></td>
        <td><a href="https://arxiv.org/pdf/1602.07261.pdf" title="Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning">[SIVA16]</a></td>
    </tr>
    <tr>
        <td><a href="http://yann.lecun.com/exdb/mnist/">MNIST</a></td>
        <td>2013</td>
        <td style="text-align: right;">0.21 %</td>
        <td>error <span title="lower is better">-</span></td>
        <td><a href="http://www.matthewzeiler.com/pubs/icml2013/icml2013.pdf" title="Regularization of Neural Networks using DropConnect">[WZZ+13]</a></td>
    </tr>
    <tr>
        <td><a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a></td>
        <td>2017</td>
        <td style="text-align: right;">2.72 %</td>
        <td>error <span title="lower is better">-</span></td>
        <td><a href="https://openreview.net/forum?id=HkO-PCmYl" title="Shake-Shake regularization of 3-branch residual networks">[G17]</a></td>
    </tr>
    <tr>
        <td><a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-100</a></td>
        <td>2016</td>
        <td style="text-align: right;">15.85 %</td>
        <td>error <span title="lower is better">-</span></td>
        <td><a href="https://arxiv.org/abs/1705.07485" title="Shake-Shake regularization">[G17]</a></td>
    </tr>
    <tr>
        <td><a href="http://cs.stanford.edu/~acoates/stl10/">STL-10</a></td>
        <td>2017</td>
        <td style="text-align: right;">78.66 %</td>
        <td>accuracy <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/abs/1707.09725" title="Analysis and Optimization of Convolutional Neural Network Architectures">[Tho17-2]</a></td>
    </tr>
    <tr>
        <td><a href="http://ufldl.stanford.edu/housenumbers/">SVHN</a></td>
        <td>2016</td>
        <td style="text-align: right;">1.54 %</td>
        <td>error <span title="lower is better">-</span></td>
        <td><a href="https://arxiv.org/abs/1605.07146" title="Wide Residual Networks">[ZK16]</a></td>
    </tr>
    <tr>
        <td><a href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/Caltech101.html">Caltech-101</a></td>
        <td>2014</td>
        <td style="text-align: right;">91.4 %</td>
        <td>accuracy <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/abs/1406.4729v1" title="Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition">[HZRS14]</a></td>
    </tr>
    <tr>
        <td><a href="http://www.vision.caltech.edu/Image_Datasets/Caltech256/">Caltech-256</a></td>
        <td>2014</td>
        <td style="text-align: right;">74.2 %</td>
        <td>accuracy <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/abs/1311.2901" title="Visualizing and Understanding Convolutional Networks">[ZF14]</a></td>
    </tr>
    <tr>
        <td><a href="https://zenodo.org/record/259444" title="The HASYv2 dataset">HASYv2</a></td>
        <td>2017</td>
        <td style="text-align: right;">85.92 %</td>
        <td>accuracy <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/abs/1707.09725" title="Analysis and Optimization of Convolutional Neural Network Architectures">[Tho17-2]</a></td>
    </tr>
    <tr>
        <td><a href="http://lear.inrialpes.fr/people/marszalek/data/ig02/">Graz-02</a></td>
        <td>2010</td>
        <td style="text-align: right;">78.98 %</td>
        <td>accuracy <span title="higher is better">+</span></td>
        <td><a href="http://imagine.enpc.fr/publications/papers/ECCV2010b.pdf" title="Towards Optimal Naive Bayes Nearest Neighbor">[BMDP10]</a></td>
    </tr>
    <tr>
        <td><a href="http://yfcc100m.appspot.com/">YFCC100m</a></td>
        <td></td>
        <td style="text-align: right;"></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td><a href="http://www.vision.caltech.edu/visipedia/CUB-200-2011.html">CUB-200-2011</a> Birds</td>
        <td>2015</td>
        <td style="text-align: right;">84.1</td>
        <td>accuracy <span title="higher is better">+</span></td>
        <td><a href="http://vis-www.cs.umass.edu/bcnn/docs/bcnn_iccv15.pdf">[LRM15]</a></td>
    </tr>
    <tr>
        <td><a href="http://mohammadmahoor.com/databases/denver-intensity-of-spontaneous-facial-action/">DISFA</a></td>
        <td>2017</td>
        <td style="text-align: right;">48.5</td>
        <td>accuracy <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/abs/1702.02925" title="EAC-Net: A Region-based Deep Enhancing and Cropping Approach for Facial Action Unit Detection">[LAZY17]</a></td>
    </tr>
    <tr>
        <td>BP4D</td>
        <td></td>
        <td style="text-align: right;"></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td><a href="https://arxiv.org/abs/1702.05373" title="EMNIST: an extension of MNIST to handwritten letters">EMNIST</a></td>
        <td>2017</td>
        <td style="text-align: right;">50.93</td>
        <td>accuracy <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/abs/1702.05373" title="EMNIST: an extension of MNIST to handwritten letters">[CATS17]</a></td>
    </tr>
    <tr>
        <td><a href="http://megaface.cs.washington.edu/">Megaface</a></td>
        <td>2015</td>
        <td style="text-align: right;">74.6%</td>
        <td>accuracy <span title="higher is better">+</span></td>
        <td>Google - <a href="https://arxiv.org/abs/1503.03832">FaceNet</a> v8</td>
    </tr>
    <tr>
        <td><a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CelebA</a></td>
        <td>?</td>
        <td style="text-align: right;">?</td>
        <td>?</td>
        <td>?</td>
    </tr>
    <tr>
        <td><a href="http://benchmark.ini.rub.de/?section=gtsrb&subsection=news">GTSRB</a></td>
        <td>2017</td>
        <td style="text-align: right;">99.51%</td>
        <td>accuracy <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/abs/1707.09725" title="Analysis and Optimization of Convolutional Neural Network Architectures">[Tho17-2]</a></td>
    </tr>
</table>

State of the art in this category are CNN models which use skip connections
in the form of residual connections or dense connections.

The evaluation metrics are straight-forward:

* **Accuracy**: Count how many elements of the test dataset you got right,
  divided by the total number of elements in the test dataset. The accuracy is
  in $[0, 1]$. Higher is better.
* **Error** = 1 - accuracy. The error is in $[0, 1]$. Lower is better.
* **Top-k accuracy**: Sometimes, there are either extremely similar classes or
  the application allows having multiple guesses. Hence not the Top-1 guess
  of the network has to be right, but the correct label has to be within the
  top $k$ guesses. The top-$k$ accuracy is in $[0, 1]$. Higher is better.


### Detection (Images)

Face recognition is a special case of detection.

Common metrics are:
<ul>
    <li>mAP (Mean Average Precision): A detection is successfull, if the bounding
        box prediction and the true bounding box $\frac{intersection}{union}$ (IU, IoU)
        ratio is at least 0.5. Then the average precision = $\frac{TP}{TP + FP}$ is
        calculated for each class and the mean is calculated of those (see <a href="http://stackoverflow.com/q/36274638/562769">Explanation</a>, <a href="http://datascience.stackexchange.com/q/16797/8820">What does the notation mAP@[.5:.95] mean?</a>)</li>
    <li>MR (miss rate)</li>
</ul>

<table class="table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html">PASCAL VOC 2012</a></td>
        <td>2015</td>
        <td style="text-align: right;">75.9</td>
        <td>mAP@.5 <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/abs/1506.01497" title="Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks">[RHGS15]</a></td>
    </tr>
    <tr>
        <td><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2011/index.html">PASCAL VOC 2011</a></td>
        <td>2014</td>
        <td style="text-align: right;">62.7</td>
        <td>mean IU <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/abs/1411.4038" title="Fully Convolutional Networks for Semantic Segmentation">[LSD14]</a></td>
    </tr>
    <tr>
        <td><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2010/index.html">PASCAL VOC 2010</a></td>
        <td>2011</td>
        <td style="text-align: right;">30.2</td>
        <td>mean accuracy <span title="higher is better">+</span></td>
        <td><a href="https://papers.nips.cc/paper/4296-efficient-inference-in-fully-connected-crfs-with-gaussian-edge-potentials.pdf" title="Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials">[Kol11]</a></td>
    </tr>
    <tr>
        <td><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2007/index.html">PASCAL VOC 2007</a></td>
        <td>2015</td>
        <td style="text-align: right;">71.6</td>
        <td>mAP@.5 <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/abs/1512.02325" title="SSD: Single Shot MultiBox Detector">[LAES+15]</a></td>
    </tr>
    <tr>
        <td><a href="http://mscoco.org/">MS COCO</a></td>
        <td>2015</td>
        <td style="text-align: right;">46.5</td>
        <td>mAP@.5 <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/abs/1512.02325" title="SSD: Single Shot MultiBox Detector">[LAES+15]</a></td>
    </tr>
    <tr>
        <td><a href="https://arxiv.org/abs/1702.05693" title="CityPersons: A Diverse Dataset for Pedestrian Detection">CityPersons</a></td>
        <td>2017</td>
        <td style="text-align: right;">33.10</td>
        <td><abbr title="log miss-rate">MR</abbr> <span title="lower is better">-</span></td>
        <td><a href="https://arxiv.org/abs/1702.05693" title="CityPersons: A Diverse Dataset for Pedestrian Detection">[ZBS17]</a></td>
    </tr>
</table>


### Detection (Videos)

<table>
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td><a href="https://research.google.com/youtube-bb/">YouTube-BoundingBoxes</a></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
</table>


### Person Re-Identitification

Person Re-ID is the task of identifying a person again which was already seen
in a video stream. Person following and <abbr title="Multi Target Multi
Camera">MTMCT</abbr> seems to be very similar if not identical.

<table class="table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td><a href="http://www.liangzheng.org/Project/project_reid.html">Market-1501</a></td>
        <td>2017</td>
        <td>62.1</td>
        <td>mAP <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/abs/1703.05693">[SZDW17]</a></td>
    </tr>
    <tr>
        <td><a href="http://www.ee.cuhk.edu.hk/~xgwang/CUHK_identification.html">CUHK03</a></td>
        <td>2017</td>
        <td>84.8</td>
        <td>mAP <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/abs/1703.05693">[SZDW17]</a></td>
    </tr>
    <tr>
        <td><a href="http://vision.cs.duke.edu/DukeMTMC/">DukeMTMC</a></td>
        <td>2017</td>
        <td>56.8</td>
        <td>mAP <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/abs/1703.05693">[SZDW17]</a></td>
    </tr>
</table>


### Semantic Segmentation

A summary of classical methods for semantic segmentation, more information
to several datasets and metrics for evaluation can be found in [A Survey of Semantic Segmentation](https://arxiv.org/abs/1602.06541).

<table class="table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td><a href="https://www.microsoft.com/en-us/research/project/image-understanding/">MSRC-21</a></td>
        <td>2011</td>
        <td style="text-align: right;">84.7</td>
        <td>mean accuracy <span title="higher is better">+</span></td>
        <td><a href="https://papers.nips.cc/paper/4296-efficient-inference-in-fully-connected-crfs-with-gaussian-edge-potentials.pdf" title="Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials">[Kol11]</a></td>
    </tr>
    <tr>
        <td><a href="http://www.cvlibs.net/datasets/kitti/eval_road.php">KITTI Road</a></td>
        <td></td>
        <td style="text-align: right;">96.69</td>
        <td>Max F1 <span title="higher is better">+</span></td>
        <td></td>
    </tr>
    <tr>
        <td><a href="http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html" title="NYU Depth Dataset V2">NYUDv2</a></td>
        <td>2014</td>
        <td style="text-align: right;">34.0</td>
        <td>mean IO <span title="higher is better">+</span></td>
        <td><a href="https://papers.nips.cc/paper/4296-efficient-inference-in-fully-connected-crfs-with-gaussian-edge-potentials.pdf" title="Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials">[Kol11]</a></td>
    </tr>
    <tr>
        <td><a href="https://people.csail.mit.edu/celiu/SIFTflow/">SIFT Flow</a></td>
        <td>2014</td>
        <td style="text-align: right;">39.5</td>
        <td>mean IU <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/abs/1411.4038" title="Fully Convolutional Networks for Semantic Segmentation">[LSD14]</a></td>
    </tr>
    <tr>
        <td><a href="http://www.it.lut.fi/project/imageret/diaretdb1/">DIARETDB1</a></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td><a href="https://www2.warwick.ac.uk/fac/sci/dcs/research/combi/research/bic/glascontest/download/">Warwick-QU</a></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td><a href="https://dataverse.scholarsportal.info/dataset.xhtml?persistentId=doi:10.5683/SP/NTUOK9">Ciona17</a></td>
        <td>2017</td>
        <td>51.36 %</td>
        <td>mean IoU <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/pdf/1702.05564.pdf" title="The Ciona17 Dataset for Semantic Segmentation of Invasive Species in a Marine Aquaculture Environment">[GTRM17]</a></td>
    </tr>
</table>


### Instance Segmentation

See <a href="https://arxiv.org/abs/1512.04412" title="Instance-aware Semantic Segmentation via Multi-task Network Cascades">[DHS15]</a>

<table class="table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td><a href="https://www.cityscapes-dataset.com/benchmarks/">CityScapes</a></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
</table>


### Action Recognition

Action recognition is a classification problem over a short video clip.


<table class="table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td><a href="https://research.google.com/youtube8m/">YouTube-8M</a></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td><a href="https://github.com/gtoderici/sports-1m-dataset/blob/wiki/ProjectHome.md">Sports-1M</a></td>
        <td>2015</td>
        <td>68.7 %</td>
        <td>Clip Hit@1 accuracy <span title="higher is better">+</span></td>
        <td><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ng_Beyond_Short_Snippets_2015_CVPR_paper.pdf" title="Beyond Short Snippets: Deep Networks for Video Classification">[NHV+15]</a></td>
    </tr>
    <tr>
        <td>UCF-101</td>
        <td>2015</td>
        <td>70.8 %</td>
        <td>Clip Hit@1 accuracy <span title="higher is better">+</span></td>
        <td><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ng_Beyond_Short_Snippets_2015_CVPR_paper.pdf" title="Beyond Short Snippets: Deep Networks for Video Classification">[NHV+15]</a></td>
    </tr>
    <tr>
        <td><a href="http://www.nada.kth.se/cvap/actions/">KTH</a></td>
        <td>2015</td>
        <td style="text-align: right;">95.6 %</td>
        <td>EER <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/pdf/1512.03980.pdf" title="Action Recognition with Image Based CNN Features">[RMRMD15]</a></td>
    </tr>
    <tr>
        <td><a href="http://crcv.ucf.edu/data/UCF_Sports_Action.php">UCF Sport</a></td>
        <td>2015</td>
        <td style="text-align: right;">97.8 %</td>
        <td>EER <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/pdf/1512.03980.pdf" title="Action Recognition with Image Based CNN Features">[RMRMD15]</a></td>
    </tr>
    <tr>
        <td><a href="http://crcv.ucf.edu/data/UCF_YouTube_Action.php">UCF-11 Human Action</a></td>
        <td>2015</td>
        <td style="text-align: right;">89.5 %</td>
        <td>EER <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/pdf/1512.03980.pdf" title="Action Recognition with Image Based CNN Features">[RMRMD15]</a></td>
    </tr>
</table>


### Super Resolution

See [github.com/huangzehao](https://github.com/huangzehao/Super-Resolution.Benckmark)

<table class="table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
</table>

I'm not sure how super resolution is benchmarked. One way to do it would be
to get high resolution images, scale them down, feed them to the network and
measure the mean squared error for each pixel:

$$\frac{1}{|I|} \sum_{t \in I} {(t - \hat{t})}^2$$

However, this might be sensitive to the way the images were downsampled.


### Lip Reading

<table class="table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td><a href="http://spandh.dcs.shef.ac.uk/gridcorpus/" title="The GRID audiovisual sentence corpus

">GRID</a></td>
        <td>2016</td>
        <td>95.2 %</td>
        <td>accuracy <span title="higher is better">+</span></td>
        <td><a href="https://openreview.net/forum?id=BkjLkSqxg" title="LipNet: End-to-End Sentence-level Lipreading">[ASWF16]</a></td>
    </tr>
</table>


### Other Datasets

For the following datasets, I was not able to find where to download them

* Mapping global urban areas using MODIS 500-m data: New methods and datasets
  based on urban ecoregions
* [TorontoCity: Seeing the World with a Million Eyes](https://arxiv.org/abs/1612.00423)


## ASR

Automatic Speech Recognition (ASR).

### Sentence-Level

<table class="table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td><a href="" title="Wall Street Journal">WSJ</a> (eval92)</td>
        <td>2015</td>
        <td>3.47</td>
        <td>WER <span title="lower is better">-</span></td>
        <td><a href="https://arxiv.org/pdf/1504.01482v1.pdf" title="Deep Recurrent Neural Networks for Acoustic Modelling">[CL15]</a></td>
    </tr>
    <tr>
        <td>Switchboard Hub5'00</td>
        <td>2016</td>
        <td>6.3%</td>
        <td>WER <span title="lower is better">-</span></td>
        <td><a href="https://arxiv.org/pdf/1609.03528v1.pdf" title="The Microsoft 2016 conversational speech recognition system">[XDSS+16]</a></td>
    </tr>
</table>

See [Word Error Rate](https://martin-thoma.com/word-error-rate-calculation/) (WER)
for an explanation of the metric.

Relevant papers might be

* [Deep Speech 2: End-to-End Speech Recognition in English and Mandarin](https://arxiv.org/abs/1512.02595v1)

### Phoneme-Level

<table class="table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td><a href="https://catalog.ldc.upenn.edu/ldc93s1" title="TIMIT Acoustic-Phonetic Continuous Speech Corpus">TIMIT</a></td>
        <td>2013</td>
        <td>17.7 %</td>
        <td>error rate <span title="lower is better">-</span></td>
        <td><a href="http://ieeexplore.ieee.org/abstract/document/6638947/" title="Speech recognition with deep recurrent neural networks">[GMH13]</a></td>
    </tr>
</table>


## Language

Natural Language Processing (NLP) deals with how to represent language. It is
related and often a part of ASR.

<table class="table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td><a href="https://metamind.io/research/the-wikitext-long-term-dependency-language-modeling-dataset/">WikiText-103</a></td>
        <td>2016</td>
        <td style="text-align: right;">48.7</td>
        <td>Perplexity <span title="lower is better">-</span></td>
        <td><a href="https://openreview.net/pdf?id=B184E5qee" title="Improving Neural Language Models with a Continuous Cache">[GJU16]</a></td>
    </tr>
    <tr>
        <td>Penn Treebank (PTB)</td>
        <td>2016</td>
        <td style="text-align: right;">62.4</td>
        <td>Perplexity <span title="lower is better">-</span></td>
        <td><a href="https://arxiv.org/abs/1611.01578" title="Neural Architecture Search with Reinforcement Learning">[ZL16]</a> (<a href="http://www.shortscience.org/paper?bibtexKey=journals/corr/1611.01578#martinthoma">summary</a>)</td>
    </tr>
    <tr>
        <td><a href="https://www.reddit.com/r/MachineLearning/comments/5s6ixw/d_what_are_the_current_benchmark_datasets_and_the/ddcwoay/">Stanford Sentiment Treebank</a></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
</table>

NLP benchmarks use [perplexity](https://en.wikipedia.org/wiki/Perplexity) to
measure how good a result is.


## Translation

<table class="table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td>MT03</td>
        <td>2003</td>
        <td>35.76</td>
        <td>BLEU</td>
        <td><a href="https://www.cs.sfu.ca/~anoop/papers/pdf/jhu-ws03-report.pdf" title="Syntax for Statistical Machine Translation">[OGKS+03]</a></td>
    </tr>
</table>

The <a href="https://en.wikipedia.org/wiki/BLEU" title="bilingual evaluation understudy">BLEU</a>
score is used to measure how good a translation system is.

Another score is the *Translation Edit Rate* (TER) introduced by
<a href="http://mt-archive.info/AMTA-2006-Snover.pdf">Snover et al., 2006</a>.


## Matrix completion

Collaborative filtering is an application of matrix completion.
More datasets are on [entaroadun/gist:1653794](https://gist.github.com/entaroadun/1653794).

<table class="table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td><a href="https://grouplens.org/datasets/movielens/">MovieLens</a></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td><a href="http://eigentaste.berkeley.edu/dataset/">Jester</a></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
</table>


## Reinforcment Learning

The <a href="https://gym.openai.com/">OpenAI Gym</a> offers many environments
for testing RL algorithms.

<table class="table">
    <tr>
        <th>Challenge</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td>Chess</td>
        <td></td>
        <td>3395</td>
        <td></td>
        <td><a href="https://stockfishchess.org/">Stockfishchess</a></td>
    </tr>
    <tr>
        <td>Go</td>
        <td>2015</td>
        <td>3,168</td>
        <td>ELO <span title="higher is better">+</span></td>
        <td><a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html" title="Mastering the game of Go with deep neural networks and tree search">AlphaGo</a></td>
    </tr>
    <tr>
        <td>Star Craft</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
</table>


## Control

<table class="table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td><a href="CartPole-v0">Cart Pole</a></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
</table>


## See also

* [Are we there yet ?](http://rodrigob.github.io/are_we_there_yet)
* [Some state-of-the-arts in natural language processing and their discussion](https://www.reddit.com/r/MachineLearning/comments/4dkrw1/some_stateofthearts_in_natural_language/)
* aclweb.org: [State of the art](https://www.aclweb.org/aclwiki/index.php?title=State_of_the_art) - NLP tasks
* [wer_are_we](https://github.com/syhw/wer_are_we/tree/master): SotA in ASR
* [github.com/michalwols/ml-sota](https://github.com/michalwols/ml-sota)


More datasets

* [List of datasets for machine learning research](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research)
* [traffic-signs-dataset](http://www.cvl.isy.liu.se/research/datasets/traffic-signs-dataset/download/)
* [Stanford Dogs](http://vision.stanford.edu/aditya86/ImageNetDogs/)
* [Awesome Public Datasets](https://github.com/caesar0301/awesome-public-datasets)
* [archive.ics.uci.edu/ml/datasets.html](https://archive.ics.uci.edu/ml/datasets.html)
* [Tiny ImageNet Visual Recognition Challenge](https://tiny-imagenet.herokuapp.com/)
