---
layout: post
title: State of the Art in ML
slug: sota
author: Martin Thoma
date: 2017-02-06 20:00
category: Machine Learning
tags: Machine Learning, Datasets
featured_image: logos/ml.png
---

It is difficult to keep track of the current state of the art (SotA). Also, it
might not be directly clear which datasets are relevant. The following list
should help. If you think some datasets / problems / SotA results are missing,
let me know in the comments or via E-mail (<code>info@martin-thoma.de</code>).
I will update it.

Papers and blog posts which summarize a topic or give a good introduction are
always welcome.

In the following, a <code>+</code> will indicate "higher is better" and a
<code>-</code> will indicate "lower is better".


## Computer Vision

### Image Classification

<table class="table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td><a href="http://www.image-net.org/challenges/LSVRC/2012/nonpub-downloads">ImageNet 2012</a></td>
        <td>2015</td>
        <td style="text-align: right;">3.57 %</td>
        <td>Top-5 error <span title="lower is better">-</span></td>
        <td><a href="https://arxiv.org/pdf/1512.03385v1.pdf" title="Deep Residual Learning for Image Recognition">[HZRS15a]</a></td>
    </tr>
    <tr>
        <td><a href="http://yann.lecun.com/exdb/mnist/">MNIST</a></td>
        <td>2013</td>
        <td style="text-align: right;">0.21 %</td>
        <td>error <span title="lower is better">-</span></td>
        <td><a href="http://www.matthewzeiler.com/pubs/icml2013/icml2013.pdf" title="Regularization of Neural Networks using DropConnect">[WZZ+13]</a></td>
    </tr>
    <tr>
        <td><a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a></td>
        <td>2016</td>
        <td style="text-align: right;">3.46 %</td>
        <td>error <span title="lower is better">-</span></td>
        <td><a href="https://arxiv.org/abs/1608.06993v1" title="Densely Connected Convolutional Networks">[HLW16]</a></td>
    </tr>
    <tr>
        <td><a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-100</a></td>
        <td>2016</td>
        <td style="text-align: right;">17.18 %</td>
        <td>error <span title="lower is better">-</span></td>
        <td><a href="https://arxiv.org/abs/1608.06993v1" title="Densely Connected Convolutional Networks">[HLW16]</a></td>
    </tr>
    <tr>
        <td><a href="http://cs.stanford.edu/~acoates/stl10/">STL-10</a></td>
        <td>2015</td>
        <td style="text-align: right;">74.80 %</td>
        <td>accuracy <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/abs/1506.02351v1" title="Stacked What-Where Auto-encoders">[ZMGL15]</a></td>
    </tr>
    <tr>
        <td><a href="http://ufldl.stanford.edu/housenumbers/">SVHN</a></td>
        <td>2016</td>
        <td style="text-align: right;">1.59 %</td>
        <td>error <span title="lower is better">-</span></td>
        <td><a href="https://arxiv.org/abs/1608.06993v1" title="Densely Connected Convolutional Networks">[HLW16]</a></td>
    </tr>
    <tr>
        <td><a href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/Caltech101.html">Caltech-101</a></td>
        <td>2014</td>
        <td style="text-align: right;">86.5 %</td>
        <td>accuracy <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/abs/1311.2901" title="Visualizing and Understanding Convolutional Networks">[ZF14]</a></td>
    </tr>
    <tr>
        <td><a href="http://www.vision.caltech.edu/Image_Datasets/Caltech256/">Caltech-256</a></td>
        <td>2014</td>
        <td style="text-align: right;">74.2 %</td>
        <td>accuracy <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/abs/1311.2901" title="Visualizing and Understanding Convolutional Networks">[ZF14]</a></td>
    </tr>
    <tr>
        <td><a href="https://zenodo.org/record/259444" title="The HASYv2 dataset">HASYv2</a></td>
        <td>2017</td>
        <td style="text-align: right;">81.00 %</td>
        <td>accuracy <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/abs/1701.08380" title="The HASYv2 dataset">[Tho17]</a></td>
    </tr>
    <tr>
        <td><a href="http://lear.inrialpes.fr/people/marszalek/data/ig02/">Graz-02</a></td>
        <td>2010</td>
        <td style="text-align: right;">78.98 %</td>
        <td>accuracy <span title="higher is better">+</span></td>
        <td><a href="http://imagine.enpc.fr/publications/papers/ECCV2010b.pdf" title="Towards Optimal Naive Bayes Nearest Neighbor">[BMDP10]</a></td>
    </tr>
    <tr>
        <td><a href="http://yfcc100m.appspot.com/">YFCC100m</a></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
</table>

State of the art in this category are CNN models which use skip connections
in the form of residual connections or dense connections.


### Detection

Face recognition is a special case of detection.

<table class="table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html">PASCAL VOC 2012</a></td>
        <td>2014</td>
        <td style="text-align: right;">62.2</td>
        <td>mean IU <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/abs/1411.4038" title="Fully Convolutional Networks for Semantic Segmentation">[LSD14]</a></td>
    </tr>
    <tr>
        <td><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2011/index.html">PASCAL VOC 2011</a></td>
        <td>2014</td>
        <td style="text-align: right;">62.7</td>
        <td>mean IU <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/abs/1411.4038" title="Fully Convolutional Networks for Semantic Segmentation">[LSD14]</a></td>
    </tr>
    <tr>
        <td><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2010/index.html">PASCAL VOC 2010</a></td>
        <td>2011</td>
        <td style="text-align: right;">30.2</td>
        <td>mean accuracy <span title="higher is better">+</span></td>
        <td><a href="https://papers.nips.cc/paper/4296-efficient-inference-in-fully-connected-crfs-with-gaussian-edge-potentials.pdf" title="Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials">[Kol11]</a></td>
    </tr>
</table>

### Semantic Segmentation

A summary of classical methods for semantic segmentation, more information
to several datasets and metrics for evaluation can be found in [A Survey of Semantic Segmentation](https://arxiv.org/abs/1602.06541).

<table class="table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td><a href="https://www.microsoft.com/en-us/research/project/image-understanding/">MSRC-21</a></td>
        <td>2011</td>
        <td style="text-align: right;">84.7</td>
        <td>mean accuracy <span title="higher is better">+</span></td>
        <td><a href="https://papers.nips.cc/paper/4296-efficient-inference-in-fully-connected-crfs-with-gaussian-edge-potentials.pdf" title="Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials">[Kol11]</a></td>
    </tr>
    <tr>
        <td><a href="http://www.cvlibs.net/datasets/kitti/eval_road.php">KITTI Road</a></td>
        <td></td>
        <td style="text-align: right;">96.69</td>
        <td>Max F1 <span title="higher is better">+</span></td>
        <td></td>
    </tr>
    <tr>
        <td><a href="http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html" title="NYU Depth Dataset V2">NYUDv2</a></td>
        <td>2014</td>
        <td style="text-align: right;">34.0</td>
        <td>mean IO <span title="higher is better">+</span></td>
        <td><a href="https://papers.nips.cc/paper/4296-efficient-inference-in-fully-connected-crfs-with-gaussian-edge-potentials.pdf" title="Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials">[Kol11]</a></td>
    </tr>
    <tr>
        <td><a href="https://people.csail.mit.edu/celiu/SIFTflow/">SIFT Flow</a></td>
        <td>2014</td>
        <td style="text-align: right;">39.5</td>
        <td>mean IU <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/abs/1411.4038" title="Fully Convolutional Networks for Semantic Segmentation">[LSD14]</a></td>
    </tr>
    <tr>
        <td><a href="http://mscoco.org/">MS COCO</a></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td><a href="http://www.it.lut.fi/project/imageret/diaretdb1/">DIARETDB1</a></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td><a href="https://www2.warwick.ac.uk/fac/sci/dcs/research/combi/research/bic/glascontest/download/">Warwick-QU</a></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
</table>


### Instance Segmentation

<table class="table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td><a href="https://www.cityscapes-dataset.com/benchmarks/">CityScapes</a></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
</table>


### Action Recognition

Action recognition is a classification problem over a short video clip.


<table class="table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td><a href="https://research.google.com/youtube8m/">YouTube-8M</a></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td><a href="https://github.com/gtoderici/sports-1m-dataset/blob/wiki/ProjectHome.md">Sports-1M</a></td>
        <td>2015</td>
        <td>68.7 %</td>
        <td>Clip Hit@1 accuracy <span title="higher is better">+</span></td>
        <td><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ng_Beyond_Short_Snippets_2015_CVPR_paper.pdf" title="Beyond Short Snippets: Deep Networks for Video Classification">[NHV+15]</a></td>
    </tr>
    <tr>
        <td>UCF-101</td>
        <td>2015</td>
        <td>70.8 %</td>
        <td>Clip Hit@1 accuracy <span title="higher is better">+</span></td>
        <td><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ng_Beyond_Short_Snippets_2015_CVPR_paper.pdf" title="Beyond Short Snippets: Deep Networks for Video Classification">[NHV+15]</a></td>
    </tr>
    <tr>
        <td><a href="http://www.nada.kth.se/cvap/actions/">KTH</a></td>
        <td>2015</td>
        <td style="text-align: right;">95.6 %</td>
        <td>EER <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/pdf/1512.03980.pdf" title="Action Recognition with Image Based CNN Features">[RMRMD15]</a></td>
    </tr>
    <tr>
        <td><a href="http://crcv.ucf.edu/data/UCF_Sports_Action.php">UCF Sport</a></td>
        <td>2015</td>
        <td style="text-align: right;">97.8 %</td>
        <td>EER <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/pdf/1512.03980.pdf" title="Action Recognition with Image Based CNN Features">[RMRMD15]</a></td>
    </tr>
    <tr>
        <td><a href="http://crcv.ucf.edu/data/UCF_YouTube_Action.php">UCF-11 Human Action</a></td>
        <td>2015</td>
        <td style="text-align: right;">89.5 %</td>
        <td>EER <span title="higher is better">+</span></td>
        <td><a href="https://arxiv.org/pdf/1512.03980.pdf" title="Action Recognition with Image Based CNN Features">[RMRMD15]</a></td>
    </tr>
</table>


### Super Resolution

<table class="table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
</table>


## ASR

Automatic Speech Recognition (ASR).

<table class="table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
</table>


## Language

Natural Language Processing (NLP) deals with how to represent language. It is
related and often a part of ASR.

<table class="table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td><a href="https://metamind.io/research/the-wikitext-long-term-dependency-language-modeling-dataset/">WikiText-103</a></td>
        <td>2016</td>
        <td style="text-align: right;">48.7</td>
        <td>Perplexity <span title="lower is better">-</span></td>
        <td><a href="https://openreview.net/pdf?id=B184E5qee" title="Improving Neural Language Models with a Continuous Cache">[GJU16]</a></td>
    </tr>
    <tr>
        <td>Penn Treebank?</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
</table>


## Reinforcment Learning

<table class="table">
    <tr>
        <th>Challenge</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td>Chess</td>
        <td></td>
        <td>3395</td>
        <td></td>
        <td><a href="https://stockfishchess.org/">Stockfishchess</a></td>
    </tr>
    <tr>
        <td>Go</td>
        <td>2015</td>
        <td>3,168</td>
        <td>ELO <span title="higher is better">+</span></td>
        <td><a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html" title="Mastering the game of Go with deep neural networks and tree search">AlphaGo</a></td>
    </tr>
    <tr>
        <td>Star Craft</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
</table>


## Control

<table class="table">
    <tr>
        <th>Dataset</th>
        <th>Year</th>
        <th>Score</th>
        <th>Type</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td>Pole balancing</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
</table>


## See also

* [Are we there yet ?](http://rodrigob.github.io/are_we_there_yet)
