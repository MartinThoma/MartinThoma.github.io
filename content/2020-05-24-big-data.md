---
layout: post
title: Big Data: Sorting
slug: big-data-sorting
author: Martin Thoma
status: draft
date: 2020-05-21 20:00
category: Code
tags: Big Data, Python, Bash, sorting
featured_image: logos/python.png
---
[Big Data](https://en.wikipedia.org/wiki/Big_data) was a common buzzword in
industry in 2012 until 2017. It is still common, but hopefully less of a
buzzword. Now other other buzzwords are more common:

<script type="text/javascript" src="https://ssl.gstatic.com/trends_nrtr/2152_RC03/embed_loader.js"></script> <script type="text/javascript"> trends.embed.renderExploreWidget("TIMESERIES", {"comparisonItem":[{"keyword":"/m/0bs2j8q","geo":"","time":"2008-01-01 2020-03-24"},{"keyword":"/m/01hyh_","geo":"","time":"2008-01-01 2020-03-24"},{"keyword":"/m/0138n0j1","geo":"","time":"2008-01-01 2020-03-24"},{"keyword":"/m/0wkcjgj","geo":"","time":"2008-01-01 2020-03-24"},{"keyword":"/m/02_7vm","geo":"","time":"2008-01-01 2020-03-24"}],"category":0,"property":""}, {"exploreQuery":"date=2008-01-01%202020-03-24&q=%2Fm%2F0bs2j8q,%2Fm%2F01hyh_,%2Fm%2F0138n0j1,%2Fm%2F0wkcjgj,%2Fm%2F02_7vm","guestPath":"https://trends.google.de:443/trends/embed/"}); </script>

By the way, I didn't include "artificial intelligence" because it dominates all
of the others.

For me, a big-data solution means that I can't fit the critical part into
memory. This means that it is a question of time and money what problems
actually require a big data solution. Amazons `u-24tb1.metal` instance has 24
TB of memory. Yes, that is not a typo on my side. TB, not GB.
([source](https://aws.amazon.com/de/ec2/instance-types/high-memory/)). They
don't even publicly say how expensive those beasts are.

When it comes to big data, people think of
[Hadoop](https://en.wikipedia.org/wiki/Apache_Hadoop) and
[Spark](https://en.wikipedia.org/wiki/Apache_Spark). However, for this article
I want to use pure Python. In this article I want to show how to sort huge
amounts of data on a single machine: My Thinkpad T460p with 8GB of memory.

Please note that an `x1e.32xlarge` EC2 instance with about 4 TB of RAM costs
about 27 USD per hour
([source](https://aws.amazon.com/de/ec2/pricing/on-demand/)). This means you
might not need such a solution for quite a while.


## Data Generation

I want to generate a bit of data and make it at least a tiny bit realistic. So
lets generate about 20GB of data in a CSV file.

### Disk Space

With `df -h` I realized that I only have 5 GB left of my 500GB HDD ðŸ˜¢

The [Disk Usage Analyzer](http://www.marzocca.net/linux/baobab/) showed the
following disk space hogs:

* 404 GB: My Home Directory:
    * 152 GB: Various git repsitories
        * 103 GB: My "algorithms" repository
            * 99 GB for PyPI (see [PyPI Analysis 2020](https://martin-thoma.com/pypi-2020/))
            * 4 GB for a [database benchmark](https://github.com/MartinThoma/algorithms/tree/master/Python/databases/benchmark). I never really finished this; I moved on to other topics
        * 28 GB for [YouTube-Report](https://github.com/A3M4/YouTube-Report). Or rather the Takeout
    * 97 GB: Downloads
        * 1.5 GB: Photos of a hiking trip
        * 800 MB: [HASYv2](https://arxiv.org/pdf/1701.08380.pdf)
        * many smaller things ... that would take a while. Maybe I should just delete everything
    * 44 GB: Pictures
    * 22 GB: Dropbox
    * 20 GB: .local
    * 10 GB: .cache (mostly pip and pipenv)
    * 9 GB: Anaconda
    * many smaller things
* 26 GB `/usr`
    * 7 GB TexLive
    * 6 GB CUDA in different versions
* 7 GB `/var`
    * 5 GB journal logs
* some smaller things

I'm astonished not to see Docker here ðŸ¤”

After this short cleanup, I have `128G` available ðŸŽ‰

### Generate it!

I don't want to spend too much time dealing with the actual element-by-element
comparison, so I want to use integers or strings to compare. I also don't want
to fiddle around with data organization, so I don't add a payload. We only
generate data which is sorted.

I can imagine two ways to generate data to sort: Random numbers and UUIDs.
Let's see which is faster ([code on GitHub](https://github.com/MartinThoma/algorithms/blob/master/sorting/timing.py)):

<figure class="wp-caption aligncenter img-thumbnail">
    <a href="../images/2020/03/number-generation-speed.png"><img src="../images/2020/03/number-generation-speed.png" alt="Time for generating 10k random elements" style="width: 512px;"/></a>
    <figcaption class="text-center">Time for generating 10k random elements</figcaption>
</figure>

As you can see, numpy is the fastest and UUIDv4 generation is by far the
slowest. But numpy cannot generate random integers which are that big and the
time difference is not that huge. Everything as expected.

So, let's say we generate those 36-character random numbers. Each of them needs
37 Byte - don't forget the newline. We want 20GB, we need 540,540,541 elements.
Lets say 550 million numbers. My machine needed about 0.05s to generate 10k, so
I expect it to take `540540541 / 10_000  * 0.05 / 60 = 45 min`. A good time to
get some food ðŸ™‚

Done after 32&nbsp;minutes. The file size is 20.4&nbsp;GB.


## Bash Sorting

Sorting with standard unix tools - namely `split` and `sort` - is the simplest
solution that popped to my mind which should also be pretty fast. I will
wrap all commands in `time (the actual command)` so that you can see how
fast they are.

Chunk the data:

```shell
$ time (split -l 10000000 numbers-large.txt chunk-)

real    89,00s
user    5,16s
sys    25,29s
```

Sort the chunks:

```shell
$ time (for X in chunk-*; do sort -n < $X > sorted-$X; done)

real    625,44s
user    2768,27s
sys    85,40s
```

Merge:

```shell
$ time (sort -n -m sorted-chunk* > sorted.txt)

real    531,50s
user    470,09s
sys    52,20s
```

In total, this approach needed **20 min and 46 seconds**.

I will check other solutions for correctnes by the following command. It takes
91s just to check if the two files are identical!

```shell
$ cmp -l shell-sort/sorted.txt other-solution.txt
```


## Python

Let's see how quick I can do it with Python!

### Merge Sort

### Trivial Radix Sort

The idea is:

1. **Chunking**: Generate 90 chunks for all of the 2-letter prefixes (10, 11,
   12, ... 97, 98, 99). Each chunk is now small enough to fit into memory.
2. **Chunk sorting**: Sort each chunk individually.
3. **Combining**: Combine all of the chunks. As they were created by prefixes,
   this is just simply pasting the files together in the order of the prefixes.


Timing:

1. Chunking: 290.4s
2. Chunk Sorting: 779.5s
3. Combining: 208.4s

In total, the Python Radix-Sort is at **21 min and 30 seconds**.


### Multi-process Radix Sort

Most of the time is spent on sorting. However, I only used one core. So we can
improve the wall-clock execution time by running on multiple cores!

To do so, we make a minor adjustment:

```python
def sort_chunks(state: Dict[str, Any]) -> Dict[str, Any]:
    from multiprocessing import Pool

    pool = Pool(processes=8)
    pool.map(sort_chunk, state["chunks_to_sort"])
    return state
```

Timing:

1. Chunking: 290.4s
2. Chunk Sorting: 266.2s
3. Combining: 208.4s


In total, the Python parallel Radix-Sort is at **12 min and 45 seconds**.

## See also

StackExchange

* [How can I sort a 10GB file?](https://stackoverflow.com/q/34090744/562769), 2015.
