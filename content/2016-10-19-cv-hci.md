---
layout: post
title: Computer Vision for Human-Computer Interaction
slug: cv-hci
author: Martin Thoma
date: 2016-10-19 20:00
category: German posts
tags: Klausur, Computer Vision
featured_image: logos/klausur.png
---
<div class="info">Dieser Artikel beschäftigt sich mit der Vorlesung &bdquo;Computer Vision for Human-Computer Interaction&ldquo; am KIT. Er dient als Prüfungsvorbereitung. Ich habe die Vorlesungen bei <a href="https://cvhci.anthropomatik.kit.edu/people_596.php">Herrn Prof. Dr.-Ing. Rainer Stiefelhagen</a> im Wintersemester 2016/2017 gehört. Die Inhalte sind dementsprechend stark an der Vorlesung angelehnt bzw. komplette Teile sind daraus übernommen. Noch ist der Artikel in der Entwurfsphase.</div>

Der Kern der Vorlesung 'Computer Vision for Human-Computer Interaction' ist das finden und verfolgen von Personen / Gesichtern in Bildern und Bildfolgen. Dabei werden folgende Themenfelder besprochen:

* Trackingverfahren: Kalman-Filter und Partikelfilter


## Behandelter Stoff

<table>
<tr>
    <th>#</th>
    <th>Datum</th>
    <th>Kapitel</th>
    <th>Inhalt</th>
</tr>
<tr>
    <td>1</td>
    <td>18.10.2016</td>
    <td>Einführung</td>
    <td>Organisatorisches und Überblick über den Stoff</td>
</tr>
<tr>
    <td>2</td>
    <td>21.10.2016</td>
    <td>Klassifikation</td>
    <td>Gaussian Mixture Models, EM, SVMs, Perceptron</td>
</tr>
<tr>
    <td>3</td>
    <td>24.10.2016</td>
    <td>Face Detection I</td>
    <td>Color Spaces (<i>HS</i>V, Y<i>UV</i>), Histogram Backprojection, Histogram Matching, Mixture of Gaussians, ROC, Morphological Operations</td>
</tr>
<tr>
    <td>4</td>
    <td>28.10.2016</td>
    <td>Face Detection II</td>
    <td>Perceptron, MLP, Histogram Equalization, Haar-like features, Adaboost (Viola and Jones), </td>
</tr>
<tr>
    <td>5</td>
    <td>31.10.2016</td>
    <td>Face Recognition I</td>
    <td>Eigenface, Fisherface</td>
</tr>
<tr>
    <td>6</td>
    <td>04.11.2016</td>
    <td>Programmierprojekte</td>
    <td>Organisatorisches / Einführung dazu</td>
</tr>
<tr>
    <td>7</td>
    <td>07.11.2016</td>
    <td>Face Recognition 2</td>
    <td>Alignment (range affine warps), Morphing models of 3d forms (PCA), </td>
</tr>
<tr>
    <td>8</td>
    <td>11.11.2016</td>
    <td>CNNs</td>
    <td>Convolution, Pooling, ReLU, Normalization layers,</td>
</tr>
<tr>
    <td>9</td>
    <td>14.11.2016</td>
    <td>Programmierprojekte</td>
    <td>Besprechung der praktischen Aufgaben</td>
</tr>
<tr>
    <td>10</td>
    <td>18.11.2016</td>
    <td>?</td>
    <td>?</td>
</tr>
<tr>
    <td>11</td>
    <td>21.11.2016</td>
    <td>Facial Feature Detection</td>
    <td>?</td>
</tr>
<tr>
    <td>12</td>
    <td>25.11.2016</td>
    <td>Automatic Facial Expression Analysis</td>
    <td>?</td>
</tr>
<tr>
    <td>13</td>
    <td>28.11.2016</td>
    <td>Head Pose Estimation</td>
    <td>Model-based approaches, Appearence-based approaches</td>
</tr>
<tr>
    <td>14</td>
    <td>02.12.2016</td>
    <td>Person Detection</td>
    <td>Introduction, HOG people detector, Shilouette matching</td>
</tr>
<tr>
    <td>15</td>
    <td>05.12.2016</td>
    <td>?</td>
    <td>?</td>
</tr>
<tr>
    <td>16</td>
    <td>09.12.2016</td>
    <td>?</td>
    <td>?</td>
</tr>
<tr>
    <td>17</td>
    <td>16.12.2016</td>
    <td>Tracking II</td>
    <td>?</td>
</tr>
<tr>
    <td>-</td>
    <td>20.01.2017</td>
    <td>Visuelle Perzeption</td>
    <td>Kinect, Block Matching</td>
</tr>
<tr>
    <td>-</td>
    <td>23.01.2017</td>
    <td>Gesture Recognition</td>
    <td>HMMs; Pro Gesture eine HMM trainieren</td>
</tr>
<tr>
    <td>18</td>
    <td>27.01.2017</td>
    <td>Action &amp; Activity Recognition I</td>
    <td>?</td>
</tr>
<tr>
    <td>19</td>
    <td>30.01.2017</td>
    <td>Action &amp; Activity Recognition II</td>
    <td>?</td>
</tr>
<tr>
    <td>20</td>
    <td>06.02.2017</td>
    <td>Wrap-up</td>
    <td>Zusammenfassung der wichtigsten Themen</td>
</tr>
</table>


### Klassifikatoren

* [Curse of Dimensionality](https://martin-thoma.com/average-distance-of-points/)
* [k-means](https://martin-thoma.com/k-nearest-neighbor-classification-interactive-example/)
* [SVM](https://martin-thoma.com/svm-with-sklearn/)


### Histogramme

<dl>
    <dt><dfn id="histogram-equalization"><a href="https://en.wikipedia.org/wiki/Histogram_equalization">Histogram equalization</a></dfn></dt>
    <dd>

        This method usually increases the global contrast of many images,
        especially when the usable data of the image is represented by close
        contrast values. Through this adjustment, the intensities can be better
        distributed on the histogram. This allows for areas of lower local
        contrast to gain a higher contrast. Histogram equalization accomplishes
        this by effectively spreading out the most frequent intensity values.<br/>
        <br/>
        (Source: Wikipedia)

        The intensity values of the image are modified in such a way that the
        histogram is flattened.

    </dd>
    <dt><dfn id="image-normalization"><a href="https://en.wikipedia.org/wiki/Normalization_(image_processing)">Image normalization</a></dfn></dt>
    <dd>

        Normalization is a process that changes the range of pixel intensity
        values. Applications include photographs with poor contrast due to
        glare, for example. Normalization is sometimes called contrast
        stretching or histogram stretching.<br/>
        <br/>
        (Source: Wikpedia)

    </dd>
</dl>


### Face Detection

<dl>
    <dt><dfn>Face Detection</dfn></dt>
    <dd>

        Face Detection ist die Aufgabe, in einem gegebenen Bild eine
        Bounding-Box um jedes Gesicht zu zeichnen.<br/>
        <br/>
        Ein Ansatz ist, zu versuchen "Gesichtsfarbe" zu erkennen.<br/>
        <br/>
        Die Modellierung kann mit Histogrammen erfolgen.
    </dd>
    <dt><dfn>Histogram Backprojection</dfn></dt>
    <dd>

        Man geht für die Trainingsbeispiele alle Hautfarbe-Pixel durch. Bekommt
        man nun ein neues Bild, so geht man für dieses Bild jeden Pixel durch.
        Die Ausgabe ist ein Graustufenbild selber Größe, wo die Pixelfarbe die
        Anzahl der Hautfarbenen Pixel dieser Farbe ist.

    </dd>
    <dt><dfn><a href="https://en.wikipedia.org/wiki/Histogram_matching">Histogram Matching</a></dfn></dt>
    <dd>

        Erstelle ein Histogram für Hautfarbe. Um ein neues Bild zu
        klassifizieren, bildet man Ausschnitte für das Bild. Für jeden
        Ausschnitt vergleicht man das Histogram des Ausschnitts mit dem
        Hautfarbe-Histogram. Dazu können folgende Metriken verwendet werden:

        <ul>
            <li>Battacharya distance</li>
            <li>Histogram intersection</li>
            <li>Earth-movers distance</li>
        </ul>

    </dd>
    <dt><dfn><a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC</a></dfn></dt>
    <dd>TODO</dd>
</dl>


### Face Recognition

Alignment:

* Works only with "nice" images (no occlusion, high resolution)
* Eyes are hard
* Computationally intensive (not suitable for real time applications)

Face features:

<dl>
    <dt><dfn id="face-recognition-dfn">Face Recognition</dfn></dt>
    <dd>Face recognition has 4 main tasks:

    <ul>
        <li><b>Face detection</b>: Given an image, draw a rectangle around every face</li>
        <li><b>Face alignment</b>: Transform a face to be in a canonical pose</li>
        <li><b>Face representation</b>: Find a representation of a face which is suitable for follow-up tasks (small size, computationally cheap to compare, invariant to irrelevant changes)</li>
        <li><b>Face verification</b>: Images of two faces are given. Decide if it is the same person or not.</li>
    </ul>

    The face verification task is sometimes (more simply) a face classification
    task (given a face, decide which of a fixed set of people it is).

    Important papers:

        <ul>
            <li><a href="https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf">Deep Face: closing the gap to human level performance</a> (<a href="http://www.shortscience.org/paper?bibtexKey=conf/cvpr/TaigmanYRW14#martinthoma">summary</a>)</li>
            <li><a href="https://arxiv.org/abs/1503.03832">FaceNet: A unified embedding for face recognition and clustering</a> (<a href="http://www.shortscience.org/paper?bibtexKey=journals/corr/1503.03832">summary</a>)</li>
            <li><a href="https://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf">Deep Face recognition</a> (<a href="http://www.shortscience.org/paper?bibtexKey=conf/bmvc/ParkhiVZ15">summary</a>)</li>
        </ul>


        Datasets

        <ul>
            <li><a href="http://vis-www.cs.umass.edu/lfw/">LFW</a> (Labeled Faces in the Wild)</li>
            <li><a href="http://www.cs.tau.ac.il/~wolf/ytfaces/">YTF</a> (Youtube Faces)</li>
        </ul>

    </dd>
    <dt><dfn>Face Recognition Tasks</dfn></dt>
    <dd>

        <ul>
            <li>Closed set recognition: 120 Celebrities - who is it?</li>
            <li>Known / unknown: Is it one of the persons in the known set?</li>
            <li>Verification: Is it George Clooney?</li>
            <li>Open Set recognition: First known/unknwon, then closed set.</li>
        </ul>

    </dd>
    <dt><dfn id="gabor-filter"><a href="https://en.wikipedia.org/wiki/Gabor_filter">Gabor Filter</a></dfn></dt>
    <dd>TODO</dd>
    <dt><dfn><a href="https://en.wikipedia.org/wiki/Local_binary_patterns">Local Binary Pattern</a></dfn> (<dfn id="lbp">LBP</dfn>)</dt>
    <dd>TODO</dd>
    <dt><dfn id="sift"><a href="https://en.wikipedia.org/wiki/Scale-invariant_feature_transform">SIFT</a></dfn></dt>
    <dd>TODO

        SIFT vector is 128 dimensional
    </dd>
    <dt><dfn><a href="https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision">Bag of Visual Words</a></dfn> (<dfn id="bovw">BoVW</dfn>)</dt>
    <dd>TODO</dd>
    <dt><dfn id="fisher-encoding">Fisher Encoding</dfn></dt>
    <dd>TODO</dd>
</dl>


### CNNs

* Lernen Gabor Wavelets (TODO: Quelle?)
* Warum tiefere Netze? → Muster können geteilt werden (Räder für Traktoren / Motorräder)
* TODO: Filter response = Feature Map?
* TODO: Warum ist es ok, dass der Gradient von ReLU(x) für $x \leq 0$ gleich 0 ist? (Saturierungsproblem)
* TODO: Negative Log Likelihood vs Cross Correlation - what are differences?


<dl>
    <dt><dfn>Pooling</dfn> (<dfn>Subsampling</dfn>)</dt>
    <dd>

        Auf eine $k \times k$ Region der Feature-Map wird ein Operator (z.B. max, mean)
        angewendet, der diese Zusammenfasst. Typischerweise wird diese Operation
        mit einem Stride > 1 verwendet. Durch den Stride wird zugleich die
        Datenmenge auf $\frac{1}{s^2}$ reduziert.<br/>
        <br/>
        Pooling wird seperat pro Feature-Map angewendet.

    </dd>
    <dt><dfn>Normalization Layers</dfn></dt>
    <dd>

        TODO

        <ul>
            <li>Contrast Normalization</li>
            <li>Range Normalization</li>
        </ul>

    </dd>
</dl>


### Head-pose Estimation

* State of the Art: Regression based apprach (Random Regression Forests) is 15 years old
* slide 14: $r$ is distance, $f$ is focus length, $b$ is baseline, $d_L, d_R$ is distance left/right - in the later slides is a figure.
* Pan / Tilt / Jaw

<dl>
    <dt><a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)"><dfn id="entropy">Entropy</dfn></a></dt>
    <dd>The entropy $H$ of a discrete random variable $X$ is

        $$H(X) = \mathbb{E}[-\ln(P(X))] = - \sum_{i=1}^n P(x_i) \log_2 P(x_i)$$

    </dd>
    <dt><a href="https://en.wikipedia.org/wiki/Information_gain_in_decision_trees"><dfn>Information Gain</dfn></a> (<dfn>Kullback–Leibler divergence</dfn>)</dt>
    <dd>

        The expected information gain is the change in entropy $H$ from a prior
        state to a state that takes some information as given:

        $$IG(T,a) = H(T) - H(T|a)$$

    </dd>
    <dt><dfn>Disparitätenkarte</dfn></dt>
    <dd>TODO</dd>
</dl>


### Facial Feature Detection

<dl>
    <dt><dfn>Facial Features</dfn></dt>
    <dd>
        
        <ul>
            <li>Nose tip</li>
            <li>Ears</li>
            <li>Eyes</li>
            <li>Chin</li>
            <li>Lips (corners)</li>
            <li>Eyebrow</li>
        </ul>

    </dd>
    <dt><dfn>Facial Feature Detection</dfn></dt>
    <dd>Is important for alignment
        
        Problems:
        <ul>
            <li>Expression variations</li>
            <li>Scale variations / angle</li>
            <li>Lightning</li>
            <li>Occlusions</li>
        </ul>

    </dd>
</dl>

Statistical Appearence Models

* Represent shape and texture (learned seperately): $x \approx \bar{x} + P_s b_s$,
  wobei $P_s$ eigenvektor of covariance; $b_s = P_s^T (x-\bar{x})$

Fit shape Model

* Active shape model (fitting algorithm)


### Automatic Facial Expression Analysis

<dl>
    <dt><dfn>Allgemein</dfn></dt>
    <dd>
        
        <ul>
            <li>Valence-Arousal Modell von Russel 1980</li>
            <li>Facial Action Cod</li>
        </ul>

    </dd>
    <dt><dfn>FACS</dfn> (<dfn>Facial Action Coding System</dfn>)</dt>
    <dd><ul>
        <li>44 Action Units, welche Menschliche Mimik beschreiben
        <ul>
            <li>30 Gesichtsmuskeln (12 upper, 12 lower)</li>
            <li>Viele Binär, manche mit Intensität</li>
        </ul>
        </li>
        <li>Emotionen sind Kombinationen daraus</li>
    </ul></dd>
    <dt><dfn>Datenbanken</dfn></dt>
    <dd>
        <ul>
            <li>Cohen-Kanade AU-Coded Facial Expression Database</li>
            <li>Emotion Recognition in the Wild (EmotiW)</li>
            <li>AFEW db</li>
        </ul>
    </dd>
    <dt><dfn>Bag of Visual Words</dfn></dt>
    <dd>TODO</dd>
</dl>


### Person detection

<dl>
    <dt>Detection</dt>
    <dd>Detection is classification with localization</dd>
    <dt>Person detection</dt>
    <dd>Person detection (sometimes also pedestrian detection) is the
        task of finding people in an image or video. This is usually done with
        bounding boxes, although tight segmentation methodes exist.<br/>
        <br/>
        One can divide the methods the following way:
        <ul>
            <li>Input: Single image / video</li>
            <li>Detection approach
            <ul>
                <li>Global: Detection of the person as a whole</li>
                <li>Part-based: Detection of arms, legs, head, ...</li>
            </ul>
            </li>
            <li>Model type
            <ul>
                <li>Generative: Models how the data was generated (+ is interpretable - hard to create)</li>
                <li>Discriminative</li>
            </ul>
            </li>
        </ul>

        Shilouette matchin is a discriminative, global, single image approach.
        HOG people detector is a single image approach (TODO: global/part based? generative / discriminative?)
    </dd>
    <dt>L2 hysterese</dt>
    <dd>A variant of the L2 norm which cuts peaks</dd>
    <dt>HOG People Detector</dt>
    <dd>TODO</dd>
    <dt>Shilouette Matching</dt>
    <dd>

        Shilouette matching is practically not used anymore. It uses chemfer
        matching, 2/3 distance. It can be speeded up with a template hierarchy.

    </dd>
</dl>


### Tracking

<dl>
    <dt><dfn>Multi Camera Systems</dfn></dt>
    <dd>Topologies

        <ul>
            <li>Stero-Cameras (cars)</li>
            <li>wide baseline multi-camera system (conference room)</li>
            <li>non-overlapping fields of view (security system)</li>
        </ul>

        Kalibrierung: Intensität und extTODOs z.B. mit Schachbrettmuster bestimmen.
    </dd>
    <dt><dfn>TDOA</dfn> (<dfn>Time Delay of Arrival</dfn>)</dt>
    <dd>In the case of multiple microphones and one audio source, the
        time delay when microphone 1 records the same as microphone 2
        is called TDOA. It can be used to locate the audio source.</dd>
    <dt><dfn>Adaptive Merkmalsgewichtung</dfn></dt>
    <dd>TODO</dd>
</dl>


### Gesten

<dl>
    <dt><dfn>Gesten</dfn></dt>
    <dd>Gesten bedeuten nicht überall das selbe. Ein Nicken bedeutet in
        vielen, aber nicht in allen Kulturen Zustimmung (<a href="http://www.geo.de/geolino/mensch/6703-rtkl-gestik-kultur-mal-anders-gesten-aus-aller-welt">Quelle</a>)</dd>
</dl>


### Action / Activity Recognition

<dl>
    <dt><dfn>Aktion</dfn></dt>
    <dd>Zielgerichtete Interaktion mit der Umgebung; tendentiell mit wenigen
        oder einzelschritten / kurzen Perioden</dd>
    <dt><dfn>Aktivität</dfn></dt>
    <dd>Sind auf viele Einzelaktionen aufgebaut. Zwei verschiedene Aktivitäten
        können aus sehr ähnlichen Bewegungen bestehen, aber deutlich
        unterschiedliche Semantik haben. Ein Beispiel ist das öffnen einen
        Schlosses mit einem Schlüssel und das herausdrehen einer Schraube
        mit einem Schraubenzieher.<br/>
        <br/>
        Es ist ein Klassifikationsproblem mit Bild-Sequenzen.<br/>
        <u>Lösungsansätze:</u> HMMs, RNNs<br/>
        <br/>
        Features: Optical Flow aus Histogram
        <ul>
            <li>Global</li>
            <li>Lokal (Bild aufteilen)</li>
            <li>Implicit Shape Models</li>
            <li>Deskriptoren (SIFT)</li>
        </ul>
        <br/>
        <u>Datasets</u>: UCF Sport; Hollywood 2; Sports-1M</dd>
    <dt><dfn>Zero-Crossing Rate</dfn></dt>
    <dd>Extrem einfaches Audio-Merkmal, welches es erlaubt Auto-Hintergrundrauschen
        von Sprache zu unterscheiden. TODO</dd>
    <dt><dfn>MFCC</dfn> (<a href="https://de.wikipedia.org/wiki/Mel_Frequency_Cepstral_Coefficients"><dfn>Mel Frequency Cepstral Coefficients</dfn></a>)</dt>
    <dd>MFCCs sind gängige Features der Sprachverarbeitung.</dd>
    <dt><dfn>BoW</dfn> (<dfn>Bag of Words</dfn>, <dfn>Bag of Visual Words</dfn>)</dt>
    <dd>Cluster Features for objects by feature similarity; gives histogram representation of object; TODO</dd>
    <dt><dfn>HOG</dfn> (<dfn>Histogram of Gradients</dfn>)</dt>
    <dd>Ein Merkmal für Bilder welches in SIFT eingesetzt wird; TODO</dd>
    <dt><dfn>Optical Flow</dfn> (<dfn>OF</dfn>)</dt>
    <dd>TODO</dd>
    <dt><dfn>HOF</dfn> (<dfn>Histogram of Optical Flow</dfn>)</dt>
    <dd>TODO</dd>
    <dt><dfn>MBH</dfn> (<dfn>Motion Boundary Histogram</dfn>)</dt>
    <dd>TODO</dd>
    <dt><dfn>Dense Trajectories</dfn></dt>
    <dd>TODO</dd>
</dl>


### Wrap-up

* pinhole model, calibrarion (extrinsische / intrinsische Parameter),
  stereo processing (disparitäten)
* Features
    * Color, fg/bg, stereo edges, edge histogram, Gabor- und Haar-Filter (Viola&Jones),
      LBP
    * *Mid-level-Representations**: GMM, Bow, BoW+Spatial layout (body parts)
    * **Dimensionalitätsreduktion**: PCA (Eigenfaces) / LDA (Fischer-Faces)
* Classifiers: Boosting, SVM, CNN, Regression Trees, HMMs, k-NN
* Wie finde ich keypoints?
* Descriptors (SIFT!)
* Space-Time-Features: Space-Time-Interest points; HOG / HOF, Dense Trajectories: Bildfolge, finde Trajektorie
* Implicit Shape Model: Baog-of-Words + Spacial Layout
* Statistische Modelle: Active Shape / Active appearence
* Chromatische Farbräume: Helligkeit rausnormalisieren (2-dimensional: rg, HS, UV)
* Morphable 3D models: PCA - laser scans
* Active Shape: Statistische Modellierung der Form; Textur entlang der shape. Nutzt PCA. Lernt Textur.
* Active Appearence: 2D. Gemeinsames statistisches Modell von Shape & Appearence; Textur innerhalb von Mesh / Punkten. Nutzt auch PCA. Lernt Textur.
* 6 Basic Emotions: FACS, Action Units
* HOG modelliert Textur
* HOF modelliert Bewegung


## Praktische Aufgaben

Es gibt 3 praktische Aufgaben, die 10% der Note ausmachen:

* Haut erkennen
* Detektieren ob auf einem Bild eine Person ist oder nicht
* Erkennen ob auf zwei gegebenen Bildern die selbe Person ist

Die Aufgaben müssen mit C++ gemacht werden. OpenCV kann verwendet werden. Es
ist Beispielcode gegeben.

Man muss in 180s die Modelle trainieren.

Bewertet wird eine Präsentation, die am **16.01.2016** gemacht werden muss. Die
Präsentation soll mindestens 3 Folien, maximal 5 Folien haben. In diesen 5
Folien sollen alle 3 Aufgaben beschrieben werden. Es soll beschrieben werden
wie die Aufgaben gelöst wurden / was geklappt bzw. nicht geklappt hat. Die
Präsentation soll ca. 8 - 10 min pro Team dauern.

Es ist ok private Datensätze zu verwenden um ggf. Hyperparameter zu bestimmen.

Für weitere Fragen steht <a href="https://cvhci.anthropomatik.kit.edu/~manel/">Manuel Martinez</a> zur Verfügung.

## Prüfungsfragen

* Was ist das Hauptproblem der Gesichtserkennung?<br/>
  → Beleuchtung / Pose missmatch (ECCV'94)
* Warum macht man Tracking und nicht einfach Frame-weise Detection?<br/>
  → Tracking ist leichter, weil man Annahmen über die Position machen kann
* Was sind Anwendungen von Action und Activity Recognition?<br/>
  → Fahrer beobachten (ist er aufmerksam? spielt er mit dem Handy?),
     Fußgänger beobachten (will er auf die Straße?),
     Patientenüberwachung, Sicherheitssystem (aggressives Verhalten)
* Was ist Kalibrierung?<br/>
  → TODO
* Woher kommt die Skalen-Invarianz bei SIFT?<br/>
  → Finden einer charakteristischen Skala
* Woher kommt die Rotationsinvarianz bei SIFT?<br/>
  → Kantenhistogramme, Maximalausrichtung (vgl. People detection)
* Was ist der Unterschied zwischen Diskriminativen und Generativen Modellen?<br/>
  → Generative Modelle modellieren explizit die Verteilung (Bayes-Formel)
* Wie funktioniert Histogram Backprojection?<br/>
  → TODO
* Welche Metriken kann man zum Benchmarken benutzen??<br/>
  → ROC, TP-Rate, FP-Rate
* Was macht Computer Vision schwer?<br/>
  → Beleuchtung, Pose, Occlusion (manche Ansätze sind weniger Anfällig, z.B. Part-based approaches)
* Welche Annahmen macht man im Kalman-Filter, die man nicht im Particle-Filter hat?<br/>
  → TODO

## Material und Links

* [Vorlesungswebsite](https://cvhci.anthropomatik.kit.edu/600_1526.php)
* [lecture-demo.ira.uka.de](https://lecture-demo.ira.uka.de/): Interaktive Demos, insbesondere  Rosenblatt-Perceptron, ...
* StackExchange:
    * [Why do CNNs with ReLU learn that well?](http://datascience.stackexchange.com/q/15081/8820)
    * [How is the evaluation setup for YouTube faces of FaceNet?](http://datascience.stackexchange.com/q/15215/8820)
    * [What are interleaved layers of convolutions?](http://datascience.stackexchange.com/q/15214/8820)
* [Video Analysis lecture](https://www.youtube.com/playlist?list=PLuRaSnb3n4kSgSV35vTPDRBH81YgnF3Dd)

<!-- * [Anki-Karteikarten Deck](https://ankiweb.net/shared/info/22317474) -->


## Fazit

Kommt noch.


## Vorlesungs&shy;empfehlungen

Folgende Vorlesungen sind ähnlich:

* [Analysetechniken großer Datenbestände](https://martin-thoma.com/analysetechniken-grosser-datenbestaende/)
* [Informationsfusion](https://martin-thoma.com/informationsfusion/)
* [Machine Learning 1](https://martin-thoma.com/machine-learning-1-course/)
* [Machine Learning 2](https://martin-thoma.com/machine-learning-2-course/)
* [Mustererkennung](https://martin-thoma.com/mustererkennung-klausur/)
* [Neuronale Netze](https://martin-thoma.com/neuronale-netze-vorlesung/)
* [Lokalisierung Mobiler Agenten](https://martin-thoma.com/lma/)
* [Probabilistische Planung](https://martin-thoma.com/probabilistische-planung/)

Weitere:

* Einführung in die Bildfolgenauswertung
* [Content-based Image and Video Retrival](https://cvhci.anthropomatik.kit.edu/600_1482.php) - 3 ECTS, 2 SWS


## Termine und Klausurablauf

Die Veranstaltung wird mündlich geprüft, jedoch sind 10% der Note durch
praktische Aufgaben zu erlangen. Üblicherweise dauert eine Prüfung
etwa 30 min.

Die Anmeldung zur Prüfung erfolgt per Email an das Sekretariat (`corinna.haas-hecker@kit.edu`).

Weitere Prüfungstermine erst nach dem 18. April.
