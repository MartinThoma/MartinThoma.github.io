---
layout: post
title: RL Agents
slug: rl-agents
author: Martin Thoma
date: 2017-11-07 20:00
category: Machine Learning
tags: Reinforcement Learning
featured_image: logos/ml.png
---

<table class="table">
    <tr>
        <th>Name</th>
        <th>Observation Space</th>
        <th>Action Space</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td><abbr title="State-Action-Reward-State-Action">SARSA</abbr></td>
        <td>discrete or continuous</td>
        <td>discrete</td>
        <td><abbr title="Reinforcement learning: An introduction">Sutton and Barto, 2011</abbr>, <a href="https://martin-thoma.com/probabilistische-planung/#sarsa">Blog Post</a></td>
    </tr>
    <tr>
        <td><abbr title="Deep Q-Networks">DQN</abbr></td>
        <td>discrete or continuous</td>
        <td>discrete</td>
        <td><a href="https://arxiv.org/abs/1312.5602"><abbr title="Playing Atari with Deep Reinforcement Learning">[MKSG+13]</abbr></a>, <a href="http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html"><abbr title="Human-level control through deep reinforcement learning">[MKSR+15]</abbr></a>, <a href="https://arxiv.org/abs/1509.06461"><abbr title="Deep Reinforcement Learning with Double Q-learning">[HGS15]</abbr></a></td>
    </tr>
    <tr>
        <td><abbr title="Cross-Entropy Method">CEM</abbr></td>
        <td>discrete or continuous</td>
        <td>discrete</td>
        <td><abbr title="Learning Tetris Using the Noisy Cross-Entropy Method"><a href="http://www.mitpressjournals.org/doi/abs/10.1162/neco.2006.18.12.2936?journalCode=neco&">Szita et al., 2006</a></abbr>, <abbr title="Deep Reinforcement Learning (MLSS lecture notes)">Schulman, 2016</abbr></td>
    </tr>
    <tr>
        <td><abbr title="deep deterministic policy gradient">DDPG</abbr></td>
        <td>discrete or continuous</td>
        <td>continuous</td>
        <td><a href="https://arxiv.org/abs/1509.02971"><abbr title="Continuous control with deep reinforcement learning">[LHPH+15]</abbr></a></td>
    </tr>
    <tr>
        <td><abbr title="normalized adantage functions">NAF</abbr></td>
        <td>discrete or continuous</td>
        <td>continuous</td>
        <td><a href="https://arxiv.org/abs/1603.00748"><abbr title="Continuous Deep Q-Learning with Model-based Acceleration">[GLSL16]</abbr></a></td>
    </tr>
</table>


## Ideas

<dl>
    <dt>DQN</dt>
    <dd>Like Q-Learning, but find the states by a neural network (?)</dd>
    <dt>SARSA</dt>
    <dd>Initialize the Q-Function $Q: \mathcal{X} \times \mathcal{A} \rightarrow \mathbb{R}$</dd>
</dl>


## Comparisons

* [Comparison of DQN performance with linear function approximator](https://www.nature.com/articles/nature14236/tables/4)