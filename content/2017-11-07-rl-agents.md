---
layout: post
title: RL Agents
slug: rl-agents
author: Martin Thoma
date: 2017-11-07 20:00
category: Machine Learning
tags: Reinforcement Learning
featured_image: logos/ml.png
---

<table class="table">
    <tr>
        <th>Name</th>
        <th>Observation Space</th>
        <th>Action Space</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td><abbr title="State-Action-Reward-State-Action">SARSA</abbr></td>
        <td>discrete or continuous</td>
        <td>discrete</td>
        <td><a href="http://incompleteideas.net/sutton/book/the-book-2nd.html"><abbr title="Reinforcement learning: An introduction">Sutton and Barto, 2011</abbr></a>, <a href="https://martin-thoma.com/probabilistische-planung/#sarsa">Blog Post</a></td>
    </tr>
    <tr>
        <td><abbr title="Deep Q-Networks">DQN</abbr></td>
        <td>discrete or continuous</td>
        <td>discrete</td>
        <td><a href="https://arxiv.org/abs/1312.5602"><abbr title="Playing Atari with Deep Reinforcement Learning">[MKSG+13]</abbr></a>, <a href="http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html"><abbr title="Human-level control through deep reinforcement learning">[MKSR+15]</abbr></a>, <a href="https://arxiv.org/abs/1509.06461"><abbr title="Deep Reinforcement Learning with Double Q-learning">[HGS15]</abbr></a></td>
    </tr>
    <tr>
        <td><abbr title="Cross-Entropy Method">CEM</abbr></td>
        <td>discrete or continuous</td>
        <td>discrete</td>
        <td><abbr title="Learning Tetris Using the Noisy Cross-Entropy Method"><a href="http://www.mitpressjournals.org/doi/abs/10.1162/neco.2006.18.12.2936?journalCode=neco&">Szita et al., 2006</a></abbr>, <abbr title="Deep Reinforcement Learning (MLSS lecture notes)">Schulman, 2016</abbr></td>
    </tr>
    <tr>
        <td><abbr title="deep deterministic policy gradient">DDPG</abbr></td>
        <td>discrete or continuous</td>
        <td>continuous</td>
        <td><a href="https://arxiv.org/abs/1509.02971"><abbr title="Continuous control with deep reinforcement learning">[LHPH+15]</abbr></a></td>
    </tr>
    <tr>
        <td><abbr title="normalized adantage functions">NAF</abbr></td>
        <td>discrete or continuous</td>
        <td>continuous</td>
        <td><a href="https://arxiv.org/abs/1603.00748"><abbr title="Continuous Deep Q-Learning with Model-based Acceleration">[GLSL16]</abbr></a></td>
    </tr>
</table>


## Twitter-Style Explanations

<dl>
    <dt>DQN</dt>
    <dd>Like Q-Learning, but represent the current q-function by a neural
        network as function approximator.</dd>
    <dt>SARSA</dt>
    <dd>Initialize the Q-Function $Q: \mathcal{X} \times \mathcal{A} \rightarrow \mathbb{R}$ randomly, adjust it by time. See [Pseudocode](https://martin-thoma.com/probabilistische-planung/#sarsa)</dd>
    <dt>DDPG</dt>
    <dd>?</dd>
    <dt>NAF</dt>
    <dd>?</dd>
</dl>


## Comparisons

The [Code is on Github](https://github.com/MartinThoma/algorithms/blob/master/ML/rl/).

### CartPole-v0

The [`CartPole-v0`](https://gym.openai.com/envs/CartPole-v0/) environemnt has
2 actions: move the paddle to the right or to the left. A reward of +1 is given
is the pole is upright. The episode is finished when the pole is more than
15 degrees from vertical or moves more than 2.4 units from the center.

CartPole-v0 defines "solving" as getting average reward of 195.0 over 100
consecutive trials.

<table class="table">
    <tr>
        <th>Agent</th>
        <th>NN Parameters</th>
        <th>Configuration</th>
        <th>Time</th>
        <th>Test reward</th>
    </tr>
    <tr>
        <td>CEM</td>
        <td>10</td>
        <td>steps=1000</td>
        <td>9s</td>
        <td>mean= 9.49, std= 0.79, min= 8.00, max=11.00</td>
    </tr>
    <tr>
        <td>CEM</td>
        <td>10</td>
        <td>(default, steps=10000)</td>
        <td>39s</td>
        <td>mean=77.14, std=44.18, min=41.00, max=200.00</td>
    </tr>
    <tr>
        <td>CEM</td>
        <td>10</td>
        <td>steps=100000</td>
        <td>284s</td>
        <td>mean=106.21, std=19.99, min=71.00, max=185.00</td>
    </tr>
    <tr>
        <td>CEM</td>
        <td>658</td>
        <td>bigger NN</td>
        <td>60s</td>
        <td>mean=42.61, std=36.36, min=10.00, max=200.00</td>
    </tr>
    <tr>
        <td>CEM</td>
        <td>658</td>
        <td>steps=10000, bigger NN</td>
        <td>60s</td>
        <td>mean=200.00, std= 0.00, min=200.00, max=200.00</td>
    </tr>
    <tr>
        <td>DQN</td>
        <td></td>
        <td>(default)</td>
        <td>40s</td>
        <td>mean=42.61, std=36.36, min=10.00, max=200.00</td>
    </tr>
</table>

The bigger NN is

```
model = Sequential()
model.add(Flatten(input_shape=input_shape))
model.add(Dense(16))
model.add(Activation('relu'))
model.add(Dense(16))
model.add(Activation('relu'))
model.add(Dense(16))
model.add(Activation('relu'))
model.add(Dense(nb_actions))
model.add(Activation('softmax'))
```

Ok, so the bigger network is important. Also, 1000 training steps are not
enough, but 10000 are. Let's see if we can reduce the episode memory. The
episode memory is what is used for training.

<table class="table">
    <tr>
        <th>Agent</th>
        <th>EpisodeParameterMemory</th>
        <th>Time</th>
        <th>Test reward</th>
    </tr>
    <tr>
        <td>CEM</td>
        <td>1000</td>
        <td>60s</td>
        <td>mean=200.00, std= 0.00, min=200.00, max=200.00</td>
    </tr>
    <tr>
        <td>CEM</td>
        <td>500</td>
        <td>65s</td>
        <td>&nbsp;mean=200.00, std= 0.00, min=200.00, max=200.00</td>
    </tr>
    <tr>
        <td>CEM</td>
        <td>450</td>
        <td>68s</td>
        <td>mean=200.00, std= 0.00, min=200.00, max=200.00</td>
    </tr>
    <tr>
        <td>CEM</td>
        <td>400</td>
        <td>59s</td>
        <td>mean=200.00, std= 0.00, min=200.00, max=200.00</td>
    </tr>
    <tr>
        <td>CEM</td>
        <td>300</td>
        <td>51s</td>
        <td>mean=103.73, std=37.07, min=55.00, max=200.00</td>
    </tr>
    <tr>
        <td>CEM</td>
        <td>200</td>
        <td>38s</td>
        <td>mean=34.22, std= 6.75, min=17.00, max=52.00</td>
    </tr>
    <tr>
        <td>CEM</td>
        <td>100</td>
        <td>56s</td>
        <td>mean=82.77, std=25.05, min=32.00, max=172.00</td>
    </tr>
</table>


### CartPole-v1

<table class="table">
    <tr>
        <th>Agent</th>
        <th>Config</th>
        <th>Time</th>
        <th>Test reward</th>
    </tr>
    <tr>
        <td>CEM</td>
        <td>(default)</td>
        <td>100s</td>
        <td>mean=461.70, std=66.26, min=264.00, max=500.00</td>
    </tr>
    <tr>
        <td>DQN</td>
        <td>(default)</td>
        <td>30s</td>
        <td>mean=10.62, std= 4.31, min= 8.00, max=30.00</td>
    </tr>
</table>

### Other

* [Comparison of DQN performance with linear function approximator](https://www.nature.com/articles/nature14236/tables/4)