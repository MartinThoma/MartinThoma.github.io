---
layout: post
title: Gunicorn
slug: gunicorn
author: Martin Thoma
date: 2019-05-24 20:00
category: Code
tags: gunicorn, Flask
featured_image: logos/flask.png
---

<table>
    <tr>
        <th></th>
        <th rowspan="2">sync</th>
        <th colspan="2">Async Workers</th>
        <th rowspan="2">tornado</th>
        <th colspan="2">AsyncIO Workers</th>
    </tr>
    <tr>
        <td>worker</td>
        <td>gevent</td>
        <td>eventlet</td>
        <td>gaiohttp</td>
        <td>gthread</td>
    </tr>
    <tr>
        <td>core characteristic</td>
        <td>* Per request a process<br/>* Blocks other requests until it is finished</td>
        <td>*&nbsp;based on the Greenlet library<br/>* Program-level threads</td>
        <td>*&nbsp;based on the Greenlet library<br/>* Program-level threads</td>
        <td>*provides async I/O non-blocking design model<br/>*&nbsp;ideal for handling long requests</td>
        <td></td>
        <td>* Per request a thread</td>
    </tr>
    <tr>
        <td>workload</td>
        <td>* No long I/O<br/>* No heavy CPU<br/>* No requests to external pages</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>URL</td>
        <td></td>
        <td>http://www.gevent.org/</td>
        <td>https://eventlet.net/</td>
        <td></td>
        <td>https://aiohttp.readthedocs.io/en/stable/</td>
        <td></td>
    </tr>
</table>


## Performance

Performance can be measured in at least three ways:

* throughput (requests per second),
* latency (minimum),
* latency (average)

I use locust with a hatch rate so that in 10s all users are hatched to get some
numbers. I use the following `locustfile.py`:

```python
from locust import HttpLocust, TaskSet, task


class WebsiteTasks(TaskSet):
    @task
    def index(self):
        self.client.get("/")


class WebsiteUser(HttpLocust):
    task_set = WebsiteTasks
    min_wait = 5000
    max_wait = 15000
```

I had to execute `ulimit -S -n 10240` in order to test the 10,000 users. Otherwise
I got

```text
[2019-05-26 12:25:03,499] pc07/ERROR/stderr: OSError: [Errno 16] Device or resource busy
[2019-05-26 12:25:03,499] pc07/ERROR/stderr:
[2019-05-26 12:25:03,499] pc07/ERROR/stderr: 2019-05-26T10:25:03Z
[2019-05-26 12:25:03,499] pc07/ERROR/stderr:
[2019-05-26 12:25:03,499] pc07/ERROR/stderr: (<ThreadPool at 0x7fc2e275d4a8 0/10/10 hub=<Hub at 0x7fc2e7c84780 thread_ident=0x140475181328192>>, <built-in function getaddrinfo>) failed with OSError
```


### Constant Response

The following `app.py` is probably the simplest one possible.

```python
from flask import Flask

app = Flask(__name__)


@app.route("/")
def hello():
    return "hello world!"
```

Let's first have a look at the median response time by the number of users:

<table class="table">
    <tr>
        <th>Users</th>
        <th>Development Server</th>
        <th>gevent (1 worker, 1000 connections)</th>
        <th>gevent (4 workers, 1000 connections)</th>
        <th>gevent (12 workers, 1000 connections)</th>
        <th>sync (4 workers)</th>
        <th>gthread (4 workers)</th>
    </tr>
    <tr>
        <td>10</td>
        <td>8ms - 9ms; 1 Req/ s</td>
        <td>8ms - 9ms; 1 Req/ s</td>
        <td>8ms - 11ms; 1 Req/ s</td>
        <td>6ms - 10ms; 1 Req/ s</td>
        <td>7ms - 9ms; 1 Req/s</td>
        <td>6ms - 12ms; 1 Req/s</td>
    </tr>
    <tr>
        <td>100</td>
        <td>10ms; 10 Req/s</td>
        <td>8ms - 9ms; 10 Req/s</td>
        <td>8ms - 9ms; 10 Req/s</td>
        <td>8ms - 9ms; 10 Req/s</td>
        <td>7ms - 9ms; 10 Req/s</td>
        <td>8ms - 12ms; 10 Req/s</td>
    </tr>
    <tr>
        <td>1000</td>
        <td>12ms - 30ms; 100 Req/s</td>
        <td>8ms - 12ms; 100 Req/s</td>
        <td>7ms - 10ms; 100 Req/s</td>
        <td>5ms - 12ms; 100 Req/s</td>
        <td>6ms - 8ms; 100 Req/s</td>
        <td>8ms - 11ms; 100 Req/s</td>
    </tr>
    <tr>
        <td>10&thinsp;000</td>
        <td>150ms - 34000ms; 170 - 270 Req/s</td>
        <td>8100ms - 41000ms; 220 - 440 Req/s</td>
        <td>340ms - 11000ms; 200 - 625 Req/s</td>
        <td>150ms - 96000ms; 130 - 410 Req/s</td>
        <td>81ms - 101000ms; 220 - 750 Req/s</td>
        <td>350ms - 93000ms; 270 - 533 Req/s</td>
    </tr>
</table>

You can clearly see that there is no real difference up to 100 users. At 1000
users, the development server gets slow. gevent does not care about it. At
10,000 users, gevent gets slow. But the real difference was the 95-percentile
response time. This varies over time. A lot.


### I/O Bound

Let's say our workload was I/O bound. Maybe we need to request another API
before we can answer. To simulate this, I let the request sleep for 1s before
it answers:

```python
import time

from flask import Flask

app = Flask(__name__)


@app.route("/")
def hello():
    time.sleep(1)
    return "hello world!"
```

<table>
    <tr>
        <th></th>
        <th>Development Server</th>
        <th>gevent (1 worker, 1000 connections)</th>
        <th>gevent (4 workers, 1000 connections)</th>
        <th>gevent (12 workers, 1000 connections)</th>
        <th>sync (4 workers)</th>
        <th>gthread (4 workers)</th>
    </tr>
    <tr>
        <td>1000</td>
        <td>1000ms; 1000-1200ms</td>
        <td>1000ms; 1000-1100ms</td>
        <td>1000ms; 1000ms</td>
        <td>1000ms; 1s-1.1ms</td>
        <td>32s-37s;35s-97s</td>
        <td>Fails</td>
    </tr>
    <tr>
        <td>10,000</td>
        <td>3.2s-8.4ms;5.2ms-76s</td>
        <td>2.4s-8.5s; 28s - 132s</td>
        <td>2.8s-9.7s; 16s-290s</td>
        <td>1.8s - 11s; 6.7s - 150s</td>
        <td>29s - 131s;45s-183s</td>
        <td>Fails</td>
    </tr>
</table>


### CPU Bound

Let's say our workload was CPU bound:

```python
import time

from flask import Flask

app = Flask(__name__)


@app.route("/")
def hello():
    time.sleep(1)
    return "hello world!"
```


## Alternatives

* [uWSGI](https://en.wikipedia.org/wiki/UWSGI) ([docs](https://uwsgi-docs.readthedocs.io/en/latest/) )is both a protocol and an application server; the application server can serve uWSGI, FastCGI, and HTTP protocols. Seems to be used together with gevent.
* [Tornado](https://en.wikipedia.org/wiki/Tornado_(web_server)): a scalable, non-blocking web server and web application framework

While I searched, I also found:

* TWISTED WEB
* CHERRYPY
* WAITRESS
* CHAUSSETTE
* [meinheld](https://github.com/mopemope/meinheld): a high performance asynchronous WSGI Web Server (based on picoev)


## Notes

* `gevent_pywsgi` and `gevent_wsgi` are the same.[^1]
* `gevent` uses the gunicorn event parser, while `gevent_wsgi` uses the event
  parser within itself.[^1]
* If you need to handle a high volume of concurrent requests and your
  application performs a lot of waiting on I/O (database, streaming responses,
  upstream requests, etc) then gevent can be a good choice.[^1]
* I didn't include Sanic, because it is less tested then Flask.[^2]

## See also

* 2018-01-30: [Brief introduction about the types of worker in gunicorn and respective suitable scenario](https://medium.com/@genchilu/brief-introduction-about-the-types-of-worker-in-gunicorn-and-respective-suitable-scenario-67b0c0e7bd62)
* 2015-01-20: [Gunicorn Worker Types](https://www.spirulasystems.com/blog/2015/01/20/gunicorn-worker-types/)
* 2017-01-17: Reddit-comment by [desmoulinmichel](https://www.reddit.com/r/Python/comments/5og4hl/is_there_any_reason_to_use_flask_over_sanic/dcjbqml/)
* 2012-09-12: [Benchmark uWSGI vs gunicorn for async workers](https://ivan-site.com/2012/09/benchmark-uwsgi-vs-gunicorn-for-async-workers/) by Ivan Dyedov

## Sources

[^1]: Randall Leeds: [Github, 2019-03-04](https://github.com/benoitc/gunicorn/issues/305#issuecomment-469345296)
