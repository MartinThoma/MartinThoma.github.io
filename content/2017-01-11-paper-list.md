---
layout: post
title: Paper List
slug: paper-list
author: Martin Thoma
date: 2017-01-11 20:00
category: Science
tags: Science, Papers, Reading, Academia, Computer Science
featured_image: logos/science.png
---
The following includes my reading list and a list of papers organized in tracks
which I can recommend to read. Most (all?) of them are about machine learning
and neural networks.


## Reading List

I am aware of the following papers and I want to read them ... when I have time:

<ol>
    <li><a href="https://arxiv.org/abs/1610.09716v1">Doubly Convolutional Neural Networks</a></li>
    <li><a href="https://arxiv.org/abs/1602.03616">Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks</a></li>
    <li><a href="http://ieeexplore.ieee.org/document/7404017/?arnumber=7404017">Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning</a></li>
    <li><a href="https://arxiv.org/abs/1606.02492">Convolutional Neural Fabrics</a></li>
    <li><a href="http://www.mitpressjournals.org/doi/abs/10.1162/106365602320169811">Evolving Neural Networks through Augmenting Topologies</a> and <a href="http://ieeexplore.ieee.org/document/6792316/">A Hypercube-Based Encoding for Evolving Large-Scale Neural Networks</a></li>
</ol>


## Best of

The following is a list of papers, organized by the year I read (or written)
them. Not when they were published.

### 2016

<ol>
    <li>Lipton, Z.C., 2016. <a href="http://zacklipton.com/media/papers/mythos_model_interpretability_lipton2016.pdf">The Mythos of Model Interpretability</a>. IEEE Spectrum. (<a href="http://www.shortscience.org/paper?bibtexKey=journals/corr/1606.03490">summary</a>)</li>
    <li>Zhang, C., Bengio, S., Hardt, M., Recht, B. and Vinyals, O., 2016. <a href="https://arxiv.org/abs/1611.03530">Understanding deep learning requires rethinking generalization</a>. arXiv preprint arXiv:1611.03530. (<a href="http://www.shortscience.org/paper?bibtexKey=journals%2Fcorr%2F1611.03530">summary</a>)</li>
    <li><a href="https://papers.nips.cc/paper/6112-deep-learning-without-poor-local-minima">Deep Learning without Poor Local Minima</a> and <a href="https://papers.nips.cc/paper/6048-matrix-completion-has-no-spurious-local-minimum.pdf">Matrix Completion has No Spurious Local Minimum</a></li>
</ol>



## Tracks

### Weight Initialization

<ol>
    <li>X. Glorot and Y. Bengio, “<a href="http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf?hc_location=ufi">Understanding the difficulty of training deep feedforward neural networks</a>.” in Aistats, vol. 9, 2010, pp. 249–256. (<a href="http://www.shortscience.org/paper?bibtexKey=journals/jmlr/GlorotB10">summary</a>)</li>
    <li>A. M. Saxe, J. L. McClelland, and S. Ganguli, “<a href="https://arxiv.org/abs/1312.6120">Exact solutions to
the nonlinear dynamics of learning in deep linear neural networks</a>,”
arXiv preprint arXiv:1312.6120, Dec. 2013. (<a href="http://www.shortscience.org/paper?bibtexKey=journals/corr/1312.6120">summary</a>)</li>
    <li>K. He, X. Zhang, S. Ren, and J. Sun, “<a href="https://arxiv.org/abs/1502.01852">Delving deep into rectifiers: Surpassing human-level performance
on imagenet classification</a>,” in Proceedings of the IEEE International
Conference on Computer Vision, Feb. 2015, pp. 1026–1034. (<a href="http://www.shortscience.org/paper?bibtexKey=journals/corr/1502.01852">summary</a>)</li>
    <li>D. Mishkin and J. Matas, “<a href="https://arxiv.org/abs/1511.06422">All you need is a good init</a>,” arXiv
preprint arXiv:1511.06422,
Nov. 2015. (<a href="http://www.shortscience.org/paper?bibtexKey=journals/corr/MishkinM15">summary</a>)</li>
</ol>


## Ideas

* Establishing Human-Level scores for Benchmarks
    * User Interfaces: What are good examples?
    * Herarchical Classification
