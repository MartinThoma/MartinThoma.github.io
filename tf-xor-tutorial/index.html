<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Martin Thoma" />
        <meta name="copyright" content="Martin Thoma" />
        <link title = "Martin Thoma"
              type  = "application/opensearchdescription+xml"
              rel   = "search"
              href  = "../opensearch.xml">

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="Machine Learning, Python, Tensorflow, sklearn, Machine Learning, " />

<meta property="og:title" content="XOR tutorial with TensorFlow "/>
<meta property="og:url" content="../tf-xor-tutorial/" />
<meta property="og:description" content="The XOR-Problem is a classification problem, where you only have four data points with two features. The training set and the test set are exactly the same in this problem. So the interesting question is only if the model is able to find a decision boundary which classifies all four …" />
<meta property="og:site_name" content="Martin Thoma" />
<meta property="og:article:author" content="Martin Thoma" />
<meta property="og:article:published_time" content="2016-07-19T14:00:00+02:00" />
<meta name="twitter:title" content="XOR tutorial with TensorFlow ">
<meta name="twitter:description" content="The XOR-Problem is a classification problem, where you only have four data points with two features. The training set and the test set are exactly the same in this problem. So the interesting question is only if the model is able to find a decision boundary which classifies all four …">
<meta property="og:image" content="logos/tensor-flow.png" />
<meta name="twitter:image" content="logos/tensor-flow.png" >

        <title>XOR tutorial with TensorFlow  · Martin Thoma
</title>
        <!-- Latest compiled and minified CSS -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="../theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../static/custom.css" media="screen">

        <!-- MathJax -->
<script type="text/x-mathjax-config">
<!--
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [ ['$$', '$$'], ['\\[','\\]']],
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    processEscapes: true
  }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
// -->
</script>
<script type="text/javascript" async
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

        <link href="https://martin-thoma.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Martin Thoma - Full Atom Feed" />
        <link href="https://martin-thoma.com/feeds/index.xml" type="application/rss+xml" rel="alternate" title="Martin Thoma - Full RSS Feed" />
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top navbar-default">
            <div class="container">
                <div class="container-fluid">
                    <div class="collapse navbar-collapse">
                        <ul class="nav pull-left top-menu navbar-nav">
                            <li><a href=".." style="font-family: 'Monaco', 'Inconsolata', 'Andale Mono', 'Lucida Console', 'Bitstream Vera Sans Mono', 'Courier New', Courier, Monospace;
                        font-size: 20px;" class="navbar-brand">Martin Thoma</a>
                            </li>
                        </ul>
                        <ul class="nav pull-right top-menu navbar-nav">
                            <li ><a href="..">Home</a></li>
                            <li ><a href="../categories.html">Categories</a></li>
                            <li ><a href="../tags.html">Tags</a></li>
                            <li ><a href="../archives.html">Archives</a></li>
                            <li><a href="../support-me/">Support me</a></li>
                            <li><form class="navbar-form" action="../search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="search" class="search-query form-control" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row">
                <div class="col-sm-1 col-md-1"></div>
                <div class="col-sm-10 col-md-10">
<article>
<div class="row">
    <header class="page-header col-sm-10 col-md-10 col-md-offset-2">
    <h1><a href="../tf-xor-tutorial/"> XOR tutorial with TensorFlow  </a></h1>
    </header>
</div>

<div class="row">
    <div class="col-sm-2 col-md-2 table-of-content">
        <nav>
        <h4>Contents</h4>
        <div id="toc"><ul><li><a class="toc-href" href="#neural-network-basics" title="Neural Network basics">Neural Network basics</a></li><li><a class="toc-href" href="#targets-and-error-function" title="Targets and Error function">Targets and Error function</a></li><li><a class="toc-href" href="#install-tensorflow" title="Install Tensorflow">Install Tensorflow</a></li><li><a class="toc-href" href="#tensorflow-basics" title="Tensorflow basics">Tensorflow basics</a></li><li><a class="toc-href" href="#footnotes" title="Footnotes">Footnotes</a></li></ul></div>
        </nav>
    </div>
    <div class="col-sm-8 col-md-8 article-content" id="contentAfterTitle">

            
            <p>The XOR-Problem is a classification problem, where you only have four data
points with two features. The training set and the test set are exactly
the same in this problem. So the interesting question is only if the model is
able to find a decision boundary which classifies all four points correctly.</p>
<figure class="wp-caption aligncenter img-thumbnail">
<img alt="The XOR classification problem. 4 datapoints and two classes. All datapoints have 2 features." src="../images/2016/07/xor-problem.png"/>
<figcaption class="text-center">The XOR classification problem. 4 datapoints and two classes. All datapoints have 2 features.</figcaption>
</figure>
<h2 id="neural-network-basics">Neural Network basics</h2>
<p>I think of neural networks as a construction kit for functions. The basic building block - called a "neuron" - is usually visualized like this:</p>
<p><a href="http://i.stack.imgur.com/YD9IS.png"><img alt="enter image description here" src="http://i.stack.imgur.com/YD9IS.png"/></a></p>
<p>It gets a variable number of inputx <span class="math">\(x_0, x_1, \dots, x_n\)</span>, they get multiplied with weights <span class="math">\(w_0, w_1, \dots, w_n\)</span>, summed and a function <span class="math">\(\varphi\)</span> is applied to it. The weights is what you want to "fine tune" to make it actually work. When you have more of those neurons, you visualize it like this:</p>
<p><a href="http://i.stack.imgur.com/awAz8.png"><img alt="enter image description here" src="http://i.stack.imgur.com/awAz8.png"/></a></p>
<p>In this example, it is only one output and 5 inputs, but it could be any number. The number of inputs and outputs is usually defined by your problem, the intermediate is to allow it to fit more exact to what you need (which comes with some other implications).</p>
<p>Now you have some structure of the function set, you need to find weights which work. This is where backpropagation<sup id="fnref-3"><a class="footnote-ref" href="#fn-3">3</a></sup> comes into play. The idea is the following: You took functions (<span class="math">\(\varphi\)</span>) which were differentiable and combined them in a way which makes sure the complete function is differentiable. Then you apply an error function (e.g. the euclidean distance of the output to the desired output, Cross-Entropy) which is also differentiable. Meaning you have a completely differentiable function. Now you see the weights as variables and the data as given parameters of a HUGE function. You can differentiate (calculate the gradient) and go from your random weights "a step" in the direction where the error gets lower. This adjusts your weights. Then you repeat this steepest descent step and hopefully end up some time with a good function.</p>
<p>For two weights, this awesome image by Alec Radford visualizes how different algorithms based on gradient descent find a minimum (<a href="http://imgur.com/a/Hqolp">Source</a> with even more of those):</p>
<p><a href="http://i.stack.imgur.com/ocZHU.gif"><img alt="enter image description here" src="http://i.stack.imgur.com/ocZHU.gif"/></a></p>
<p>So think of back propagation as a shortsighted hiker trying to find the lowest point on the error surface: He only sees what is directly in front of him. As he makes progress, he adjusts the direction in which he goes.</p>
<h2 id="targets-and-error-function">Targets and Error function</h2>
<p>First of all, you should think about how your targets look like. For
classification problems, one usually takes as many output neurons as one has
classes. Then the softmax function is applied.<sup id="fnref-1"><a class="footnote-ref" href="#fn-1">1</a></sup> The softmax function makes sure that the output of every single neuron is in <span class="math">\([0, 1]\)</span> and the sum of all outputs is exactly <span class="math">\(1\)</span>. This means the output can be interpreted as a probability distribution over all classes.</p>
<p>Now you have to adjust your targets. It is likely that you only have a list of labels, where the <span class="math">\(i\)</span>-th element in the list is the label for the <span class="math">\(i\)</span>-th element in your feature list <span class="math">\(X\)</span> (or the <span class="math">\(i\)</span>-th row in your feature matrix <span class="math">\(X\)</span>). But the tools need a target value which fits to the error function. The usual error function for classification problems is cross entropy (CE). When you have a list of <span class="math">\(n\)</span> features <span class="math">\(x\)</span>, the target <span class="math">\(t\)</span> and a classifier <span class="math">\(clf\)</span>, then you calculate the cross entropy loss for this single sample by:</p>
<div class="math">$$CE(x, t) = - \sum_{i=1}^n \left (t^{(i)} \log \left ({clf(x)}^{(i)} \right ) \right)$$</div>
<p>Now we need a target value for each single neuron for every sample <span class="math">\(x\)</span>. We get those by so called <em>one hot encoding</em>: The <span class="math">\(k\)</span> classes all have their own neuron. If a sample <span class="math">\(x\)</span> is of class <span class="math">\(i\)</span>, then the <span class="math">\(i\)</span>-th neuron should give <span class="math">\(1\)</span> and all others should give <span class="math">\(0\)</span>.<sup id="fnref-2"><a class="footnote-ref" href="#fn-2">2</a></sup></p>
<p><code>sklearn</code> provides a very useful <code>OneHotEncoder</code> class. You first have to fit
it on your labels (e.g. just give it all of them). In the next step you can
transform a list of labels to an array of one-hot encoded targets:</p>
<div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>

<span class="sd">"""Mini-demo how the one hot encoder works."""</span>

<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c1"># The most intuitive way to label a dataset "X"</span>
<span class="c1"># (list of features, where X[i] are the features for a datapoint i)</span>
<span class="c1"># is to have a flat list "labels" where labels[i] is the label for datapoint i.</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># The OneHotEncoder transforms those labels to something our models can</span>
<span class="c1"># work with</span>
<span class="n">enc</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">trans_for_ohe</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
    <span class="sd">"""Transform a flat list of labels to what one hot encoder needs."""</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">labels_r</span> <span class="o">=</span> <span class="n">trans_for_ohe</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
<span class="c1"># The encoder has to know how many classes there are and what their names are.</span>
<span class="n">enc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">labels_r</span><span class="p">)</span>

<span class="c1"># Now you can transform</span>
<span class="k">print</span><span class="p">(</span><span class="n">enc</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">trans_for_ohe</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
</pre></div>
<h2 id="install-tensorflow">Install Tensorflow</h2>
<p>The documentation about the installation makes a VERY good impression. Better
than anything I can write in a few minutes, so ... <a href="http://tensorflow.org/get_started/os_setup.md">RTFM</a>
 😜</p>
<p>For Linux systems with CUDA and without root privileges, you can install it
with:</p>
<div class="highlight"><pre><span></span>$ pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl --user
</pre></div>
<p>But remember you have to set the environment variable <code>LD_LIBRARY_PATH</code> and
<code>CUDA_HOME</code>. For many configurations, adding the following lines to your
<code>.bashrc</code> will work:</p>
<div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="s2">"</span><span class="nv">$LD_LIBRARY_PATH</span><span class="s2">:/usr/local/cuda/lib64"</span>
<span class="nb">export</span> <span class="nv">CUDA_HOME</span><span class="o">=</span>/usr/local/cuda
</pre></div>
<p>I currently (19.07.2016) to use Tensorflow rc0.7 (<a href="https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html">installation instructions</a>) with CUDA 7.5 (<a href="http://askubuntu.com/a/799185/10425">installation instructions</a>). I had a couple
of problems with other versions (e.g. <a href="https://github.com/tensorflow/tensorflow/issues/3342">#3342</a>, <a href="https://github.com/tensorflow/tensorflow/issues/2810">#2810</a>, <a href="https://github.com/tensorflow/tensorflow/issues/2034">#2034</a>, but that might only have been bad luck. Who knows.).</p>
<h2 id="tensorflow-basics">Tensorflow basics</h2>
<p>Tensorflow helps you to define the neural network in a symbolic way. This means you do not explicitly tell the computer what to compute to inference with the neural network, but you tell it how the data flow works. This symbolic representation of the computation can then be used to automatically caluclate the derivates. This is awesome! So you don't have to make this your own. But keep it in mind that it is only symbolic as this makes a few things more complicated and different from what you might be used to.</p>
<p>Tensorflow has <em>placeholders</em> and <em>variables</em>. Placeholders are the things in which
you later put your input. This is your features and your targets, but might be
also include more. Variables are the things the optimizer calculates.</p>
<p>Now you should be able to understand the following code which solves the XOR
problem. It defines a neural network with two input neurons, 2&nbsp;neurons in
a first hidden layer and 2&nbsp;output neurons. All neurons have biases.</p>
<div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>

<span class="sd">"""</span>
<span class="sd">Solve the XOR problem with Tensorflow.</span>

<span class="sd">The XOR problem is a two-class classification problem. You only have four</span>
<span class="sd">datapoints, all of which are given during training time. Each datapoint has</span>
<span class="sd">two features:</span>

<span class="sd">      x    o</span>

<span class="sd">      o    x</span>

<span class="sd">As you can see, the classifier has to learn a non-linear transformation of</span>
<span class="sd">the features to find a propper decision boundary.</span>
<span class="sd">"""</span>

<span class="n">__author__</span> <span class="o">=</span> <span class="s2">"Martin Thoma"</span>
<span class="n">__email__</span> <span class="o">=</span> <span class="s2">"info@martin-thoma.de"</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>


<span class="k">def</span> <span class="nf">trans_for_ohe</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
    <span class="sd">"""Transform a flat list of labels to what one hot encoder needs."""</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">analyze_classifier</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">XOR_X</span><span class="p">,</span> <span class="n">XOR_T</span><span class="p">):</span>
    <span class="sd">"""Visualize the classification."""</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">Epoch </span><span class="si">%i</span><span class="s1">'</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'Hypothesis </span><span class="si">%s</span><span class="s1">'</span> <span class="o">%</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">hypothesis</span><span class="p">,</span>
                                     <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">input_</span><span class="p">:</span> <span class="n">XOR_X</span><span class="p">,</span>
                                                <span class="n">target</span><span class="p">:</span> <span class="n">XOR_T</span><span class="p">}))</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'w1=</span><span class="si">%s</span><span class="s1">'</span> <span class="o">%</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">w1</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'b1=</span><span class="si">%s</span><span class="s1">'</span> <span class="o">%</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">b1</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'w2=</span><span class="si">%s</span><span class="s1">'</span> <span class="o">%</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">w2</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'b2=</span><span class="si">%s</span><span class="s1">'</span> <span class="o">%</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">b2</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'cost (ce)=</span><span class="si">%s</span><span class="s1">'</span> <span class="o">%</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">,</span>
                                    <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">input_</span><span class="p">:</span> <span class="n">XOR_X</span><span class="p">,</span>
                                               <span class="n">target</span><span class="p">:</span> <span class="n">XOR_T</span><span class="p">}))</span>
    <span class="c1"># Visualize classification boundary</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">pred_classes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">ys</span><span class="p">:</span>
            <span class="n">pred_class</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">hypothesis</span><span class="p">,</span>
                                  <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">input_</span><span class="p">:</span> <span class="p">[[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]]})</span>
            <span class="n">pred_classes</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pred_class</span><span class="o">.</span><span class="n">argmax</span><span class="p">()))</span>
    <span class="n">xs_p</span><span class="p">,</span> <span class="n">ys_p</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="n">xs_n</span><span class="p">,</span> <span class="n">ys_n</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">pred_classes</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">c</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">xs_n</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">ys_n</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">xs_p</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">ys_p</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs_p</span><span class="p">,</span> <span class="n">ys_p</span><span class="p">,</span> <span class="s1">'ro'</span><span class="p">,</span> <span class="n">xs_n</span><span class="p">,</span> <span class="n">ys_n</span><span class="p">,</span> <span class="s1">'bo'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="c1"># The training data</span>
<span class="n">XOR_X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>  <span class="c1"># Features</span>
<span class="n">XOR_Y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># Class labels</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">XOR_X</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">XOR_Y</span><span class="p">)</span>  <span class="c1"># sanity check</span>

<span class="c1"># Transform labels to targets</span>
<span class="n">enc</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">()</span>
<span class="n">enc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trans_for_ohe</span><span class="p">(</span><span class="n">XOR_Y</span><span class="p">))</span>
<span class="n">XOR_T</span> <span class="o">=</span> <span class="n">enc</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">trans_for_ohe</span><span class="p">(</span><span class="n">XOR_Y</span><span class="p">))</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>

<span class="c1"># The network</span>
<span class="n">nb_classes</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">input_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">XOR_X</span><span class="p">[</span><span class="mi">0</span><span class="p">])],</span>
                        <span class="n">name</span><span class="o">=</span><span class="s2">"input"</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="n">nb_classes</span><span class="p">],</span>
                        <span class="n">name</span><span class="o">=</span><span class="s2">"output"</span><span class="p">)</span>
<span class="n">nb_hidden_nodes</span> <span class="o">=</span> <span class="mi">2</span>
<span class="c1"># enc = tf.one_hot([0, 1], 2)</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="n">nb_hidden_nodes</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                 <span class="n">name</span><span class="o">=</span><span class="s2">"Weights1"</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="n">nb_hidden_nodes</span><span class="p">,</span> <span class="n">nb_classes</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                                   <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                 <span class="n">name</span><span class="o">=</span><span class="s2">"Weights2"</span><span class="p">)</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">nb_hidden_nodes</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Biases1"</span><span class="p">)</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">nb_classes</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Biases2"</span><span class="p">)</span>
<span class="n">activation2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="n">w1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
<span class="n">hypothesis</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">activation2</span><span class="p">,</span> <span class="n">w2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span>
<span class="n">cross_entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">target</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">hypothesis</span><span class="p">))</span>
<span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">)</span>

<span class="c1"># Start training</span>
<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">initialize_all_variables</span><span class="p">()</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20001</span><span class="p">):</span>
        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_step</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">input_</span><span class="p">:</span> <span class="n">XOR_X</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">XOR_T</span><span class="p">})</span>

        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">analyze_classifier</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">XOR_X</span><span class="p">,</span> <span class="n">XOR_T</span><span class="p">)</span>
</pre></div>
<p>The output is:</p>
<div class="highlight"><pre><span></span>Epoch 0
Hypothesis [[ 0.48712057  0.51287943]
 [ 0.3380821   0.66191792]
 [ 0.65063184  0.34936813]
 [ 0.50317246  0.4968276 ]]
w1=[[-0.79593647  0.93947881]
 [ 0.68854761 -0.89423609]]
b1=[-0.00733338  0.00893857]
w2=[[-0.79084051  0.93289936]
 [ 0.69278169 -0.8986907 ]]
b2=[ 0.00394399 -0.00394398]
cost (ce)=2.87031

Epoch 10000
Hypothesis [[ 0.99773693  0.00226305]
 [ 0.00290442  0.99709558]
 [ 0.00295531  0.99704474]
 [ 0.99804318  0.00195681]]
w1=[[-6.62694693  7.5230279 ]
 [ 6.91208076 -7.39292192]]
b1=[ 3.32245016  3.76204181]
w2=[[ 6.63465023 -6.49259233]
 [ 6.40471792 -6.61061859]]
b2=[-9.65064621  9.65065193]
cost (ce)=0.0100926

Epoch 20000
Hypothesis [[  9.98954773e-01   1.04520109e-03]
 [  1.35455502e-03   9.98645484e-01]
 [  1.37042452e-03   9.98629570e-01]
 [  9.99092221e-01   9.07782756e-04]]
w1=[[-7.04857063  7.84673214]
 [ 7.33061123 -7.68837786]]
b1=[ 3.53246188  3.89587545]
w2=[[ 7.35948515 -7.21742725]
 [ 7.14059925 -7.34649038]]
b2=[-10.74944687  10.74944115]
cost (ce)=0.00468077
</pre></div>
<p>The resulting decision boundary looks like this:</p>
<figure class="wp-caption aligncenter img-thumbnail">
<img alt="Decision boundary of the trained network." src="../images/2016/07/xor-classification.png"/>
<figcaption class="text-center">Decision boundary of the trained network.</figcaption>
</figure>
<p>I recommend reading the <a href="http://download.tensorflow.org/paper/whitepaper2015.pdf">Tensorflow Whitepaper</a> if you want to understand Tensorflow better.</p>
<h2 id="footnotes">Footnotes</h2>
<div class="footnote">
<hr>
<ol>
<li id="fn-1">
<p>Softmax is similar to the sigmoid function, but with normalization.&nbsp;<a class="footnote-backref" href="#fnref-1" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
<li id="fn-2">
<p>Actually, we don't want this. The probability of any class should never be exactly zero as this might cause problems later. It might get very very small, but should never be 0.&nbsp;<a class="footnote-backref" href="#fnref-2" title="Jump back to footnote 2 in the text">↩</a></p>
</li>
<li id="fn-3">
<p>Backpropagation is only a clever implementation of gradient descent. It belongs to the bigger class of iterative descent algorithms.&nbsp;<a class="footnote-backref" href="#fnref-3" title="Jump back to footnote 3 in the text">↩</a></p>
</li>
</ol>
</hr></div>
            
            <div id="disqus_thread"></div>
<script>
    /*
    var disqus_config = function () {
        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW
        var d = document, s = d.createElement('script');

        s.src = '//martinthoma.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

            <hr/>
        </div>
        <section>
        <div class="col-sm-2 col-md-2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time pubdate="pubdate" datetime="2016-07-19T14:00:00+02:00">Jul 19, 2016</time>
            <br/>
            by <a rel="author" class="vcard author post-author" itemprop="author" href="../author/martin-thoma/"><span class="fn" itemscope="" itemtype="https://schema.org/Person"><span itemprop="name">Martin Thoma</span></span></a>
            <h4>Category</h4>
            <a class="category-link" href="../categories.html#machine-learning-ref">Machine Learning</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="../tags.html#machine-learning-ref">Machine Learning
                    <span>37</span>
</a></li>
                <li><a href="../tags.html#python-ref">Python
                    <span>78</span>
</a></li>
                <li><a href="../tags.html#sklearn-ref">sklearn
                    <span>2</span>
</a></li>
                <li><a href="../tags.html#tensorflow-ref">Tensorflow
                    <span>2</span>
</a></li>
            </ul>
<h4>Contact</h4>
    <a href="https://twitter.com/themoosemind" title="My Twitter Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-twitter sidebar-social-links"></i></a>
    <a href="mailto:info@martin-thoma.de" title="My Email Address" class="sidebar-social-links" target="_blank">
    <i class="fa fa-envelope sidebar-social-links"></i></a>
    <a href="https://github.com/MartinThoma" title="My Github Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-github sidebar-social-links"></i></a>
    <a href="http://stackoverflow.com/users/562769/martin-thoma" title="My Stackoverflow Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-stackoverflow sidebar-social-links"></i></a>
        </div>
        </section>
</div>
</article>
                </div>
                <div class="col-sm-1 col-md-1"></div>
            </div>
        </div>
        <div id="push"></div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-subtitle"><span class="site-name">Martin Thoma</span> - A blog about Code, the Web and Cyberculture</li>
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>        <script src="//code.jquery.com/jquery.min.js"></script>
        <!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>
        <script>
(function(){
    'use strict';

    /*
    Create intra-page links
    Requires that your headings already have an `id` attribute set (because that's what jekyll does)
    For every heading in your page, this adds a little anchor link `#` that you can click to get a permalink to the heading.
    Ignores `h1`, because you should only have one per page.
    The text content of the tag is used to generate the link, so it will fail "gracefully-ish" if you have duplicate heading text.

    Credit: https://gist.github.com/SimplGy/a229d25cdb19d7f21231
     */

    var headingNodes = [], results, link,
        tags = ['h2', 'h3', 'h4', 'h5', 'h6'];

    tags.forEach(function(tag){
        var contentTag = document.getElementById('contentAfterTitle');
      results = contentTag.getElementsByTagName(tag);
      Array.prototype.push.apply(headingNodes, results);
    });

    headingNodes.forEach(function(node){
      link = document.createElement('a');
      link.className = 'deepLink';
      link.textContent = ' ¶';
      link.href = '#' + node.getAttribute('id');
      node.appendChild(link);
    });

  })();
</script>
    </body>
    <!-- Theme: Elegant built for Pelican
    License : http://oncrashreboot.com/pelican-elegant -->
</html>