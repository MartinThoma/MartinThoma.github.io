---
layout: post
title: Lokalisierung Mobiler Agenten
slug: lma
author: Martin Thoma
date: 2016-05-18 20:00
category: German posts
tags: Klausur, Sensors, Machine Learning
featured_image: logos/klausur.png
---
<div class="info">Dieser Artikel beschäftigt sich mit der Vorlesung &bdquo;Lokalisierung Mobiler Agenten&ldquo; am KIT. Er dient als Prüfungsvorbereitung. Ich habe die Vorlesungen bei <a href="http://isas.uka.de/User:Kurz/de?uselang=de">Herrn Dr.-Ing. Gerhard Kurz</a> im Sommersemester 2016 gehört.</div>

## Behandelter Stoff

### Übersicht

<table>
<tr>
    <th>Datum</th>
    <th>Kapitel</th>
    <th>Inhalt</th>
</tr>
<tr>
    <td>17.05.2016</td>
    <td>5. Vorlesung</td>
    <td>Hessische Normalform, Least Squares</td>
</tr>
<tr>
    <td>24.05.2016</td>
    <td>6. Vorlesung</td>
    <td>Least Squares</td>
</tr>
<tr>
    <td>21.06.2016</td>
    <td>10. Vorlesung</td>
    <td>Optimalitätsbeweise (Erwartungstreue); Stochastische Kombination</td>
</tr>
<tr>
    <td>28.06.2016</td>
    <td>11. Vorlesung</td>
    <td>Dynamische Lokalisierung (Kalman Filter: Prädiktions und Filterschritt)</td>
</tr>
<tr>
    <td>05.07.2016</td>
    <td>12. Vorlesung</td>
    <td>Kalman Filter: Filterschritt; SLAM</td>
</tr>
<tr>
    <td>12.07.2016</td>
    <td>13. Vorlesung</td>
    <td>Particle Filter, SLAM</td>
</tr>
</table>


### Statische Lokalisierung

Slides: `20160517-lma-1`

<dl>
  <dt><dfn>Hessische Normalform</dfn> (<a href="https://en.wikipedia.org/wiki/Hesse_normal_form"><dfn>Hesse normal form</dfn></a>)</dt>
  <dd>An equation describing a plane:

      $$x \cdot n - d = 0$$

      where $x$ is a point, $n$ is the normal of the plane and $d$ is the
      distance of the plane to $0$.
  </dd>
  <dt><a href="https://de.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate"><dfn>Methode der kleinsten Quadrate</dfn></a> (<dfn>Least Squares Methode</dfn>)</dt>
  <dd>Bei der Lokalisierung mobiler Agenten in einem Raum mit 4&nbsp;Wänden
      genügen zwei nicht-parallele Abstandsmessungen. Hat man mehr Messungen,
      so kann man mit der Methode kleinster Quadrate die beste Position
      bestimmen.<br/>
      <br/>
      Der Fehler ist dann $e = y - H x$.<br/>
      <br/>
      Damit definiert man das Gütemaß

      $$G(x) = (y- Hx)^T W^{-1} (y-Hx)$$

      wobei $W^{-1}$ eine symmetrisch positiv definite Gewichtungsmatrix ist.
      </dd>
   <dt><dfn>Rekursive Methode kleinster Quadrate</dfn></dt>
   <dd>Wenn bereits einmal die beste Ebene bestimmt wurde will man nicht wieder
       alles neu berechnen, wenn ein neues Sensorergebnis hinzukommt.

       Bei uns mit Vektorwertigen Sensoren, nicht wie im wie im Skript mit
       Skalaren.</dd>
</dl>

Slides: `6. Vorlesung`

<dl>
    <dt><a href="https://de.wikipedia.org/wiki/Sherman-Morrison-Woodbury-Formel"><dfn>Sherman-Morrison-Woodbury Formel</dfn></a></dt>
    <dd>Für zwei $n\times k$-Matrizen $U, V$ gilt:<br/>
        Die $k\times k$-Matrix $E-V^T A^{-1}U$ sei regulär, dann gilt
        $$(A-UV^T)^{-1} = A^{-1}+A^{-1}U(E-V^TA^{-1}U)^{-1}V^T A^{-1}$$
    </dd>
</dl>


## Dynamische Lokalisierung

<dl>
    <dt><dfn id="kalman-filter">Kalman-Filter</dfn></dt>
    <dd>Siehe <a href="https://martin-thoma.com/kalman-filter/">Kalman-filter Artikel</a>.</dd>
</dl>


## SLAM

<dl>
    <dt><a href="https://de.wikipedia.org/wiki/Simultaneous_Localization_and_Mapping"><dfn id="slam">SLAM</dfn></a> (<dfn>Simultaneous Location and Mapping</dfn>)</dt>
    <dd>SLAM ist ein Algorithmus, welcher zugleich eine Karte erstellt und
        einen mobilen Agenten auf der Karte lokalisiert. Eine Karte sind
        positionen von Landmarken.<br/>
        <br/>
        <u>Ansätze</u>:

        <ul>
            <li>Filterung (rekursiv): z.B. Kalman-Filter;
                <abbr title="Extended Kalman Filter">EKF</abbr>,
                <abbr title="Unscented Kalman Filter">UKF</abbr> (sigma-Punkte / samples; einfach zu implementieren und liefert häufig bessere Ergebnisse als EKF),
                Partikelfilter (Wahrscheinlichkeitsdichten werden durch Partikel / samples repräsentiert)<br/>
                $\Rightarrow$ Schätzen für Zeitschritt</li>
            <li>Smoothing / Glättung (batch): Nachteile (Rechenaufwendig, erst nach batch ist Ergebnis da, man muss Daten speichern) und Vorteile (komplette Trajektorie, optimale Schätzung selbiger); z.B.
                Graph-based SLAM</li>
        </ul>

        <u>Loop Closure</u>: Wenn der Agent wieder an einen Punkt kommt, an dem
        er bereits war, kann ein Kreis geschlossen werden. Daher können die
        vorherigen Landmarken im Kreis neu geschätzt werden. Allerdings kann
        ein falsches Loop Closure zur Divergenz des Filters führen.<br/>
        <br/>
        <u>Herausforderungen</u>:
        <ul>
            <li>Datenassoziation: Erkennen ob Landmarken die gleichen sind.</li>
            <li>Komplexität (große Karte)</li>
            <li>Veränderung der Umgebung: z.B. Auto als Landmarke erkannt; neue Brücke kommt hinzu oder wurde abgerissen</li>
            <li>Nichtlinearitäten des Prolems</li>
        </ul>
    </dd>
    <dt><dfn>Landmarke</dfn></dt>
    <dd>Ein gut sichtbares / wiedererkennbares Objekt, welches zur Abstandsmessung
        benutzt werden kann.</dd>
    <dt><dfn>EKF SLAM</dfn></dt>
    <dd>

        EKF SLAM basiert auf dem Extended Kalman Filter.

        <u>Vereinfachungen</u>:

        <ul>
            <li>Problem in 2D</li>
            <li>Feste Zahl von punktförmigen Landmarken</li>
            <li>Assoziationen bekannt: Landmarken werden zuverlässig erkannt
                und nicht verwechselt.</li>
        </ul>

        Zustand $\mathbf{x}_k = [\underbrace{x_k, y_k, \theta_k}_{\text{Pose des Roboters}}, \underbrace{m_{1, x}, m_{1, y}}_{\text{LM 1}}, \dots, \underbrace{m_{M, x}, m_{M, y}}_{\text{LM } M}]$
        <br/>

        Dimension von $\mathbf{x}_k$: $3 + 2 M$.<br/>
        Kovarianz hat $(3+2M)^2$ Elemente, d.h. der Speicheraufwand ist
        quadratisch in der Zahl der Landmarken.<br/>
        <br/>
        Systemgleichung: $x_{k+1} = a_k (x_k, M_k) + w_k$<br/>
        $x_{k+1} \approx A_k x_k + w_k,$
        wobei $A_k$ Jaccobi-Matrix von $a_k(x_k, u_k)$ für festes $u_k$ an der
        Stelle $\hat{x}_k^e$.<br/>
        <br/>
        Messgleichung: $y_k = h(x_k) + v_k$<br/>
        In der Regel sind nicht alle Landmarken zugleich sichtbar. In diesem
        Fall werden Zeilen in der Messabbildung weggelassen.
        $$y_k = H_k(x_k) + v_k,$$
        wobei $H_k$ die Jaccobi-Matrix von $h(x_k)$ an der Stelle $\hat{x}_k^P$
        ist.
    </dd>
    <dt><a href="https://en.wikipedia.org/wiki/Particle_filter"><dfn id="particle-filter">Particle Filter</dfn></a> (<dfn>Partikelfilter</dfn>, <dfn>Sequential Monte Carlo</dfn>, <dfn>SMC</dfn>)</dt>
    <dd>

        Idee: Schätze Zustand über Partikel.

        <u>Ansatz</u>: Sample aus Tranistionsdichte
        $$f(x_{k+1} | x_k = p_k^1, u_k$$

        Implementierung:

        <ul>
            <li>Rauschen $w_k$ samples $\rightarrow v_k^1, \dots, v_k^N$</li>
            <li>Propagiere durch $x_{k+1} = a_N(x_N, u_N) + w_N$, d.h.
                $p^1_{k+1} = a_k (p_k^1, u_k) + v_k^1$</li>
            <li>Gewichte bleiben gleich</li>
        </ul>

        <u>Problem</u>: Viele Partikel $p_k^i$ haben geringes Gewicht $w_k^i$<br/>
        <u>Lösung</u>: Resampling mit sequential importance resampling (SIR:
                       ziehe $N$ neue Partikel mit Gewichten $\frac{1}{n}$ aus
                       Menge der alten Partikel,
                       wobei die Wahrscheinlichkeit das Partikel $p_k^i$ zu ziehen
                       gerade $w_k^i$ ist.)
    </dd>
</dl>


## Material und Links

Die Vorlesung wurde gestreamt und ist unter
[mml-streamdb01.ira.uka.de](http://mml-streamdb01.ira.uka.de/) verfügbar.

* [Vorlesungswebsite](http://isas.uka.de/Lokalisierung_mobiler_Agenten_(SS_2016)/de?uselang=de)

Literatur und Links:

* [SLAM-Vorlesung der Uni Freiburg](https://www.youtube.com/watch?v=U6vr3iNrwRA&list=PLgnQpQtFTOGQrZ4O5QzbIHgl3b1JHimN_) ([Material](http://ais.informatik.uni-freiburg.de/teaching/ws13/mapping/))


## Vorlesungsempfehlungen

Folgende Vorlesungen sind ähnlich:

* [Analysetechniken großer Datenbestände](https://martin-thoma.com/analysetechniken-grosser-datenbestaende/)
* [Informationsfusion](https://martin-thoma.com/informationsfusion/)
* [Machine Learning 1](https://martin-thoma.com/machine-learning-1-course/)
* [Machine Learning 2](https://martin-thoma.com/machine-learning-2-course/)
* [Mustererkennung](https://martin-thoma.com/mustererkennung-klausur/)
* [Neuronale Netze](https://martin-thoma.com/neuronale-netze-vorlesung/)
* [Lokalisierung Mobiler Agenten](https://martin-thoma.com/lma/)
* [Probabilistische Planung](https://martin-thoma.com/probabilistische-planung/)


## Termine und Klausurablauf

Es ist eine mündliche Prüfung.
