---
layout: post
title: Lokalisierung Mobiler Agenten
slug: lma
author: Martin Thoma
date: 2016-05-18 20:00
category: German posts
tags: Klausur, Sensors, Machine Learning
featured_image: logos/klausur.png
---
<div class="info">Dieser Artikel beschäftigt sich mit der Vorlesung &bdquo;Lokalisierung Mobiler Agenten&ldquo; am KIT. Er dient als Prüfungsvorbereitung. Ich habe die Vorlesungen bei <a href="http://isas.uka.de/User:Kurz/de?uselang=de">Herrn Dr.-Ing. Gerhard Kurz</a> im Sommersemester 2016 gehört.</div>

## Behandelter Stoff

### Übersicht

<table>
<tr>
    <th>Datum</th>
    <th>Kapitel</th>
    <th>Inhalt</th>
</tr>
<tr>
    <td>17.05.2016</td>
    <td>5. Vorlesung</td>
    <td>Hessische Normalform, Least Squares</td>
</tr>
<tr>
    <td>24.05.2016</td>
    <td>6. Vorlesung</td>
    <td>Least Squares</td>
</tr>
<tr>
    <td>21.06.2016</td>
    <td>10. Vorlesung</td>
    <td>Optimalitätsbeweise (Erwartungstreue); Stochastische Kombination</td>
</tr>
<tr>
    <td>28.06.2016</td>
    <td>11. Vorlesung</td>
    <td>Dynamische Lokalisierung (Kalman Filter: Prädiktions und Filterschritt)</td>
</tr>
<tr>
    <td>05.07.2016</td>
    <td>12. Vorlesung</td>
    <td>Kalman Filter: Filterschritt; SLAM</td>
</tr>
</table>


### Statische Lokalisierung

Slides: `20160517-lma-1`

<dl>
  <dt><dfn>Hessische Normalform</dfn> (<a href="https://en.wikipedia.org/wiki/Hesse_normal_form"><dfn>Hesse normal form</dfn></a>)</dt>
  <dd>An equation describing a plane:

      $$x \cdot n - d = 0$$

      where $x$ is a point, $n$ is the normal of the plane and $d$ is the
      distance of the plane to $0$.
  </dd>
  <dt><a href="https://de.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate"><dfn>Methode der kleinsten Quadrate</dfn></a> (<dfn>Least Squares Methode</dfn>)</dt>
  <dd>Bei der Lokalisierung mobiler Agenten in einem Raum mit 4&nbsp;Wänden
      genügen zwei nicht-parallele Abstandsmessungen. Hat man mehr Messungen,
      so kann man mit der Methode kleinster Quadrate die beste Position
      bestimmen.<br/>
      <br/>
      Der Fehler ist dann $e = y - H x$.<br/>
      <br/>
      Damit definiert man das Gütemaß

      $$G(x) = (y- Hx)^T W^{-1} (y-Hx)$$

      wobei $W^{-1}$ eine symmetrisch positiv definite Gewichtungsmatrix ist.
      </dd>
   <dt><dfn>Rekursive Methode kleinster Quadrate</dfn></dt>
   <dd>Wenn bereits einmal die beste Ebene bestimmt wurde will man nicht wieder
       alles neu berechnen, wenn ein neues Sensorergebnis hinzukommt.

       Bei uns mit Vektorwertigen Sensoren, nicht wie im wie im Skript mit
       Skalaren.</dd>
</dl>

Slides: `6. Vorlesung`

<dl>
    <dt><a href="https://de.wikipedia.org/wiki/Sherman-Morrison-Woodbury-Formel"><dfn>Sherman-Morrison-Woodbury Formel</dfn></a></dt>
    <dd>Für zwei $n\times k$-Matrizen $U, V$ gilt:<br/>
        Die $k\times k$-Matrix $E-V^T A^{-1}U$ sei regulär, dann gilt
        $$(A-UV^T)^{-1} = A^{-1}+A^{-1}U(E-V^TA^{-1}U)^{-1}V^T A^{-1}$$
    </dd>
</dl>


## Dynamische Lokalisierung

<dl>
    <dt><dfn id="kalman-filter">Kalman-Filter</dfn></dt>
    <dd>Siehe <a href="https://martin-thoma.com/kalman-filter/">Kalman-filter Artikel</a>.</dd>
</dl>


## SLAM

<dl>
    <dt><a href="https://de.wikipedia.org/wiki/Simultaneous_Localization_and_Mapping"><dfn id="slam">SLAM</dfn></a> (<dfn>Simultaneous Location and Mapping</dfn>)</dt>
    <dd>SLAM ist ein Algorithmus, welcher zugleich eine Karte erstellt und
        einen mobilen Agenten auf der Karte lokalisiert. Eine Karte sind
        positionen von Landmarken.<br/>
        <br/>
        <u>Ansätze</u>:

        <ul>
            <li>Filterung (rekursiv): z.B. Kalman-Filter;
                <abbr title="Extended Kalman Filter">EKF</abbr>,
                <abbr title="Unscented Kalman Filter">UKF</abbr> (sigma-Punkte / samples; einfach zu implementieren und liefert häufig bessere Ergebnisse als EKF),
                Partikelfilter (Wahrscheinlichkeitsdichten werden durch Partikel / samples repräsentiert)<br/>
                $\Rightarrow$ Schätzen für Zeitschritt</li>
            <li>Smoothing / Glättung (batch): Nachteile (Rechenaufwendig, erst nach batch ist Ergebnis da, man muss Daten speichern) und Vorteile (komplette Trajektorie, optimale Schätzung selbiger); z.B.
                Graph-based SLAM</li>
        </ul>

        <u>Loop Closure</u>: Wenn der Agent wieder an einen Punkt kommt, an dem
        er bereits war, kann ein Kreis geschlossen werden. Daher können die
        vorherigen Landmarken im Kreis neu geschätzt werden. Allerdings kann
        ein falsches Loop Closure zur Divergenz des Filters führen.<br/>
        <br/>
        <u>Herausforderungen</u>:
        <ul>
            <li>Datenassoziation: Erkennen ob Landmarken die gleichen sind.</li>
            <li>Komplexität (große Karte)</li>
            <li>Veränderung der Umgebung: z.B. Auto als Landmarke erkannt; neue Brücke kommt hinzu oder wurde abgerissen</li>
            <li>Nichtlinearitäten des Prolems</li>
        </ul>
    </dd>
</dl>


## Material und Links

Die Vorlesung wurde gestreamt und ist unter
[mml-streamdb01.ira.uka.de](http://mml-streamdb01.ira.uka.de/) verfügbar.

* [Vorlesungswebsite](http://isas.uka.de/Lokalisierung_mobiler_Agenten_(SS_2016)/de?uselang=de)

Literatur und Links:

* [SLAM-Vorlesung der Uni Freiburg](https://www.youtube.com/watch?v=U6vr3iNrwRA&list=PLgnQpQtFTOGQrZ4O5QzbIHgl3b1JHimN_) ([Material](http://ais.informatik.uni-freiburg.de/teaching/ws13/mapping/))


## Vorlesungsempfehlungen

Folgende Vorlesungen sind ähnlich:

* [Analysetechniken großer Datenbestände](https://martin-thoma.com/analysetechniken-grosser-datenbestaende/)
* [Informationsfusion](https://martin-thoma.com/informationsfusion/)
* [Machine Learning 1](https://martin-thoma.com/machine-learning-1-course/)
* [Machine Learning 2](https://martin-thoma.com/machine-learning-2-course/)
* [Mustererkennung](https://martin-thoma.com/mustererkennung-klausur/)
* [Neuronale Netze](https://martin-thoma.com/neuronale-netze-vorlesung/)
* [Lokalisierung Mobiler Agenten](https://martin-thoma.com/lma/)
* [Probabilistische Planung](https://martin-thoma.com/probabilistische-planung/)


## Termine und Klausurablauf

Es ist eine mündliche Prüfung.
