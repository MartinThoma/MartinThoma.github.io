---
layout: post
title: Lokalisierung Mobiler Agenten
slug: lma
author: Martin Thoma
date: 2016-05-18 20:00
category: German posts
tags: Klausur
featured_image: logos/klausur.png
---
<div class="info">Dieser Artikel beschäftigt sich mit der Vorlesung &bdquo;Lokalisierung Mobiler Agenten&ldquo; am KIT. Er dient als Prüfungsvorbereitung. Ich habe die Vorlesungen bei <a href="http://isas.uka.de/User:Kurz/de?uselang=de">Herrn Dr.-Ing. Gerhard Kurz</a> im Sommersemester 2016 gehört.</div>

## Behandelter Stoff

### Übersicht

<table>
<tr>
    <th>Datum</th>
    <th>Kapitel</th>
    <th>Inhalt</th>
</tr>
<tr>
    <td>17.05.2016</td>
    <td>5. Vorlesung</td>
    <td>Hessische Normalform, Least Squares</td>
</tr>
<tr>
    <td>24.05.2016</td>
    <td>6. Vorlesung</td>
    <td>Least Squares</td>
</tr>
</table>


### Statische Lokalisierung

(20160517-lma-1)

Mit der hessischen Normalform

<dl>
  <dt><dfn>Hessische Normalform</dfn> (<a href="https://en.wikipedia.org/wiki/Hesse_normal_form"><dfn>Hesse normal form</dfn></a>)</dt>
  <dd>An equation describing a plane:

      $$x \cdot n - d = 0$$

      where $x$ is a point, $n$ is the normal of the plane and $d$ is the
      distance of the plane to $0$.
  </dd>
  <dt><a href="https://de.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate"><dfn>Methode der kleinsten Quadrate</dfn></a> (<dfn>Least Squares Methode</dfn>)</dt>
  <dd>Bei der Lokalisierung mobiler Agenten in einem Raum mit 4&nbsp;Wänden
      genügen zwei nicht-parallele Abstandsmessungen. Hat man mehr Messungen,
      so kann man mit der Methode kleinster Quadrate die beste Position
      bestimmen.<br/>
      <br/>
      Der Fehler ist dann $e = y - H x$.<br/>
      <br/>
      Damit definiert man das Gütemaß

      $$G(x) = (y- Hx)^T W^{-1} (y-Hx)$$

      wobei $W^{-1}$ eine symmetrisch positiv definite Gewichtungsmatrix ist.
      </dd>
   <dt><dfn>Rekursive Methode kleinster Quadrate</dfn></dt>
   <dd>Wenn bereits einmal die beste Ebene bestimmt wurde will man nicht wieder
       alles neu berechnen, wenn ein neues Sensorergebnis hinzukommt.

       Bei uns mit Vektorwertigen Sensoren, nicht wie im wie im Skript mit
       Skalaren.</dd>
</dl>

Slides: `6. Vorlesung`

<dl>
    <dt><a href="https://de.wikipedia.org/wiki/Sherman-Morrison-Woodbury-Formel"><dfn>Sherman-Morrison-Woodbury Formel</dfn></a></dt>
    <dd>Für zwei $n\times k$-Matrizen $U, V$ gilt:<br/>
        Die $k\times k$-Matrix $E-V^T A^{-1}U$ sei regulär, dann gilt
        $$(A-UV^T)^{-1} = A^{-1}+A^{-1}U(E-V^TA^{-1}U)^{-1}V^T A^{-1}$$$
    </dd>
</dl>

## Material und Links

Die Vorlesung wurde gestreamt und ist unter
[mml-streamdb01.ira.uka.de](http://mml-streamdb01.ira.uka.de/) verfügbar.

* [Vorlesungswebsite](http://isas.uka.de/Lokalisierung_mobiler_Agenten_(SS_2016)/de?uselang=de)

Literatur:

* TODO


## Vorlesungsempfehlungen

Folgende Vorlesungen sind ähnlich:

* [Analysetechniken großer Datenbestände](https://martin-thoma.com/analysetechniken-grosser-datenbestaende/)
* [Informationsfusion](https://martin-thoma.com/informationsfusion/)
* [Machine Learning 1](https://martin-thoma.com/machine-learning-1-course/)
* [Machine Learning 2](https://martin-thoma.com/machine-learning-2-course/)
* [Mustererkennung](https://martin-thoma.com/mustererkennung-klausur/)
* [Neuronale Netze](https://martin-thoma.com/neuronale-netze-vorlesung/)
* [Lokalisierung Mobiler Agenten](https://martin-thoma.com/lma/)
* [Probabilistische Planung](https://martin-thoma.com/probabilistische-planung/)


## Termine und Klausurablauf

Es ist eine mündliche Prüfung.
