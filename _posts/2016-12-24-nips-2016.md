---
layout: post
title: NIPS 2016
slug: nips-2016
author: Martin Thoma
date: 2016-12-24 20:00
category: Machine Learning
tags: Machine Learning, research
featured_image: logos/ml.png
---
The Conference and Workshop on Neural Information Processing Systems (NIPS) is
probably the biggest conference with machine learning / deep learning as a
main topic. This year, about 6000 people attended it. My friend Marvin and me
were supported by [Begabtenstiftung Informatik Karlsruhe](https://www.informatik.kit.edu/begabtenstiftung_informatik_karlsruhe.php).

The complete program can be found in the [Conference Book](https://media.nips.cc/Conferences/2016/NIPS-2016-Conference-Book.pdf), but I would like to point out some of my highlights.


## Hot Topics

To get an idea what NIPS 2016 was about, I generated a word cloud from the titles
of the accepted papers:

![Wordcloud of NIPS titles of 2016](https://martin-thoma.com/images/2016/12/nips-2016-wordcloud.png)

The organization team made something similar, but they had access to the
information how the papers were tagged:

![NIPS 2016 areas submitted](https://martin-thoma.com/images/2016/12/subject_areas_submitted1.png)

(Image source: <a href="http://www.tml.cs.uni-tuebingen.de/team/luxburg/misc/nips2016/index.php">www.tml.cs.uni-tuebingen.de</a>)

The top 10 topics were:

<ol>
    <li>01: Deep Learning or Neural Networks</li>
    <li>42: (Application) Computer Vision</li>
    <li>02: Large Scale Learning and Big Data</li>
    <li>05: Learning Theory</li>
    <li>53: (Other) Optimization</li>
    <li>08: Sparsity and Feature Selection</li>
    <li>51: (Other) Classification</li>
    <li>03: Convex Optimization</li>
    <li>54: (Other) Probabilistic Models and Methods</li>
    <li>56: (Other) Unsupervised Learning Methods</li>
</ol>


## GANs

Generative Adverserial Networks (short: GANs) were one hot topic at NIPS. The
idea is to train two networks: A generator $G$ and a discriminator $D$. The
generator creates content (e.g. images) and the discriminator has to decide if
the content is of the natural distribution (the training set) or made by the
generator.

Noteworthy papers and ideas are:

<ul>
    <li><a href="https://papers.nips.cc/paper/6111-learning-what-and-where-to-draw.pdf">Learning What and Where to Draw</a></li>
    <li><a href="https://papers.nips.cc/paper/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets.pdf">InfoGAN</a>: Get more control about properties of the generated content.</li>
    <li><a href="https://github.com/soumith/ganhacks">How to Train a GAN?</a> Tips and tricks to make GANs work</li>
</ul>


## Nut's and Bolts of ML

Andrew Ng gave a talk in which he summarized what he thinks are some of the
most important topics when training machine learning systems. Most of it
is probably also in his book [Machine Learning Yearning](http://www.mlyearning.org/)
or in his [Coursera course](https://www.coursera.org/learn/machine-learning).

Here are some of the things he talked about:

* When you design a speech recognition system, you can measure 3 types of errors:
  Human error, training set error and test set error. The difference between
  the human error and the training error is "avoidable error" (bias), the difference
  between training and test error is "variance".
* Human level performance is ambiguous: In a medical application, is an
  amateur, a doctor, an experienced doctor or a team of (experienced) doctors
  the "human level performance"?
* Role of an "AI Product Manager"


## More Papers

Clustering:

* [Fast and Provably Good Seedings for k-Means](https://papers.nips.cc/paper/6478-fast-and-provably-good-seedings-for-k-means)

Optimization:

* [MetaGrad: Multiple Learning Rates in Online Learning](https://papers.nips.cc/paper/6268-metagrad-multiple-learning-rates-in-online-learning)
    * https://bitbucket.org/wmkoolen/metagrad
* [Optimal Learning for Multi-pass Stochastic Gradient Methods](https://papers.nips.cc/paper/6213-optimal-learning-for-multi-pass-stochastic-gradient-methods)
* [Optimal Learning for Multi-pass Stochastic Gradient Methods](https://papers.nips.cc/paper/6213-optimal-learning-for-multi-pass-stochastic-gradient-methods.pdf)

Theory:

* [Deep Learning without Poor Local Minima](https://papers.nips.cc/paper/6112-deep-learning-without-poor-local-minima): Local Minima are global minima in "typical" networks
* [Matrix Completion has No Spurious Local Minimum](https://papers.nips.cc/paper/6048-matrix-completion-has-no-spurious-local-minimum.pdf)
* [Understanding the Effective Receptive Field in Deep Convolutional Neural Networks](http://www.cs.toronto.edu/~wenjie/papers/nips16/top.pdf)


Topology learning:

* [Learning the Number of Neurons in Deep Networks](https://papers.nips.cc/paper/6372-learning-the-number-of-neurons-in-deep-networks)
* From another talk. Most add nodes / edges over time from an initial seed network:
    * 1960, Erd√∂s & Renyi: Random graphs
    * 1998, Watts & Strogatz: Small-world graph
    * 1999, Barabasi & Albert: Preferential attachment
    * 1999, Kleinburg et al.: Copying model
    * 2003, Vazquez et al.: Duplication-divergence
    * 2007, Leskovec et al.: Forest fire
    * 2008, Clauset et al.: Hierarchical random graphs
    * 2010, Leskovec et al.: Kronecker graphs
* Network compression
    * [Dynamic Network Surgery for Efficient DNNs](https://arxiv.org/abs/1608.04493)
    * [PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions](https://arxiv.org/abs/1504.08362)
* [Swapout: Learning an ensemble of deep architectures](https://arxiv.org/abs/1605.06465)


Analysis of ML models:

* [Blind Attacks on Machine Learners](https://papers.nips.cc/paper/6482-blind-attacks-on-machine-learners)
* [Measuring Neural Net Robustness with Constraints](https://arxiv.org/abs/1605.07262): Measure robustnes against adverserial examples
* [Robustness of classifiers: from adversarial to random noise](https://arxiv.org/abs/1608.08967)
* [Unsupervised Risk Estimation Using Only Conditional Independence Structure](https://arxiv.org/abs/1606.05313)
* [Blind Attacks on Machine Learners](https://papers.nips.cc/paper/6482-blind-attacks-on-machine-learners)
* [Examples are not Enough, Learn to Criticize! Criticism for Interpretability](http://people.csail.mit.edu/beenkim/papers/KIM2016NIPS_MMD.pdf) ([github](https://github.com/BeenKim/MMD-critic))
* [Identifying Unknown Unknowns in the Open World: Representations and Policies for Guided Exploration](https://arxiv.org/abs/1610.09064)


Content creation:

* [Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks](http://visualdynamics.csail.mit.edu/)


Labeling:

* [Active Learning from Imperfect Labelers](https://arxiv.org/abs/1610.09730)
* [Fundamental Limits of Budget-Fidelity Trade-off in Label Crowdsourcing](https://arxiv.org/abs/1608.07328)
* [Avoiding Imposters and Delinquents: Adversarial Crowdsourcing and Peer Prediction](https://arxiv.org/abs/1606.05374)


Content based Image Retrival (CBIR):

* [Improved Deep Metric Learning with Multi-class N-pair Loss Objective](https://papers.nips.cc/paper/6200-improved-deep-metric-learning-with-multi-class-n-pair-loss-objective.pdf)
* [Learning Deep Embeddings with Histogram Loss](https://arxiv.org/abs/1611.00822)
* [Local Similarity-Aware Deep Feature Embedding](https://arxiv.org/abs/1610.08904)
* [CliqueCNN: Deep Unsupervised Exemplar Learning](https://arxiv.org/abs/1608.08792)
* [What Makes Objects Similar: A Unified Multi-Metric Learning Approach](https://papers.nips.cc/paper/6192-what-makes-objects-similar-a-unified-multi-metric-learning-approach)


Misc:

* [Learnable Visual Markers](https://papers.nips.cc/paper/6323-learnable-visual-markers): Visual markers are something like barcodes
* [Neurally-Guided Procedural Models: Amortized Inference for Procedural Graphics Programs using Neural Networks](https://arxiv.org/abs/1603.06143)
* [Universal Correspondence Network](https://arxiv.org/pdf/1606.03558v3.pdf): Find semantically meaningful similar points in two images. For example, to frontal images of different humans, where the network finds eyes, nose, chin, lips in both images.
* [Scene Recognition Demo](http://places.csail.mit.edu/demo.html)


## Lessons learned for conferences

* Bring a camera: The information comes very fast. Too fast to take notes, but
  you can shoot a photo of the slides. In fact, quite a lot of people do so.
* Shoot a photo of the first slide, so that you know what the talk was about
  when you look at your slides.
* If you give a talk / poster...
    * ... let the first slide be there long enough, so that people can take a
      photo of it.
    * ... or have a URL / the title of the paper on every single slide
    * ... let every slide be visible long enough, so that people can take photos
    * ... don't use QR-codes only, but also (shortened) URLs
    * ... make it available online as PDF
    * ... answer key questions: (1) Which problem did you tackle? (2) How did you test your results? (3) To what is your "solution" similar?
* As a session organizer...
    * ... make sure there is a schedule at the door (outside)
    * ... make sure the schedule is online
    * ... make sure the schedule is changed everywhere, if it is actually changed



## Miscallenious

* [Advances in Neural Information Processing Systems 29 (NIPS 2016) pre-proceedings](https://papers.nips.cc/book/advances-in-neural-information-processing-systems-29-2016)
* Jobs
    * [unify.id](https://unify.id/fellowship-fall2016.html)
    * [bluevisionlabs.com](http://www.bluevisionlabs.com/)
* [NIPS Papers Kaggle Dataset](https://www.kaggle.com/benhamner/nips-papers)
* blog.aylien.com: [Highlights of NIPS 2016: Adversarial Learning, Meta-learning and more](http://blog.aylien.com/highlights-nips-2016)
* [salon-des-refuses.org](http://salon-des-refuses.org/) should contain papers
  which were refused, but are also of high quality. However, the website seems to be down.
* Martin Zinkevich: [Rules of Machine Learning: Best Practices for ML Engineering](http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf)
* [Implementations for NIPS 2016 papers](https://www.reddit.com/r/MachineLearning/comments/5hwqeb/project_all_code_implementations_for_nips_2016/)
