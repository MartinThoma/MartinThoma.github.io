<!doctype html>
<html lang="en">
  <!-- type: head.html -->
  <head>
    <meta charset="utf-8">
    
    

    
        <meta name="image" content="https://martin-thoma.com/images/logos/klausur.png" />
        <meta name="thumbnail" content="//martin-thoma.com/images/logos/klausur.png" />
        <meta property="og:image" content="//martin-thoma.com/images/logos/klausur.png" />
    

    <meta property="og:type" content="blog"/>

    <title>Analysetechniken für große Datenbestände</title>
    <meta property="og:title" content="Analysetechniken für große Datenbestände" />
    <meta property="og:url" content="//martin-thoma.com/analysetechniken-grosser-datenbestaende" />
    <link rel="stylesheet" href="//martin-thoma.com/css/screen.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="//martin-thoma.com/css/style.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="//martin-thoma.com/css/pygments.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="//martin-thoma.com/css/tocplus-screen.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="//martin-thoma.com/css/print.css" type="text/css" media="print" />
    <link rel="stylesheet" href="//martin-thoma.com/css/handheld.css" type="text/css" media="only screen and (max-width: 480px)" />
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="alternate" type="application/rss+xml" title="Martin Thoma RSS Feed" href="//martin-thoma.com/feed/" /><!--TODO-->
    <link rel="shortcut icon" href="//martin-thoma.com/favicon.ico" type="image/x-icon" />

    <link rel="canonical" href="//martin-thoma.com/analysetechniken-grosser-datenbestaende" />
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:site" content="@themoosemind"/>
<meta name="twitter:creator" content="@themoosemind"/>
<meta name="twitter:title" content="Analysetechniken für große Datenbestände"/>

    <meta name="twitter:description" content="A blog about Code, the Web and Cyberculture" />


    <meta name="twitter:image" content="//martin-thoma.com/images/logos/klausur.png"/>



<meta name="twitter:url" content="//martin-thoma.com/analysetechniken-grosser-datenbestaende"/>
<meta name="twitter:domain" content="Martin Thoma.com"/>


    <script type='text/javascript' src="//martin-thoma.com/js/jquery.js"></script>
    <script type='text/javascript' src="//martin-thoma.com/js/jquery-migrate.min.js"></script>
    <style type="text/css">div#toc_container {width: 275px;}</style>
    <style type="text/css" id="syntaxhighlighteranchor"></style>

<!-- MathJax -->
<script type="text/x-mathjax-config">
<!--
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [ ['$$', '$$'], ['\\[','\\]']],
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    processEscapes: true
  }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
// -->
</script>
<script type="text/javascript" async
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<!-- Latest compiled and minified CSS bootstrap -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">
</head>

<!-- type: post.html -->
<body>
    <div id="wrapper">
        <div id="container" class="container">
            <div class="span-16">
                <!-- type: header.html -->
<div id="header" role="banner">
    <h1><a href="//martin-thoma.com">Martin Thoma</a></h1>
    <h2 style="margin-top: 0;">A blog about Code, the Web and Cyberculture.</h2>
</div>
<nav class="navcontainer" role="navigation">
    <ul id="nav">
        <li class=""><a href="//martin-thoma.com">Home</a></li>
        <li class="page_item page-item-41 "><a href="//martin-thoma.com/author/martin-thoma/">About Me</a></li>
        <li class="page_item page-item-91 "><a href="//martin-thoma.com/imprint/">Imprint</a></li>
    </ul>
</nav>

                <div id="content">
                    <article class="post type-post format-standard hentry clearfix" itemscope itemType="http://schema.org/BlogPosting">
                        <h2 class="title entry-title" itemprop="name headline">Analysetechniken für große Datenbestände</h2>
                        <link itemprop="mainEntityOfPage" href="//martin-thoma.com/analysetechniken-grosser-datenbestaende" />
                        <div class="postdate entry-date date-header">
                            <time datetime="2016-04-15T11:22:00+02:00" itemprop="datePublished">
                                April
                                15th,
                                  
                                2016
                            </time>
                        </div>

                        <div class="entry post-body" id="contentAfterTitle" itemprop="articleBody">
                            <div class="info">Dieser Artikel beschäftigt sich mit der Vorlesung „Analysetechniken für große Datenbestände“ am KIT. Er dient als Prüfungsvorbereitung. Ich habe die Vorlesungen bei <a href="https://dbis.ipd.kit.edu/english/336.php">Herrn Prof. Dr.-Ing. Klemens Böhm</a> im Wintersemester 2015/2016 gehört. Der Artikel ist noch am Entstehen.</div>

<div id="toc_container" class="toc_light_blue no_bullets">
   <p class="toc_title">Contents</p>
   <ul class="toc_list">
      <li class="toc_level-1 toc_section-1">
         <a href="#tocAnchor-1-1"><span class="tocnumber">1</span> <span class="toctext">Behandelter Stoff</span></a>
         <ul>
            <li class="toc_level-2 toc_section-2">
               <a href="#tocAnchor-1-1-1"><span class="tocnumber">1.1</span> <span class="toctext">Übersicht</span></a>
            </li>
            <li class="toc_level-2 toc_section-3">
               <a href="#tocAnchor-1-1-2"><span class="tocnumber">1.2</span> <span class="toctext">Einleitung</span></a>
            </li>
            <li class="toc_level-2 toc_section-4">
               <a href="#tocAnchor-1-1-3"><span class="tocnumber">1.3</span> <span class="toctext">Statistische Grundlagen</span></a>
            </li>
            <li class="toc_level-2 toc_section-5">
               <a href="#tocAnchor-1-1-4"><span class="tocnumber">1.4</span> <span class="toctext">Räumliche Indexstrutkuren</span></a>
            </li>
            <li class="toc_level-2 toc_section-6">
               <a href="#tocAnchor-1-1-5"><span class="tocnumber">1.5</span> <span class="toctext">Entscheidungsbäume</span></a>
            </li>
            <li class="toc_level-2 toc_section-7">
               <a href="#tocAnchor-1-1-6"><span class="tocnumber">1.6</span> <span class="toctext">Evaluation</span></a>
            </li>
            <li class="toc_level-2 toc_section-8">
               <a href="#tocAnchor-1-1-7"><span class="tocnumber">1.7</span> <span class="toctext">Association Rules</span></a>
            </li>
            <li class="toc_level-2 toc_section-9">
               <a href="#tocAnchor-1-1-8"><span class="tocnumber">1.8</span> <span class="toctext">Constraints</span></a>
            </li>
            <li class="toc_level-2 toc_section-10">
               <a href="#tocAnchor-1-1-9"><span class="tocnumber">1.9</span> <span class="toctext">Clustering</span></a>
            </li>
            <li class="toc_level-2 toc_section-11">
               <a href="#tocAnchor-1-1-10"><span class="tocnumber">1.10</span> <span class="toctext">Statistische Modellierung</span></a>
            </li>
            <li class="toc_level-2 toc_section-12">
               <a href="#tocAnchor-1-1-11"><span class="tocnumber">1.11</span> <span class="toctext">Support Vector Machines</span></a>
            </li>
            <li class="toc_level-2 toc_section-13">
               <a href="#tocAnchor-1-1-12"><span class="tocnumber">1.12</span> <span class="toctext">Ensembles</span></a>
            </li>
         </ul>
      </li>
      <li class="toc_level-1 toc_section-14">
         <a href="#tocAnchor-1-14"><span class="tocnumber">2</span> <span class="toctext">Prüfungsfragen</span></a>
         <ul>
            <li class="toc_level-2 toc_section-15">
               <a href="#tocAnchor-1-14-1"><span class="tocnumber">2.1</span> <span class="toctext">Association Rules</span></a>
            </li>
            <li class="toc_level-2 toc_section-16">
               <a href="#tocAnchor-1-14-2"><span class="tocnumber">2.2</span> <span class="toctext">Clustering</span></a>
            </li>
         </ul>
      </li>
      <li class="toc_level-1 toc_section-17">
         <a href="#tocAnchor-1-17"><span class="tocnumber">3</span> <span class="toctext">Übungen</span></a>
      </li>
      <li class="toc_level-1 toc_section-18">
         <a href="#tocAnchor-1-18"><span class="tocnumber">4</span> <span class="toctext">Material und Links</span></a>
      </li>
      <li class="toc_level-1 toc_section-19">
         <a href="#tocAnchor-1-19"><span class="tocnumber">5</span> <span class="toctext">Übungsbetrieb</span></a>
      </li>
      <li class="toc_level-1 toc_section-20">
         <a href="#tocAnchor-1-20"><span class="tocnumber">6</span> <span class="toctext">Vorlesungsempfehlungen</span></a>
      </li>
      <li class="toc_level-1 toc_section-21">
         <a href="#tocAnchor-1-21"><span class="tocnumber">7</span> <span class="toctext">Termine und Klausurablauf</span></a>
      </li>
   </ul>
</div><h2 id="tocAnchor-1-1">Behandelter Stoff</h2>

<h3 id="tocAnchor-1-1-1">Übersicht</h3>

<table>
<tr>
    <th>Datum</th>
    <th>Kapitel</th>
    <th>Inhalt</th>
</tr>
<tr>
    <td>20.10.2015, 08:00</td>
    <td>Einleitung (Folie 1-26)</td>
    <td>Overfitting, Entscheidungsbäume, 1-Rules (→ Decision Strump), Outliers<br />
        Mengenwertige Attribute, Kategorische Attribute, Zeitreihen<br />
        Clustering<br />
        Market Basket Analysis: Zusammenhang zwischen Waren<br />
        Association rules (Apriori Algorithmus)
    </td>
</tr>
<tr>
    <td>20.10.2015, 11:30</td>
    <td>Einleitung, Statistische Tests (Folie 27 - 43)</td>
    <td>Predictive Maintenance
    </td>
</tr>
<tr>
    <td>27.10.2015, 08:00</td>
    <td>Statistische Tests (Folie 38 - )</td>
    <td>\(\chi^2\)-Test, \(\chi^2 = \sum_{i=1}^{m_1} \sum_{j=1}^{m_2} \frac{(h_{ij}- e_{ij})^2}{e_{ij}}\) mit erwartetem Wert \(e\) (Sind zwei Zufallsvariablen unabhängig)<br />
    Kolmogorov-Smirnov-Test (Sind 2 Verteilungen unabhängig; bei kontinuierlichen Zufallsvariablen)<br />
    Wilcoxon-Wann-Whitney Test<br />
    Bernoulli-Experiment (Folie 53?)<br />
    Datenreduktion (Attribute entfernen, z.B. PCA; Datensätze entfernen, z.B. Clustering; Attributsgenauigkeit reduzieren)<br />
    Diskretisierung: Zielfunktion ist Information Gain. Dieser soll minimiert werden.
    </td>
</tr>
<tr>
    <td>03.11.2015, 08:00</td>
    <td>Räumliche Indexstrukturen</td>
    <td>Widerholung der Statistischen Tests</td>
</tr>
<tr>
    <td>03.11.2015, 11:30</td>
    <td>Entscheidungsbäume, Evaluation (1-18)</td>
    <td>Split-Attribute, Pruning; Loss-Funktionen</td>
</tr>
<tr>
    <td>17.11.2015, 08:00</td>
    <td>Evaluation (19-47)</td>
    <td>Qulitätsmaße (Korrelationskoeffizient)</td>
</tr>
<tr>
    <td>17.11.2015, 11:30</td>
    <td>Evaluation, Association Rules (1-26)</td>
    <td>41 min Evaluation, dann Association Rules. Frequent Itemset, Apriori-Algorithmus</td>
</tr>
<tr>
    <td>24.11.2015, 08:00</td>
    <td>Kapitel 6: Association Rules (12-Ende), Kapitel 7 (1-12)</td>
    <td>Apriori-Algorithmus, Hash-Tree, Multidimensionale Association Rules,
        Level-Crossing-Association Rules, FP-Trees</td>
</tr>
<tr>
    <td>01.12.2015, 08:00</td>
    <td>Kapitel 7, Kapitel 8 (Pattern Mining mit Constraints)</td>
    <td>Korrekturen zu Kapitel "Evaluation"; Wiederholung von Apriori-Algorithmus und Hash-Filter; ab Minute 27 FP-Trees</td>
</tr>
<tr>
    <td>01.12.2015, 11:30</td>
    <td>Kapitel 8 (Pattern Mining mit Constraints), Kapitel 9 (Clustering)</td>
    <td>Meta-Rule-Guided Mining, Anti-Monotonizität, Support-basiertes Pruning, Constrained Sequences, Clustering Criterion Function</td>
</tr>
<tr>
    <td>15.12.2015, 08:00</td>
    <td>Kapitel 9 (Clustering)</td>
    <td>k-means; CF-Trees</td>
</tr>
<tr>
    <td>15.12.2015, 11:30</td>
    <td>Kapitel 9 (Clustering)</td>
    <td>CF-Trees; Hierarchisches Clustern mit Minimum Spanning Tree; DIANA; Hochdimensionale Merkmalsräume</td>
</tr>
<tr>
    <td>08.12.2015, 08:00</td>
    <td>R-Übung</td>
    <td>-</td>
</tr>
<tr>
    <td>19.01.2016, 08:00</td>
    <td>Kapitel 9</td>
    <td>Jaccard Koeffizient, ...</td>
</tr>
<tr>
    <td>19.01.2016, 11:30</td>
    <td>Kapitel 9; Kapitel 10 (1 - )</td>
    <td>EM-Algorithmus; Generative Modelle</td>
</tr>
<tr>
    <td>26.01.2016, 08:00</td>
    <td>Übung</td>
    <td>-</td>
</tr>
<tr>
    <td>26.01.2016, 11:30</td>
    <td>Kapitel 10</td>
    <td>Regression</td>
</tr>
<tr>
    <td>02.02.2016, 08:00</td>
    <td>Kapitel 10</td>
    <td>Logistische Regression, Cross Entropy</td>
</tr>
</table>

<h3 id="tocAnchor-1-1-2">Einleitung</h3>

<p>Slides: <code>1-Einleitung.pdf</code></p>

<dl>
  <dt><dfn>Aufgabentypen</dfn></dt>
  <dd><ul>
      <li>Klassifikation</li>
      <li>Clustering</li>
      <li>Finden von <a href="https://de.wikipedia.org/wiki/Assoziationsanalyse">Association Rules</a></li>
  </ul></dd>
  <dt><a href="https://en.wikipedia.org/wiki/Decision_stump"><dfn>1-Rule</dfn></a> (<dfn>Decision stump</dfn>)</dt>
  <dd>1-Rules ist ein Klassifikationsverfahren. Jedes Attribut wird für sich
      betrachtet. Es wird anhand von dem Attribut gesplittet, bei dem die
      Fehlerquote am geringsten ist.</dd>
  <dt><dfn>Clustering</dfn></dt>
  <dd>Suchen von Punkten, die nahe bei einander liegen.

      Unterschiede:

      <ul>
          <li>Attribute: Abstandsmaße</li>
          <li>Form</li>
          <li>Dichte</li>
          <li>Größe</li>
          <li>Zeitlicher Aspekt: Alte Daten weniger wichtig</li>
          <li>Alternate Clustering</li>
      </ul>
  </dd>
  <dt><dfn>Association Rules</dfn></dt>
  <dd>Association Rules sind Regeln der Form:
      Wenn eine Transaktion A enthält, dann auch B (formal: \(A \Rightarrow B\)).

      Association rules werden z.B. in der Market Basket Analysis eingesetzt.
      Sie können aus Frequent item sets relativ einfach erzeugt werden.

      Der Apriori Algorithmus dient dem Finden von Association Rules.
  </dd>
  <dt><dfn>Predictive Maintenance</dfn></dt>
  <dd>Ziel: Für Motoren will man vorhersagen, wann diese einen Fehler aufweisen
      und damit gewartet werden müssen.

      Dabei gibt es zwei Fehlerarten, die unterschiedliche hohe Kosten
      aufweisen:
      <ul>
          <li>Ausfall wird vorhergesagt, tritt aber nicht ein: Unnötige Wartung</li>
          <li>Ausfall wird nicht vorhergesagt, tritt aber ein: Teurer Ausfall</li>
      </ul>
  </dd>
  <dt><dfn>Change detection</dfn></dt>
  <dd>Erkennung wesentlicher Veränderungen in einer Zeitreihe.</dd>
</dl>

<h3 id="tocAnchor-1-1-3">Statistische Grundlagen</h3>

<p>Slides: <code>2-statistGrundlagen.pdf</code></p>

<dl>
    <dt><dfn>Skalen</dfn></dt>
    <dd>Siehe <a href="https://martin-thoma.com/mustererkennung-klausur/#merkmale">Mustererkennung</a></dd>
    <dt><dfn>Kennzahlen für Daten</dfn></dt>
    <dd>
        <ul>
            <li>Median / Mean</li>
            <li>Min / Max</li>
            <li>Quantile</li>
            <li>Varianz / Streuung</li>
            <li>Outlier</li>
        </ul>
    </dd>
    <dt><dfn>Metrische Daten</dfn></dt>
    <dd>Ein Metrischer Raum ist eine Menge \(M\) mit einer Funktion
        \(d: M \times M \rightarrow \mathbb{R}_0^+\) für die gilt:

        <ul>
            <li>Symmetrie: \(\forall p,q \in M: d(p, q) = d(q, p) \)</li>
            <li>Definitheit: \(\forall p,q \in M: d(p, q) = 0 \Leftrightarrow p = q\)</li>
            <li>Dreiecksungleichung: \(\forall p,q,r \in M: d(p, r) \leq d(p,q) + d(q, r)\)</li>
        </ul>

    </dd>
    <dt><dfn>Aggregatfunktion</dfn></dt>
    <dd>Eine Funktion, welche als Eingabe eine Menge von Werten erwartet und
        einen Wert ausgibt (z.B. SUM, COUNT, MIN, MAX, AVG, MEAN,
        häufigster Wert, Truncated Average, mid range).<br />
        <br />
        Aggregatfunktionen sind entweder <strong>distributiv</strong>,
        <strong>algebraisch</strong> oder <strong>holistisch</strong>.
    </dd>
    <dt><dfn>Distributive Aggregatfunktion</dfn></dt>
    <dd>
        Es gibt eine Funktion \(G\), so dass
        \[F(\{X_{i,j}\}) = G(\{F(X_{i,j} | i=1, \dots, l) | j = 1, \dots, J\})\]

        MIN, MAX und COUNT sind distributive Aggregatfunktionen.
    </dd>
    <dt><dfn>Algebraische Aggregatfunktion</dfn></dt>
    <dd>Es gibt eine Funktion \(G\), die ein \(M\)-Tupel liefert und \(H\),
        so dass
        \[F(\{X_{i,j}\}) = H(\{G(\{X_{i,j} | i=1, \dots, l\}) | j=1, \dots, J\})\]

        AVG ist eine Algebraische Aggregatfunktion. Hier berechnet \(G\) die
        Summe und gibt zusätzlich die Anzahl der Werte zurück. \(H\) summiert
        die Summen auf und teilt das Ergebnis durch die Gesamtzahl.<br />
        <br />
        Weitere: Truncated Average</dd>
    <dt><dfn>Holistische Aggregatfunktion</dfn></dt>
    <dd>Man kann keine Beschränkung des Speicherbedarfs für Sub-Aggregate,
        d.h. Aggregate über \(\{X_{i,j}| i=1, \dots, l\}\), angeben.<br />
        <br />
        Der häufigste Wert und der Median sind holistische Aggregatfunktionen.</dd>
    <dt><dfn>Self-Maintainable Aggregatfunktion</dfn></dt>
    <dd>Wenn man den aktuellen Wert der Aggregatfunktion kennt und man löscht
        einen Wert bzw. fügt einen Wert ein, dann kann man direkt den neuen
        Wert der Aggregatfunktion über den angepassten Datenbestand berechnen.<br />
        <br />
        Nicht-self-maintainable ist der häufigste Wert.<br />
        <br />
        MIN und MAX ist self-maintainable bzgl. Einfügen.</dd>
    <dt><dfn>Mid-Range</dfn></dt>
    <dd>\[\frac{MAX-MIN}{2}\]</dd>
    <dt><dfn>Entropie</dfn></dt>
    <dd>\[E(S) = - \sum_{j} p_j \cdot \log p_j\]

        \(E(S)=0\) ist minimal, wenn es ein \(j\) gibt mit \(p_j = 1\).
        \(E(S)=\log(n)\) ist maximal, wenn \(p_i = p_j\) gilt für \(i, j\).</dd>
    <dt><dfn>Korrelationsmaße</dfn></dt>
    <dd>Sind üblicherweise auf [-1, 1] normiert. Die Kovarianz ist ein
        nicht-normiertes Korrelationsmaß.</dd>
    <dt><a href="https://de.wikipedia.org/wiki/Kovarianz_(Stochastik)#Definition"><dfn>Kovarianz</dfn></a></dt>
    <dd>\[\operatorname{Cov}(X,Y) := \operatorname E\bigl[(X - \operatorname E(X)) \cdot (Y - \operatorname E(Y))\bigr]\]</dd>
    <dt><a name="korrelationskoeffizient" href="https://de.wikipedia.org/wiki/Korrelationskoeffizient#Definitionen" id="korrelationskoeffizient"><dfn>Korrelationskoeffizient</dfn></a></dt>
    <dd>\[\varrho(X,Y) =\frac{\operatorname{Cov}(X,Y)}{\sigma(X)\sigma(Y)} \in [-1, 1]\]</dd>
    <dt><dfn>PCA</dfn> (<dfn>Principal Component Analysis</dfn>)</dt>
    <dd>PCA ist ein Algorithmus zur Reduktion von Daten durch das Entfernen von
        Attributen. Er projeziert die Datenobjekte auf eine Hyperebene, sodass
        ein Maximum der Varianz beibehalten wird (vgl. <a href="https://martin-thoma.com/neuronale-netze-vorlesung/#pca">Neuronale Netze</a>)</dd>
    <dt><a href="https://de.wikipedia.org/wiki/Chi-Quadrat-Test#Unabh.C3.A4ngigkeitstest"><dfn>Chi-Quadrat-Test</dfn></a></dt>
    <dd>Oberbegriff für mehrere Tests; hier nur der Unabhängigkeitstest.<br />
        <br />
        Gegeben sind zwei Verteilungen von Zufallsvariablen \(X, Y\). Die Frage
        ist, ob sie unabhängig sind.<br />
        Dazu zählt man die Ausprägungen \(i=1, \dots, m_1\) des Merkmals \(X\)
        und die Ausprägungen \(j=1, \dots, m_2\) des Merkmals \(Y\) sowie
        wie häufig diese in Kombination auftreten (\(n_{ij}\)). Man schätzt den
        erwarteten Wert durch \(e_{ij} = \frac{1}{n} \left(\sum_{k=1}^{m_2} n_{ik} \right) \cdot \left (\sum_{k=1}^{m_2} n_{kj}\right )\). Der Chi-Quadrat wert ist dann:

        \[\chi^2 = \sum_{i=1}^{m_1} \sum_{j=1}^{m_2} \frac{(n_{ij} - e_{ij})^2}{e_{ij}}\]

        Daraus wird ein \(p\)-Wert abgeleitet. Wenn dieser unter einem
        Schwellwert wie \(\alpha = 0.01\) ist, dann wird die Hypothese, dass
        die Verteilungen unabhängig sind, zurückgewiesen.

        Die Nullhypothese, dass \(X, Y\) unabhängig sind wird auf dem
        Signifikanzniveau \(\alpha\) verworfen, falls

        \[\chi^2 &gt; \chi^2_{(1-\alpha; (m_1-1)(m_2-1))}\]

        </dd>
    <dt><a href="https://de.wikipedia.org/wiki/Kolmogorow-Smirnow-Test"><dfn>Kolmogorow-Smirnow-Test</dfn></a> (<dfn>KSA-Test</dfn>)</dt>
    <dd>Test auf unabhängigkeit kontinuierlicher Verteilungen, also:
        \[H_0: F_X(x) = F_0(x)\]

        Es wird die empirsche Verteilungsfunktion \(S\) gebildet und diese mit
        der hypothetischen Verteilungsfunktion \(F_0\) verglichen, wobei
        \(S(x_0) = 0\) gesetzt wird:
        \[d_{\max} = \max(\max_{i=1, \dots, n}|S(x_i) - F_0(x_i)|, \max_{i=1, \dots, n} |S(x_{i-1} - F_0(x_i))|)\]
        \(H_0\) wird verworfen, wenn \(d_{\max} &gt; d_\alpha\), wobei \(d_\alpha\)
        bis zu \(n=35\) tabelliert vorliegt. Bei großerem \(n\) kann
        näherungsweise
        \[d_\alpha = \sqrt{\frac{-\frac{1}{2} \ln(\frac{\alpha}{2})}{n}}\]
        </dd>
    <dt><a href="https://de.wikipedia.org/wiki/Wilcoxon-Mann-Whitney-Test"><dfn>Wilcoxon-Mann-Whitney-Test</dfn></a> (\(U\)-Test)</dt>
    <dd>Es seien \(X,Y\) Zufallsvariablen mit Verteilungsfunktionen
        \(F_X(x) = F_Y(x-a)\) für ein \(a \in \mathbb{R}\).<br />
        <br />
        \(H_0: a = 0\) vs \(H_1: a \neq 0\)<br />
        Vorgehen: Gemeinsame Stichprobe sortieren, Rangsumme für \(X\) und \(Y\)
        bilden, Betrag der Differenz mit Tabelleneintrag vergleichen.
    </dd>
    <dt><dfn>Datenreduktion</dfn></dt>
    <dd>

        <ul>
            <li>Numerosity Reduction: Reduziere die Anzahl der betrachteten
                Datenobjekte
            <ul>
                <li>Parametrische Verfahren: Nehme eine bekannte
                    Wahrscheinlichkeitsverteilung der Datenobjekte an und
                    schätze deren Paramter. Arbeite  dann nur mit der
                    Verteilung</li>
                <li>Nichtparametrische Verfahren: Sampling, Clustering,
                    Histogramme</li>
            </ul>
            </li>
            <li>Dimensionality Reduction: Reduziere die Anzahl der Attribute.

            <ul>
                <li>Forward Feature Construction: Starte nur mit einem Feature
                    und gebe dem Classifier so lange neue Features, bis die
                    gewünschte Genauigkeit erreicht wurde.</li>
                <li>Feature Elimination: Starte mit allen Features und
                    entferne so lange Features, wie die gewünschte Genauigkeit
                    erhalten bleibt.</li>
                <li>PCA</li>
            </ul>
            </li>
            <li>Diskretisierung: Reduziere die Werte pro Attribut.</li>
        </ul>

    </dd>
</dl>

<p>Weitere</p>

<ul>
    <li>Boxplots: Whiskers</li>
    <li>Histogramme: Nicht geeignet für viele Dimensionen.</li>
    <li>Wahrscheinlichkeitsraum, Ereignis, Ergebnis, Ergebnismenge \(\Omega\),
        Wahrscheinlichkeitsmaß, Kovarianzmatrix, Bernoulli-Experiment</li>
</ul>

<h3 id="tocAnchor-1-1-4">Räumliche Indexstrutkuren</h3>

<p>Slides: <code>3-Informatik-Grundlagen.pdf</code></p>

<dl>
    <dt><dfn>B<sup>+</sup>-Tree</dfn> (see <a href="https://www.youtube.com/watch?v=CYKRMz8yzVU">YouTube</a>)</dt>
    <dd>A balanced search tree.</dd>
    <dt><a href="https://de.wikipedia.org/wiki/Datenbankindex"><dfn>Index</dfn></a></dt>
    <dd>Beschleunigung der Suche von linearer Suchzeit auf logarithmische
        durch <a href="https://de.wikipedia.org/wiki/B%2B-Baum">B<sup>+</sup>-Bäume</a>.</dd>
    <dt><dfn>Anfragetypen</dfn></dt>
    <dd>

        <ul>
            <li>Punkt-Anfragen: Ist ein Punkt im Datensatz?</li>
            <li>Bereichs-Anfragen: Ist mindestens ein Datenobjekt im gegebenen Bereich?</li>
            <li>Nearest-Neighbor-Anfragen (NN-Anfragen): Was ist das nächste Datenobjekt zu einem gegebenen Punkt?</li>
        </ul>

    </dd>
    <dt><dfn>kD-Baum</dfn></dt>
    <dd>Siehe <a href="https://martin-thoma.com/cg-klausur/#kd-tree">Computergrafik</a>.</dd>
    <dt><a href="https://en.wikipedia.org/wiki/K-D-B-tree"><dfn>kDB-Baum</dfn></a></dt>
    <dd>Ein balancierter kD-Baum. Die Balancierung wird durch eine Kombination
        aus heterogenem k-d-Baum und B*-Baum erreicht. Der baum ist also nicht
        auf logischer, sondern nur auf physischer Ebene balanciert.</dd>
    <dt><a name="r-tree" id="r-tree"></a><dfn>R-Baum</dfn></dt>
    <dd>Ein R-Baum ist ein balancierter Baum, welcher die Datenobjekte in
        minimale <abbr title="umhüllende achsenparallele bounding-boxen">AABBs</abbr>
        einschließt. Jeder Knoten hat eine solche AABB und jedes der Kinder -
        egal ob es wieder ein AABB oder Datenpunkte sind - ist darin.
        Diese AABBs können sich überschneiden.<br />
        <br />
        TODO <a href="http://cs.stackexchange.com/q/56337/2914">What is the difference between a R-tree and a BVH?</a></dd>
    <dt><dfn>Nearest Neighbor in R-Tree</dfn></dt>
    <dd>Siehe <a href="https://github.com/MartinThoma/algorithms/blob/master/nearest-neighbor-r-tree/nn_r_tree_pseudo.py">Pseudo-Code</a>.</dd>
</dl>

<h3 id="tocAnchor-1-1-5">Entscheidungsbäume</h3>

<p>Slides: <code>4-Entscheidungsbaeume.pdf</code></p>

<p>Dieses Kapitel beschäftigt sich mit der Klassifikation mit Entscheidungsbäumen.</p>

<dl>
    <dt><dfn>Qualitätskriterien für Entscheidungsbäume</dfn></dt>
    <dd>

        <ul>
            <li>Ergebnisqualität</li>
            <li>Kompaktheit: Je kompakter der Baum, desto besser kann die
                Entscheidung vom Benutzer nachempfunden werden.</li>
        </ul>

    </dd>
    <dt><dfn>Wahl der Split-Attribute</dfn></dt>
    <dd>Entropie eines Splits minimieren:

    \[E(S_1, S_2) = \frac{n_1}{n} E(S_1) + \frac{n_2}{n} E(S_2)\]

    </dd>
    <dt><dfn>Overfitting</dfn></dt>
    <dd>Entscheidungsbaum ist zu sehr an Trainingsdatenbestand angepasst</dd>
    <dt><dfn>Prepruning</dfn> (<dfn>Forward pruning</dfn>)</dt>
    <dd>Schon beim Erstellen des Entscheidungsbaumes wird ab einer gewissen
        Tiefe abgebrochen</dd>
    <dt><dfn>Postpruning</dfn> (<dfn>Backward pruning</dfn>)</dt>
    <dd>Der Entscheidungsbaum wird komplett aufgebaut, aber dannach wird
        greprunt.</dd>
</dl>

<h3 id="tocAnchor-1-1-6">Evaluation</h3>

<p>Slides: <code>5-Evaluation.pdf</code></p>

<dl>
    <dt><dfn>Resubsitution Error</dfn></dt>
    <dd>Trainingsfehler</dd>
    <dt><a name="cross-validation" id="cross-validation"></a><dfn>\(k\)-Fold Cross-Validation</dfn> (<dfn>Kreuzvalidierung</dfn>)</dt>
    <dd>Unterteile den Datensatz in \(k\) Teile. Dabei sollten die Klassen in
        etwa gleich häufig in allen Teilen vorkommen.

        Mache nun \(k\) durchläufe, wobei der \(k\)-te Datensatz immer zum
        Testen und alle anderen zum Trainieren verwendet werden. Berechne die
        \(k\) Testfehler. Mittle diese am Ende. Das ist ein besserer Schätzwert
        für den realen Fehler als eine einmalige Unterteilung in Training- und
        Testmenge.</dd>
    <dt><dfn>Stratification</dfn></dt>
    <dd>Sicherstellen, dass bestimmte Eigenschaften (z.B. Klassenzugehörigkeit) in Partitiionen etwa gleich verteilt ist.</dd>
    <dt><dfn>Loss function</dfn></dt>
    <dd>Eine Funktion, die angibt, wie viel man durch eine unkorrekte
        Vorhersage verliert.</dd>
    <dt><dfn>Informational Loss function</dfn></dt>
    <dd>\[- \log_2 p_i\] - Wahrscheinlichkeiten der nicht-eintretenden Klassen spielen keine Rolle</dd>
    <dt><dfn>Quadratic Loss function</dfn></dt>
    <dd>\[\sum_{j} (p_j - a_j)^2\] mit tatsächlichem Label \(a_j \in \{0,1\}\)
        und geschätzter Wahrscheinlichkeit \(p_j\) für die Klasse \(j\).</dd>
    <dt><dfn>Bias</dfn></dt>
    <dd>Das Verfahren an sich funktioniert nicht gut. Selbst beliebig viele
        Trainingsdaten beheben dieses Problem nicht. Der Fehler ist
        inhärent im Verfahren verankert.</dd>
    <dt><dfn>Varianz</dfn></dt>
    <dd>Fehler welcher durch das Fehlen von Trainingsdaten verursacht wird.</dd>
    <dt><a name="erfolgsquote" id="erfolgsquote"></a><dfn>Gesamt-Erfolgsquote</dfn></dt>
    <dd>\[\frac{TP+TN}{TP+TN+FP+FN}\]</dd>
    <dt><dfn>Konfusionsmatrix</dfn> (<dfn>Confusion matrix</dfn>)</dt>
    <dd>Eine Tabelle, in der jede Zeile für die tatsächlichen Klassen stehen
        und die Spalten für die vorhergesagten Klassen. Die Diagonalelemente
        zählen also die richtig vorhergesagten Datenobjekte; alle anderen
        Zellen zählen falsche Vorhersagen.</dd>
    <dt><dfn>Kappa-Koeffizient</dfn></dt>
    <dd>Vergleich mit Klassifier, der nur den Anteil der Klassenzugehörigkeit
        schätzt.<br />
        Sei \(D\) die Menge der Datenobjekte, \(K\) die Menge der Klassen,
        \(f: D \rightarrow K\) der Klassifizierer und \(k:D \rightarrow K\) die tatsächliche Klasse des Datenobjekts. Dann gilt:
        Die Menge der richtig klassifizierten Datenobjekte:
        \[R = |\{1|d \in D, f(d) = k(d)\}\|\]

        Die Datenobjekte \(Z\), die ein zufälliger Klassifizierer richtig raten
        würde.
        \[\kappa(f, D) = \frac{R - Z}{|D|- Z}\]
        Der Wertebereich ist also: \((-\infty; 1]\) (TODO: stimmt das?),
        da \(R &lt; |D|\)</dd>
    <dt><dfn>Lift-Faktor</dfn></dt>
    <dd>Faktor, um den sich die Rücklaufquote erhöht.</dd>
    <dt><a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic"><dfn>ROC</dfn></a> (<dfn>Receiver Operator Characteristic</dfn>)</dt>
    <dd>x-Achse: \(\frac{FP}{FP+TN} \cdot 100\) (FP-Rate),<br />
        y-Achse: \(\frac{TP}{TP+FN} \cdot 100\) (TP-Rate)

        Siehe auch: <a href="https://www.reddit.com/r/answers/comments/4g2wgx/where_does_the_name_receiver_operating/">Namensherkunft</a></dd>
    <dt><dfn>Recall</dfn> (<dfn>True Positive Rate</dfn>, <dfn>TPR</dfn>, <dfn>Sensitivität</dfn>)</dt>
    <dd>\[TPR = \frac{TP}{TP + FN} = 1 - FNR \in [0, 1]\]

        Der Recall gibt den Anteil der erkannten positiven aus allen positiven
        an.

        <i>Sensitivität</i> ist ein in der Medizin üblicher Begriff.</dd>
    <dt><dfn>Precision</dfn> (<dfn>Genauigkeit</dfn>)</dt>
    <dd>\[Precision = \frac{TP}{TP + FP} \in [0, 1]\]

        Die Precision gibt den Anteil der real positiven aus den als positiv
        erkannten an.</dd>
    <dt><a href="https://en.wikipedia.org/wiki/F1_score"><dfn>F-Measure</dfn></a> (<dfn>F1 score</dfn>)</dt>
    <dd>\[\frac{2 \cdot \text{precision} \cdot \text{recall}}{\text{recall} + \text{precision}}\]</dd>
    <dt><dfn>Correlation Coefficient</dfn></dt>
    <dd>Der Correlation Coefficient ist kein Fehlermaß. Der
        \(CC(p, a)\) ist groß, wenn sich \(p\) und \(a\) ähnlich sind.

        \[CC(p, a) = \frac{COV(p, a)}{\sigma(p) \cdot \sigma(a)}\]

        Mit \(\sigma(x) = \frac{1}{n-1} \cdot \sum_{i} (x_i - \bar{x})^2\)</dd>
    <dt><dfn>Code</dfn></dt>
    <dd>Abbildung, die jedem Element des Alphabets eine Folge aus 0en und
        1en zuweist.

        Beispiele:

        <ul>
            <li>Morse-Code</li>
            <li>Unicode</li>
            <li>Ascii-Code</li>
        </ul></dd>
    <dt><dfn>Minimum Description Length</dfn> (<dfn>MDL</dfn>)</dt>
    <dd>Minimale Länge zum Beschreiben des Modells.</dd>
</dl>

<p>Weiteres:</p>

<ul>
  <li>Qualitätsmaße für numerische Vorhersagen</li>
</ul>

<p>Fragen:</p>

<ul>
  <li>Folie 23: Wo kommt die 140 her?<br />
→ Summe der Diagonalelemente auf Folie 21.</li>
  <li>TODO, Folie 27: Lift Faktor ist 2 wenn man nur die 400 anschreibt, oder?</li>
</ul>

<h3 id="tocAnchor-1-1-7">Association Rules</h3>

<p>Slides: <code>6-Association-Rules-1.pdf</code> und <code>7-Association-Rules-2.pdf</code></p>

<p>Es zieht sich die Warenkorbanalyse als Beispiel durch. Allerdings sind folgende
Anwendungen von Association Rules denkbar:</p>

<ul>
  <li>Netflix: Man kennt User und welche Filme diese mögen (5 Sterne). Welche
weiteren, unbewerteten Filme könnten diesen gefallen?</li>
  <li>Amazon: Im Warenkorb sind Produkte XY. Was wird der User wohl noch kaufen?</li>
  <li>Online-Konfiguratoren: Welche Konfigurationen sollte man “bündeln”, z.B. bei
Autos in eine “Sport-Variante”?</li>
  <li>Last FM: Music Recommendations</li>
  <li>Medicine: <a href="http://static.ijcsce.org/wp-content/uploads/2013/12/IJCSCE110513.pdf">Implementation of Apriori Algorithm in Health Care Sector: A Survey</a></li>
</ul>

<dl>
    <dt><dfn>Frequent Itemset</dfn></dt>
    <dd>Ein Frequent Itemset ist eine Menge von Items, die häufig zusammen
        gekauft werden.</dd>
    <dt><dfn>Transaktion</dfn> (<dfn>Itemset</dfn>)</dt>
    <dd>Menge von Items, die zusammen gekauft wurden.</dd>
    <dt><dfn>Association rules</dfn></dt>
    <dd>Drücken aus wie Phänomene zueinander in Beziehung stehen.

        Beispiel: Wer Bier kauft, der kauft auch Chips.</dd>
    <dt><dfn>Support</dfn></dt>
    <dd>Anzahl der Transaktionen, die das Itemset I unterhalten wird Support von I genannt.<br />
        Der \(\text{support}(A \cup B)\) ist die Anzahl der Mengen, die \(A \cup B\) enthalten.</dd>
    <dt><dfn>Closed Itemset</dfn></dt>
    <dd>Ein Itemset \(I\) heißt closed, wenn es keine echte Obermenge \(I' \supsetneq I\) gibt,
        die den gleichen support \(\text{supp}(I') = \text{supp}(I)\) hat.</dd>
    <dt><dfn>Confidence</dfn></dt>
    <dd>Confidence von \(A \Rightarrow B\) ist \(\frac{Support(A \cup B)}{support(A)}\)</dd>
    <dt><dfn>Apriori Algorithmus</dfn></dt>
    <dd>Der Apriori-Algorithmus ist ein Generate-and-Test-Algorithmus zum
        Finden von Frequent Itemsets.

        <ol>
            <li>Erzeuge alle einelementigen Frequent Itemsets</li>
            <li>for k in range(2, n): Erzeuge die \(k\)-elementigen frequent
                Itemsets (join, prune, support counting)</li>
            <li>Frequent itemsets: Association Rules</li>
        </ol>

        Der Algorithmus nutzt aus, dass eine notwendige Bedingung für
        \(k\)-elementige Frequent Itemsets ist, dass alle \(k-1\)-elementigen
        Frequent Itemsets auch Frequent sein müssen.

        Verbesserungen:

        <ul>
            <li>Stichproben verwenden (Sampling)</li>
            <li>Aggressiver durch Datenbestand gehen (z.B. von k=3 zu k=6 springen)</li>
            <li>Hashfilter</li>
        </ul>
    </dd>
    <dt><dfn>FP-Trees</dfn></dt>
    <dd>FP-Trees (FP für "frequent pattern") sind eine Datenstrutkur zum
        schnellen Finden von Frequent Itemsets. Jeder Knoten im Baum
        repräsentiert ein Item. Jeder Knoten speichert zusätzlich die
        Häufigkeit des Präfixes, welcher durch den Pfad von der Wurzel zu dem
        Knoten dargestellt wird. Zusätzlich speichert jeder Knoten des Items
        \(i\) einen Zeiger auf einen anderen Knoten mit einem Item \(i\). Jede
        Transaktion entspricht einem Pfad im FP-Tree.<br />
        Zusätzlich zum FP-Tree gibt es eine Header-Tabelle. Die Zeilen dieser
        Tabelle sind einzelne Items \(i\), denen jeweils ein Zeiger auf einen
        Knoten im FP-Tree zugeordnet sind, der auch das Item \(i\)
        repräsentiert.<br />
        Für jedes Item gibt es also eine verkettete Liste, die das Vorkommen im
        Baum angibt.<br />

        Zum Finden von Frequent Items geht man also wie folgt vor:
    <ol>
        <li>Für jedes Item: Zähle in wie vielen Transaktionen das Item vorkommt.</li>
        <li>Sortiere Items in Transaktion absteigend nach Häufigkeit. Bei
            gleicher Häufigkeit wird z.B. alphabetisch sortiert. Damit ergibt
            sich eine eindeutige Reihenfolge.</li>
        <li>Sortiere Transaktionen nach den Items innerhalb der Transaktionen.</li>
        <li>Aufbau des FP-Trees
        <ol>
            <li>Aufbau des Baums</li>
            <li>Aufbau der Header-Tabelle: Absteigend eindeutig nach Häufigkeit
                sortiert</li>
        </ol>
        </li>
        <li>Starte mit dem niedrigsten Element in der Header-Tabelle. Überprüfe
            den Präfix auf den erwarteten support. Gehe dazu alle Elemente
            dieses Items durch (alle Präfix-Pfade im Baum) und wende eine Art
            Apriori-Algorithmus an um in diesen Präfix-Pfaden mit dem Item
            \(i\) die Frequent-Itemsets zu finden.</li>
    </ol>

    </dd>
    <dt><dfn>Sampling</dfn></dt>
    <dd>Berechnung auf einer Stichprobe durchführen</dd>
    <dt><dfn>Negative Border</dfn></dt>
    <dd>Die negative border ist abhängig vom minimalen geforderten Support.
        Wenn dieser Schwellenwert größer wird, wandert die negative border
        nach oben; es gibt also weniger frequent Itemsets.</dd>
    <dt><dfn>Projected Database</dfn></dt>
    <dd>Zerlegung des Datenbestands in Partitionen (z.B. Transaktionen mit Item A
        und Transaktionen ohne Item A).</dd>
</dl>

<p>Siehe auch:</p>

<ul>
  <li><a href="http://www.salemmarafi.com/code/market-basket-analysis-with-r/comment-page-1/">Market Basket Analysis with R</a></li>
  <li><a href="https://www.knime.org/knime-applications/market-basket-analysis-and-recommendation-engines">Market Basket Analysis and Recommendation Engines</a></li>
</ul>

<h3 id="tocAnchor-1-1-8">Constraints</h3>

<p>Slides: <code>8-ConstrainedAssociationRules.pdf</code></p>

<dl>
    <dt><dfn>Constraint-Typen</dfn></dt>
    <dd>
        <ul>
            <li>Data Constraints: Einschränken auf konkrete Werte, z.B. Transaktionen bei denen der Ort Karlsruhe ist.</li>
            <li>Rule Constraints: z.B. nur Itemsets der Größe 3</li>
        </ul>
    </dd>
    <dt><dfn>1-var Constraint</dfn></dt>
    <dd>Nur eine Seite (links oder rechts) der Association Rule wird
        eingeschränkt.</dd>
    <dt><dfn>2-var Constraint</dfn></dt>
    <dd>Beide Seiten (links und rechts) der Association Rule werden
        eingeschränkt.</dd>
    <dt><dfn>Anti-Monotonizität</dfn></dt>
    <dd>Ein 1-var Constraint heißt anti-monoton, wenn für alle Mengen \(S, S'\)
        gilt:

        \[(S \supseteq S' \land (S \text{ erfüllt } C )) \Rightarrow S' \text{ erfüllt } C\]

        Wenn also ein Constraint \(C\) für eine Menge \(S\) erfüllt ist, dann
        auch für jede Teilmenge \(S'\).

        Beispiele:
        <ul>
            <li>\(\min(S) \geq v, \;\;\; v \in \mathbb{R}\) ist anti-monoton</li>
            <li>\(\max(s) \geq v, \;\;\; v \in \mathbb{R}\) ist nicht anti-monoton</li>
            <li>\(\text{size}(s) \leq v, \;\;\; v \in \mathbb{N}\) ist anti-monoton</li>
            <li>\(\text{size}(s) \geq v, \;\;\; v \in \mathbb{N}\) ist nicht anti-monoton</li>
        </ul>

        Eine gutartige Eigenschaft von Constraints. Hier kann das
        Constraint sehr früh überprüft werden.</dd>
    <dt><dfn>Succinctness</dfn></dt>
    <dd>Ein Constraint heißt succinct, wenn alle Itemsets die es erfüllen
        schnell erzeugt werden können.<br />
        <br />
        Beispiel: Man hat das Constraint, dass der Typ "Non-Food" sein soll.
        Aber es gibt nur 3 Produkte die diesen Typ haben. Kandidaten, die
        das Constraint nicht erfüllen werden gar nicht erst erzeugt.</dd>
</dl>

<ul>
  <li>Meta-Rule Guided mining</li>
  <li>Constraint durch schwächeres Anti-Monotones Constraint ersetzen.</li>
</ul>

<h3 id="tocAnchor-1-1-9">Clustering</h3>

<p>Slides: <code>9-Clustering-1.pdf</code> und <code>9-Clustering-2.pdf</code></p>

<dl>
    <dt><dfn>Silhouette-Koeffizient</dfn></dt>
    <dd>Sei \(C = (C_1, \dots, C_k)\) ein Clustering.

    <ul>
        <li>Durchschnittlicher Abstand zwischen Objekt o und anderen Objekten in seinem Cluster:
            \[a(o) = \frac{1}{|C(o)|} \sum_{p \in C(o)} dist(o, p)\]</li>
        <li>Durchschnittlicher Abstand zum zweitnächsten Cluster:
            \[b(o) = \min_{C_i \in \text{Cluster} \setminus C(o)}(\frac{1}{C_i}) \sum_{p\in C_i} \sum_{p \in C_i} \text{dist}(o, p)\]</li>
        <li>Silhouette eines Objekts:
            \[s(o) = \begin{cases}0  &amp;\text{if } a(o) = 0, \text{i.e. } |C_i|=1\\
                    \frac{b(o)-a(o)}{\max(a(o), b(o))} &amp;\text{otherwise}\end{cases}\]
            Es gilt:
            \[s(o) \in [-1, 1]\]</li>
        <li>\(\text{silh}(C) = \frac{1}{|C|} \sum_{C_i \in C} \frac{1}{|C_i|} \sum_{o \in C_i} s(o)\).
            Es gilt:
            \[\text{silh}(C) \in [-1; 1]\]
            Es ist ein möglichst großer Wert gewünscht. Alles kleiner als 0 ist schlecht.</li>
    </ul>
    </dd>
    <dt><dfn>Distanzfunktionen für Cluster</dfn></dt>
    <dd>
        Seien \(X, Y\) Cluster.

        <ul>
            <li>Durschnittlicher Objektabstand: \(\text{dist}_{avg}(X, Y) = \frac{1}{|X| \cdot |Y|} \cdot \sum_{x in X, y\in Y} \text{dist}(x, y)\)</li>
            <li>Single Link: \(\text{dist}_{sl}(X, Y) = \min_{x \in X, y \in Y} \text{dist}(x, y)\)</li>
            <li>Complete Link: \(\text{dist}_{cl}(X, Y) = \max_{x \in X, y \in Y} \text{dist}(x, y)\)</li>
        </ul>
    </dd>
    <dt><dfn>\(k\)-means Clustering</dfn></dt>
    <dd>Siehe <a href="https://martin-thoma.com/machine-learning-1-course/#tocAnchor-1-1-15">ML 1</a>.</dd>
    <dt><dfn>CLARANS</dfn></dt>
    <dd>CLARANS ist ein Clustering-Algorithmus, der mit \(k\)-Means
        zusammenhängt. Auch er erwartet einen Parameter \(k \in \mathbb{N}\),
        der die erwartete Anzahl an Clustern angibt. Dann geht CLARANS davon
        aus, dass jeder Medeoid durch einen Datenpunkt im Datensatz
        repräsentiert werden kann. Für eine zufällige Wahl von \(k\) Punkten
        \(M = \{p_1, \dots, p_k\}\) wird ein Score berechnet. Dann überprüft
        man, was der Tausch eines Punktes \(p_i\) durch den Punkt \(p_j\)
        für beliebige \(p_i \in M\) und \(p_j \notin M\) am Score ändern würde.
        Den besten Tausch führt man durch.</dd>
    <dt><dfn>CF-Tree</dfn> (<dfn>Clustering Feature Tree</dfn>)</dt>
    <dd>Ein CF-Tree ist ein höhenbalancierter Baum. Jeder Knoten des Baums
        entspricht ein Cluster.</dd>
    <dt><a href="https://en.wikipedia.org/wiki/BIRCH"><dfn>BIRCH</dfn> (<dfn>Balanced Iterative Reducing and Clustering using Hierarchies</dfn>)</a></dt>
    <dd>KEIN hierarchisches Clustering ("hierarchies" bezieht sich auf den Baum, nicht auf das Clusteringergebnis)

    Clustering-Feature (N, LS, SS) für Cluster \(C_i\) mit
    <ul>
        <li>\(N = |C_i|\): Anzahl der Punkte im Cluster</li>
        <li>\(LS = \sum_{i \in C_i} X_i\)</li>
        <li>\(SS = \sum_{i \in C_i} X_i^2\)</li>
    </ul>

    Parameter:

    <ul>
        <li>B: (Fan-out), maximale anzahl an children</li>
        <li>B': maximale Blatt-Kapazität (Anzahl Elementarcluster)</li>
        <li>T (threshold): Maximaler Radius (durchmesser?), bevor er gesplittet wird</li>
    </ul>

    </dd>
    <dt><dfn>Hierarchisches Clustering</dfn></dt>
    <dd>Beim hierarchischen Clustern werden Datenpunkte Baumartig zu Clustern
        zusammengefasst. Das ganze sieht einem Abstammungsbaum der Arten in der
        Biologie sehr ähnlich.<br />
        <br />
        Es gibt zwei Vorgehensweisen:
        <ul>
            <li><a href="#agglomerative-clustering">Agglomorativ</a></li>
            <li><a href="#divisive-clustering">Divisives Clustering</a></li>
        </ul>

        <a href="https://de.wikipedia.org/wiki/Hierarchische_Clusteranalyse#Dendrogramm">Dendrogramme</a> sind eine typische Visualisierung für das Ergebnis einer
        hierarchischen Clusteranalyse.

    </dd>
    <dt><dfn>Probabilistisches Clustering</dfn></dt>
    <dd>Datenobjekte werden nicht hart zu einem Cluster zugeordnet sondern
        weich (also mit einer gewissen Wahrscheinlichkeit) jedem Cluster
        zugeordnet.</dd>
    <dt><dfn>Zentrum eines Centroids</dfn></dt>
    <dd>\[Z_{i} = \frac{1}{|C_i|} \sum_{i \in C_i} X_i\]</dd>
    <dt><dfn>Radius eines Centroids</dfn></dt>
    <dd>\[R(C_i) = \sqrt{\frac{1}{|C_i|} \sum_{j \in C_i} {(X_j - Z_i)}^2}\]</dd>
    <dt><dfn>Durchmesser eines Centroids</dfn></dt>
    <dd>\[D(C_i) = \sqrt{\frac{1}{|C_i| \cdot (|C_i|-1)} \sum_{j \in C_i} \sum_{k \in C_i} {(X_j - X_k)}^2}\]</dd>
    <dt><dfn>Interclusterdistanz</dfn></dt>
    <dd>Durchschnittliche Inter-Clusterdistanz von Cluster 1 und Cluster 2:

        \[D(C_1, C_2) = \sqrt{\frac{\sum_{i \in C_1} \sum_{j \in C_2} {(X_i - X_j)}^2}{|C_1| \cdot |C_2|}}\]</dd>
    <dt><a name="agglomerative-clustering" id="agglomerative-clustering"></a><dfn>Agglomeratives Clustering</dfn></dt>
    <dd>

        <ul>
            <li>Jedes Objekt ist ein Cluster. Füge die Cluster in die Menge \(M\) ein.</li>
            <li>Berechne alle paarweise Abstände zwischen Clustern in \(M\). Das ist in \(\mathcal{O}(|M|^2)\).</li>
            <li>Merge das Paar \(A, B\) mit kleinstem Abstand zu \(C = A \cup B\). Entferne \(A, B\) aus \(M\) und füge \(C\) ein.</li>
            <li>Abbruch, wenn \(|M| = 1\)</li>
            <li>Gehe zu Schritt 2.</li>
        </ul>

        Gesamtkomplexität: \(\mathcal{O}(n^2)\)
    </dd>
    <dt><a name="divisive-clustering" id="divisive-clustering"></a><dfn>Divisives Clustering</dfn> (<dfn>DIANA</dfn>)</dt>
    <dd>Divisives Clustering ist ein hierarchisches Clusteringverfahren. Es
        startet mit einem großen Cluster und unterteilt diesen rekursiv immer
        weiter in je zwei kleine Cluster.<br />
        <br />
         TODO (Splinter group)
    </dd>
    <dt><dfn>Projected Clustering</dfn></dt>
    <dd>Input sind die Anzahl \(k\) der Cluster, die gefunden werden sollen und
        die durchschnittliche Anzahl der Dimensionen pro Cluster \(l\).

        Output ist eine Partitionierung der Daten in \(k+1\) Mengen</dd>
    <dt><dfn>Manhatten Segmental Distance</dfn></dt>
    <dd>\(d(x_1, x_2) = \frac{1}{n} \cdot \sum_{i=1}^n |x_1^{(i)} - x_2^{(i)}|\) wobei
        \(n\) die Anzahl der Dimensionen von \(x_1, x_2\) ist.</dd>
    <dt><dfn>Jaccard Koeffizient</dfn></dt>
    <dd>\[J(A, B) = \frac{|A \cap B|}{|A \cup B|} \in [0; 1]\]</dd>
    <dt><dfn>DB-Scan</dfn></dt>
    <dd>

    Unterscheidet:

    <ul>
        <li>Dichte Objekte: Epsion-Umgebung hat viele Datenobjekte.</li>
        <li>Dichte-erreibare Objekte: In Epsilon-Umgebung von dichten Objekt.</li>
        <li>Ausreißer: Weder dicht noch dichte-erreichbar.</li>
    </ul>

    Idee: Gehe über alle Punkte \(p \in P\) genau ein mal. Sei \(P' = P\) die
    Menge der nicht-markierten Punkte. Solange es noch
    </dd>
    <dt><dfn>Noise</dfn></dt>
    <dd>Noise sind Punkte, die zu keinem Cluster gehören.</dd>
    <dt><dfn>Outlier</dfn></dt>
    <dd>Noise, welcher weit von jedem Objekt entfernt ist.</dd>
    <dt><dfn>Core-Distanz</dfn></dt>
    <dd>\(C(o) = \min\{\varepsilon \in \mathbb{R} | o \text{ ist mit DBSCAN und } \varepsilon \text{ dicht}\}\).<br />
        Die Core-Distanz eines Objekts \(o\) ist also die kleinste Distanz, sodass
        \(o\) noch ein dichtes Objekt ist.</dd>
    <dt><dfn>Reachability-Distanz</dfn></dt>
    <dd>\[\text{reach\_d}() = \begin{cases}d(p, o)               &amp;\text{if } d(p, o) &gt; \text{coreDist}(p, o)\\
                                 \text{coreDist}(p, o) &amp;\text{if } d(p, o) &lt; \text{coreDist}(p, o)\end{cases}\]</dd>
    <dt><a href="https://de.wikipedia.org/wiki/OPTICS"><dfn>OPTICS</dfn></a></dt>
    <dd>OPTICS ist ein Algorithmus, der mit den Parametern min_points und
        epsilon (Radius für Cluster-Distanz) automatisch Cluster findet.

        <ul>
            <li>ControlList (Priority Queue) enthält nur Objekte, die noch
                nicht in der Output-Liste sind.</li>
            <li>Kriterium: Minimale reachability-distanz zu Objekten in der
                Output-Liste.</li>
            <li>Rekursiv expandieren wie bei DB-SCAN.</li>
        </ul>

        Siehe <a href="http://www.dbs.informatik.uni-muenchen.de/Publikationen/Papers/OPTICS.pdf">OPTICS: Ordering Points To Identify the Clustering Structure</a>.
    </dd>
    <dt><dfn>Reachability-Plot</dfn></dt>
    <dd>TODO</dd>
    <dt><dfn>EM-Algorithmus</dfn> (<dfn>Expectation Maximization</dfn>)</dt>
    <dd>Siehe <a href="https://martin-thoma.com/machine-learning-2-course#em-algorithmus">ML 2</a>.</dd>
    <dt><dfn>Overall Likelihood</dfn></dt>
    <dd>Die Overall Likelihood ist ein Gütemaß für Clusterings.
        \[\prod_{i} \left ( p_A P(x_i | A) + p_B P(x_i | B) \right )\]</dd>
</dl>

<h3 id="tocAnchor-1-1-10">Statistische Modellierung</h3>

<p>Slides: <code>10-StatistModellierung.pdf</code></p>

<dl>
    <dt><dfn>Naive Baies</dfn></dt>
    <dd>\[P(H | E) = \frac{P(E_1 | H) \cdot \dots \cdot P(E_n | H) \cdot P(H)}{P(E)}\]</dd>
    <dt><dfn>Laplace-Smoothing</dfn></dt>
    <dd>Um Wahrscheinlichkeiten von 0 zu vermeiden, werden die Zähler mit \(k\) initilisiert.
        Beachte, dass man auch die Gesamtzahl dann um \(k\) erhöhen muss.</dd>
    <dt><dfn>Bayessche Netze</dfn></dt>
    <dd>Ein bayessches Netz ist ein <abbr title="Directed Acyclical Graph">DAG</abbr>.
        Ein Knoten für jedes Attribut sowie Klassenzugehörigkeit.
        Kanten zwischen nicht unabhängigen Attributen.<br />
        <br />
        Netzkonstruktion: Meist von Hand (z.B. anhand von Kausalitäten)<br />
        <br />
        Finden der Maximum-Likelihood-Parameter.<br />
        <br />
        Behandeln fehlender Werte</dd>
    <dt><dfn>Duplikateleminierung</dfn></dt>
    <dd>Spezialfall von Klassifikation</dd>
    <dt><dfn>Versteckte Variablen</dfn></dt>
    <dd>Abstraktion, damit der Raum der zu betrachteten Variablen bei Bayesschen Netzen kleiner wird.</dd>
</dl>

<p>Siehe auch:</p>

<ul>
  <li><a href="http://datascience.stackexchange.com/q/10064/8820">Is the direction of edges in a Bayes Network irrelevant?</a></li>
</ul>

<h3 id="tocAnchor-1-1-11">Support Vector Machines</h3>

<p>Slides: <code>11-SupportVectorMachines.pdf</code></p>

<dl>
    <dt><dfn>Lineare Regression</dfn></dt>
    <dd>Model \(y = M x\), wobei \(x \in \mathbb{R}^n\) die Features sind,
        \(y \in \mathbb{R}^m\) die Vorhersage und \(M \in \mathbb{R}^{n \times m}\)
        die Modellparameter.</dd>
    <dt><dfn>Cross Entropy Fehlermaß</dfn></dt>
    <dd>\[E_{CE}(w) = \sum_{i=1}^n [(1-y_i) \cdot \log (1-p) + y_i \cdot \log p]\]</dd>
    <dt><dfn>SVM</dfn> (<dfn>Support Vector Machine</dfn>)</dt>
    <dd>See <a href="https://martin-thoma.com/svm-with-sklearn/">SVM article</a>.</dd>
</dl>

<h3 id="tocAnchor-1-1-12">Ensembles</h3>

<p>Slides: <code>12-Ensembles.pdf</code> (vgl. <a href="https://martin-thoma.com/machine-learning-1-course/#boosting">ML 1</a>)</p>

<dl>
    <dt><dfn>Ensembles</dfn></dt>
    <dd>Mehrere Instanzen auf Trainingsdaten trainieren.

    Vorteile:

    <ul>
        <li>Overfitting wird minimiert.</li>
        <li>Besseres Gesamtsystem</li>
        <li>Parallelisierbarkeit</li>
        <li>Wahrscheinlichkeiten können genauer geschätzt werden</li>
    </ul>

    Typische Techniken sind Bagging und Boosting.</dd>
    <dt><dfn>Bagging</dfn></dt>
    <dd>Ensemble-Learning Technik, bei der Stichproben des
        Trainingsdatenbestandes für die Classifier verwendet werden.
    </dd>
    <dt><dfn>MetaCost</dfn></dt>
    <dd>MetaCost ist ein Verfahren zum Relabeling (TODO: Was ist Relabeling?).
        MetaCost wendet Bagging an.
    </dd>
    <dt><dfn>Boosting</dfn></dt>
    <dd>Boosting ist eine Ensemble-Learning-Technik, die mehrere Modelle vom
        gleichen Typ durch Voting / Durchschnittsberechnung kombiniert. Dabei
        nimmt Boosting Rücksicht auf zuvor falsch Klassifizierte Beispiele
        und gewichtet diese stärker.

        Gewichtungsänderung für korrekte Objekte bei Fehllerrate e: \(\frac{e}{1-e}\)</dd>
</dl>

<h2 id="tocAnchor-1-14">Prüfungsfragen</h2>

<ul>
  <li>Was ist Overfitting?<br />
→ Siehe <a href="https://martin-thoma.com/machine-learning-1-course/#overfitting">ML 1</a></li>
  <li>Wie berechnet man die Covarianz zweier Zufallsvariablen <span>\(X, Y\)</span>?<br />
→ <span>\(\operatorname{Cov}(X,Y) := \operatorname E\bigl[(X - \operatorname E(X)) \cdot (Y - \operatorname E(Y))\bigr]\)</span></li>
  <li>Warum muss man für NN-Anfragen mit kD-Bäumen nur ein paar Rechtecke anschauen?<br />
→ Weil man mit der Priority-Queue Algorithmus nur Rechtecke betrachten muss,
  die von der Sphäre, welchen durch den Anfragepunkt un den tatsächlichen
  nachsten Nachbarn gebildet wird, geschnitten werden.</li>
  <li>Warum kann man für räumliche Anfragen nicht ohne weiteres auswerten, wenn man
für jede Dimension separat einen B-Baum angelegt hat?<br />
→ TODO: Fragestellung nicht klar. War B-Baum und nicht R-Baum / kdB-Baum gemeint?</li>
  <li>Wie ist ein R-Baum aufgebaut?<br />
→ Siehe <a href="#r-tree">oben</a>.</li>
  <li>Wie funktioniert die Suche nach dem nächsten Nachbarn mit dem R-Baum?<br />
→ Man fügt den Wurzel-Knoten in eine Priority-Queue ein. Die Priority-Queue
  ist eine Min-Queue mit dem Abstand vom Anfragepunkt. Es wird im folgenden
  so lange das höchstpriore Objekt aus der Queue entfernt</li>
  <li>Was ändert sich, wenn die Objekte eine räumliche Ausdehnung haben?<br />
→ Man splittet nach mehreren Dimensionen.</li>
  <li>Stören uns Überlappungen von Knoten des R-Baums? Wenn ja, warum?<br />
→ Ja, weil die Suche nach dem nächsten Nachbarn ineffizienter wird. Es müssen
   gegebenenfalls mehr Knoten betrachtet werden.</li>
  <li>Wie unterscheiden sich R-Baum, kD-Baum und kDB-Baum?<br />
→ R-Bäume partitionieren im gegensatz zu kD- und kDB-Bäumen den Datensatz
  nicht. kDB-Bäume sind im Gegensatz zu kD-Bäumen auf physischer Ebene
  balanciert.</li>
  <li>Wie funktioniert das Einfügen in den R-Baum, inklusive Split?<br />
→ Siehe <a href="https://github.com/MartinThoma/algorithms/blob/master/nearest-neighbor-r-tree/nn_r_tree_pseudo.py#L29">Pseudocode</a></li>
  <li>Was für Anfragen unterstützen die diversen räumlichen Indexstrukturen?<br />
→ Nearest-Neighbor, Bereichsanfragen, Punktanfrage</li>
  <li><code>3-Informatik-Grundlagen.pdf</code>, Folie 19</li>
  <li>Warum werden bei der NN-Suche nur genau die Knoten inspiziert, deren Zonen
die NN-Sphere überlappen?<br />
→ Weil alle anderen Knoten in der Priority Queue weiter hinten liegen.</li>
  <li>Welche Classifier kennen Sie?<br />
→ Decision Stumps (1-Rules), Entscheidungsbäume, SVMs, Neuronale Netze, <span>\(k\)</span>-nearest neighbor (es gibt <a href="https://martin-thoma.com/comparing-classifiers/">mehr Classifier</a>)</li>
  <li>Was ist der Vorteil von Postpruning verglichen mit Prepruning?<br />
→ Es könnte sein, dass ein Feature nur in Kombination mit einem anderen
  deutliche Vorteile bringt. Dies kann man bei Prepruning nicht erkennen,
  ist bei Postpruning gegebenenfalls jedoch offensichtlich.</li>
  <li>Wie baut man einen Entscheidungsbaum auf?<br />
→ Gehe durch alle Attribute. Finde für jedes einzelne Attribut den Wert, der
   die niedrigste Schnitt-Entropie hat. Nehme dann das Attribut als
   Split-Attribut, welches die niedrigste Schnitt-Entropie hat. Fahre so mit
   den beiden Kindknoten fort, bis ein Abbruchkriterium erfüllt ist. Das
   könnte z.B. eine Entropie von 0 oder eine maximale Tiefe sein.</li>
  <li>Wie kann man Overfitting beim Aufbau eines Entscheidungsbaums
berücksichtigen?<br />
→ Prepruning oder Postpruning.</li>
  <li>Wie kann man beim Aufbau des Entscheidungsbaums berücksichtigen, dass
unterschiedliche Fehlerarten unterschiedlich schlimm sind?<br />
→ Mehr Trainingsdaten für den schlimmeren Fehler. (TODO, vgl. <a href="http://datascience.stackexchange.com/q/11379/8820">How can decision trees be tuned for non-symmetrical loss?</a>)</li>
  <li>Was ist Wertebereich der FP-Rate?<br />
→ [0, 1]: Die FP-Rate ist definiert als <span>\(\frac{FP}{FP+TN}\)</span>. Offensichtlich sind alle Werte
nicht-negativ, also kann der Bruch nicht negativ werden. Deshalb ist auch der
Nenner mindestens so groß wie der Zähler. Wenn TN=0 und <span>\(FP \neq 0\)</span>, dann ist die FP-Rate gleich 1. Das geht,
wenn man z.B. immer “True” vorhersagt. Wenn man immer “False” vorhersagt ist
die FP-Rate gleich 0.</li>
  <li>Wie berechnet man den Korrelationskoeffizienten?
→ vgl. <a href="https://martin-thoma.com/analysetechniken-grosser-datenbestaende/#korrelationskoeffizient">oben</a></li>
  <li>Was ist die “10-fold cross validation”?<br />
→ vgl. <a href="https://martin-thoma.com/analysetechniken-grosser-datenbestaende/#cross-validation">oben</a></li>
  <li>Wie haben wir die Erfolgsquote definiert?<br />
→ vgl. <a href="https://martin-thoma.com/analysetechniken-grosser-datenbestaende/#erfolgsquote">oben</a></li>
  <li>Was ist ein Lift Chart?<br />
→ Ein Lift Chart hat auf der x-Achse den Rang (Top-k) und auf der y-Achse der
   Gewinn. Die x-Achse verläuft von 0 bis 100% und die y-Achse von 0 bis zum
   maximalen Gewinn im Datenbestand. Die Diagonale von (0, 0) nach (100%,
   Maximaler Gewinn) entspricht Raten, alles über der Diagonalen ist positiv.
   Der Lift-Chart muss nicht monoton steigend sein.</li>
  <li>Wie unterscheidet sich ein Lift Chart von der ROC Kurve?<br />
→ Die ROC-Kurve ist monoton steigend, der Lift-Chart jedoch nicht.</li>
  <li>Was für Fehlerarten gibt es bei Vorhersagen von Klassenzugehörigkeiten?<br />
→ False-Positive, False-Negative (oder: Konfusionsmatrix)</li>
  <li>Was für Kennzahlen kennen Sie, die diese Fehlerarten sämtlich
berücksichtigen?<br />
→ F score und Gesamtfehler.</li>
  <li>Was ist Unterschied zwischen Kovarianz und dem Korrelationskoeffizienten?<br />
→ Der Korrelationskoeffizient ist normiert (vgl. <a href="https://martin-thoma.com/analysetechniken-grosser-datenbestaende/#korrelationskoeffizient">oben</a>)</li>
  <li>Warum kommt bei der informational loss Funktion die Logarithmusfunktion zur
Anwendung?<br />
→ Die Logarithmusfunktion hat die gewünschte Form: Bei perfekter
   Klassifizierung soll der Loss gleich 0 sein. Wenn es nicht perfekt ist,
   also (0 \leq p_i &lt; 1), dann soll der Loss streng monoton fallen.</li>
</ul>

<h3 id="tocAnchor-1-14-1">Association Rules</h3>
<ul>
  <li>Wie muss der Datenbestand beschaffen sein, damit eine Association Rule hohen
Support und hohe Confidence hat?</li>
  <li>Wie muss der Datenbestand beschaffen sein, damit eine Association Rule hohen
Support und geringe Confidence hat?</li>
  <li>Wie muss der Datenbestand beschaffen sein, damit eine Association Rule
geringen Support und hohe Confidence hat?</li>
  <li>Wie muss der Datenbestand beschaffen sein, damit eine Association Rule
geringen Support und geringe Confidence hat?</li>
  <li>Im Apriori-Algorithmus hat man bei k=2 keinen Prune-Schritt. Warum?<br />
→ (Antwort: 24.11.2015, 14:34)</li>
  <li>Wie groß sollte man die Hash-Tabelle machen?<br />
→ So groß wie sinnvoll möglich. Der verfügbare Arbeitsspeicher ist hier eine
Grenze.</li>
  <li>Welche zwei Sprachen haben wir für die Formulierung der Constraints
kennengelernt?<br />
→ 1-var und 2-var bzw. MetaRule Guided</li>
  <li>Warum ist SQL nicht geeignet um Constraints zu formulieren?<br />
→ Weil SQL keine Aussage über die Struktur machen kann (TODO)</li>
</ul>

<h3 id="tocAnchor-1-14-2">Clustering</h3>
<ul>
  <li>BIRCH-Algorithmus: Wie kann man die Interclusterdistanz aus N, LS, SS
herleiten?<br />
→ (R(C_i) = \sqrt{\frac{1}{N} (SS - 2 \frac{LS}{N} \cdot LS + N (\frac{LS}{N})^2)})</li>
  <li>BIRCH-Algorithmus: Wie kann man den Durchmesser aus N, LS, SS herleiten?<br />
→ (\sqrt{\frac{1}{N \cdot (N-1)} (N \cdot SS - 2 LS^2 + N^2 \cdot SS)})</li>
  <li>BIRCH-Algorithmus: Wie kann man die Interclusterdistanz aus N, LS, SS
herleiten?<br />
→ (D(C_1, C_2) = \sqrt{\frac{SS_{C_1} - 2 LS_{C_2} LS_{C_1} + SS_{C_2}}{N_{C_1} \cdot N_{C_2}}})</li>
  <li>Was spricht dagegen, <span>\(\mathbf{\varepsilon}\)</span> in
OPTICS riesig zu wählen?<br />
→ Dann sind gleich am Anfang mit dem ersten Objekt alle Datenobjekte in der
   Priority-Queue. Damit wäre der Aufwand für die Queue zu hoch.</li>
</ul>

<h2 id="tocAnchor-1-17">Übungen</h2>

<p>Kommt noch.</p>

<h2 id="tocAnchor-1-18">Material und Links</h2>

<p>Die Vorlesung wurde gestreamt und ist unter
<a href="http://mml-streamdb01.ira.uka.de/">mml-streamdb01.ira.uka.de</a> verfügbar.</p>

<ul>
  <li><a href="https://dbis.ipd.kit.edu/2261.php">Vorlesungswebsite</a></li>
  <li><a href="https://ilias.studium.kit.edu/goto_produktiv_crs_477914.html">Ilias</a></li>
</ul>

<h2 id="tocAnchor-1-19">Übungsbetrieb</h2>

<p>?</p>

<h2 id="tocAnchor-1-20">Vorlesungsempfehlungen</h2>

<p>Folgende Vorlesungen sind ähnlich:</p>

<ul>
  <li><a href="https://martin-thoma.com/analysetechniken-grosser-datenbestaende/">Analysetechniken großer Datenbestände</a></li>
  <li><a href="https://martin-thoma.com/machine-learning-1-course/">Machine Learning 1</a></li>
  <li><a href="https://martin-thoma.com/machine-learning-2-course/">Machine Learning 2</a></li>
  <li><a href="https://martin-thoma.com/mustererkennung-klausur/">Mustererkennung</a></li>
  <li><a href="https://martin-thoma.com/neuronale-netze-vorlesung/">Neuronale Netze</a></li>
  <li>Lokalisierung Mobiler Agenten</li>
</ul>

<h2 id="tocAnchor-1-21">Termine und Klausurablauf</h2>

<p>Es ist noch nicht klar, ob es eine mündliche oder eine schriftliche Prüfung
wird.</p>

<p>Falls es mündlich ist, soll es mindestens einen Termin pro Monat geben.</p>

<p><strong>Datum</strong>: Noch ist es eine mündliche Prüfung<br />
<strong>Ort</strong>: Noch ist es eine mündliche Prüfung<br />
<strong>Punkte</strong>: Noch ist es eine mündliche Prüfung<br />
<strong>Zeit</strong>: Noch ist es eine mündliche Prüfung<br />
<strong>Punkteverteilung</strong>: Noch ist es eine mündliche Prüfung<br />
<strong>Bestehensgrenze</strong>: Noch ist es eine mündliche Prüfung<br />
<strong>Übungsschein</strong>: Gibt es nicht.<br />
<strong>Bonuspunkte</strong>: Gibt es nicht.<br />
<strong>Ergebnisse</strong>: Noch ist es eine mündliche Prüfung<br />
<strong>Einsicht</strong>: Noch ist es eine mündliche Prüfung<br />
<strong>Erlaubte Hilfsmittel</strong>: keine</p>

                        </div>
                        <div class="postmeta">Posted in
                            
                                <a href="//martin-thoma.com/category/deutschland/">german posts</a><!--TODO: Displayed category name should be upper case! -->
                             | Tags:
                            
                                
                                    <a href="//martin-thoma.com/tag/klausur/">Klausur</a>
                                
                             by <a rel="author" class="vcard author post-author" itemprop="author" href="//martin-thoma.com/author/martin-thoma/"><span class="fn" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Martin Thoma</span></span></a> on <span class="updated"><span class="value-title" title="2016-04-15 11:22:00 +0200">
                                April
                                15th
                                  ,
                                2016</span></span></div>

                            <div class="navigation clearfix">
                                <div class="alignleft">
                                
                                    &laquo; <a href="//martin-thoma.com/microsoft-vision-api/" rel="prev">Microsoft Vision API</a>
                                
                                </div>
                                <div class="alignright">
                                
                                </div>
                            </div>

                        </article>
                        <div id="respond">
                            <h3>Leave a Reply</h3>
                                <!-- comment discuss code -->
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'martinthoma'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="//disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    <!-- comment discuss code -->

                        </div>
                    </div>
                </div>
            <div class="span-8 last">
                <div id="subscriptions">
<a href="//martin-thoma.com/feed/"><img src="//martin-thoma.com/css/images/rss.png" alt="Subscribe to RSS Feed" title="Subscribe to RSS Feed" width="72" height="47" /></a>
<a href="https://twitter.com/#!/themoosemind" title="Follow me on Twitter!"><img src="//martin-thoma.com/css/images/twitter.png" title="Follow me on Twitter!" alt="Follow me on Twitter!"  width="76" height="47" /></a>
</div>

                <div id="sidebar">
                <!-- type: searchbox.html - TODO-->
<ul>
    <li id="search">
        <div class="searchlayout">
            <form method="get" id="searchform" action="//google.com/cse" role="search">
                <input type="hidden" name="cx" value="017345337424948206369:qrnnnentkkk" />
                <input type="search" value="" name="q" id="s" placeholder="Search with Google"/>
                <input type="image" src="//martin-thoma.com/css/images/search.gif" style="border:0; vertical-align: top;" alt="search"/>
            </form>
        </div>
    </li>
</ul>

                <div class="addthis_toolbox">
    <div class="custom_images">
            <a href="//twitter.com/share?url=https://martin-thoma.com/analysetechniken-grosser-datenbestaende&amp;hashtags=klausur,&amp;via=themoosemind" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/twitter.png" width="32" height="32" alt="Twitter" /></a>
            <a href="//del.icio.us/post?url=//martin-thoma.com/analysetechniken-grosser-datenbestaende&amp;title=Analysetechniken%20für%20große%20Datenbestände" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/delicious.png" width="32" height="32" alt="Delicious" /></a>
            <a href="//www.facebook.com/sharer.php?u=//martin-thoma.com/analysetechniken-grosser-datenbestaende" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/facebook.png" width="32" height="32" alt="Facebook" /></a>
            <a href="//digg.com/submit?phase=2&amp;url=//martin-thoma.com/analysetechniken-grosser-datenbestaende&amp;title=Analysetechniken%20für%20große%20Datenbestände" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/digg.png" width="32" height="32" alt="Digg" /></a>
            <a href="//www.stumbleupon.com/submit?url=//martin-thoma.com/analysetechniken-grosser-datenbestaende&amp;title=Analysetechniken%20für%20große%20Datenbestände" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/stumbleupon.png" width="32" height="32" alt="Stumbleupon" /></a>
            <a href="//plusone.google.com/_/+1/confirm?hl=en&amp;url=//martin-thoma.com/analysetechniken-grosser-datenbestaende" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/gplus.png" width="32" height="32" alt="Google Plus" /></a>
            <a href="//reddit.com/submit?url=//martin-thoma.com/analysetechniken-grosser-datenbestaende&amp;title=Analysetechniken%20für%20große%20Datenbestände" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/reddit.png" width="32" height="32" alt="Reddit" /></a>
    </div>
</div>

                <ul>
                    <li id="categories-3" class="widget widget_categories">
                        <!-- type: categories -->
<h2 class="widgettitle">Categories</h2>
    <ul>
        <li class="cat-item cat-item-11"><a href="//martin-thoma.com/category/code/" title="Tipps for coding in different languages like Python oder C++.">Code</a></li>
        <li class="cat-item cat-item-21"><a href="//martin-thoma.com/category/web/" title="New emerging websites and technologies.">The Web</a></li>
        <li class="cat-item cat-item-31"><a href="//martin-thoma.com/category/cyberculture/" title="Lolcats, planking, Trollfaces, ...">Cyberculture</a></li>
        <li class="cat-item cat-item-3404"><a href="//martin-thoma.com/category/maths/" title="View all posts filed under Mathematics">Mathematics</a></li>
        <li class="cat-item cat-item-881"><a href="//martin-thoma.com/category/bits-and-bytes/" title="Sometimes posts don&#039;t fit in any category.">My bits and bytes</a></li>
        <li class="cat-item cat-item-41"><a href="//martin-thoma.com/category/deutschland/" title="[All Posts here are written in German about German topics] - Die Bahn, unsere Politik und Europa.">German posts</a></li>
    </ul>

                    </li>
                </ul>
                </div>
            </div>
        </div><!--/container-->
            <footer id="footer">
                <a href="//martin-thoma.com"><strong>Martin Thoma</strong></a> -  A blog about Code, the Web and Cyberculture. <br />
                <div class="footer-credits">
                    <a href="http://flexithemes.com/themes/modern-style/">Modern Style</a> theme by <a href="http://flexithemes.com/">FlexiThemes</a>
                </div>
            </footer><!--/footer-->

    </div><!--/wrapper-->
<!-- type: footer -->
<!-- TOC Plus -->
<script type='text/javascript'>
/* <![CDATA[ */
var tocplus = {"visibility_show":"show","visibility_hide":"hide","width":"275px"};
/* ]]> */
</script>
<script type='text/javascript' src="//martin-thoma.com/js/tocplus-front.js"></script>
<script>
(function(){
    'use strict';

    /*
    Create intra-page links
    Requires that your headings already have an `id` attribute set (because that's what jekyll does)
    For every heading in your page, this adds a little anchor link `#` that you can click to get a permalink to the heading.
    Ignores `h1`, because you should only have one per page.
    The text content of the tag is used to generate the link, so it will fail "gracefully-ish" if you have duplicate heading text.

    Credit: https://gist.github.com/SimplGy/a229d25cdb19d7f21231
     */

    var headingNodes = [], results, link,
        tags = ['h2', 'h3', 'h4', 'h5', 'h6'];

    tags.forEach(function(tag){
        var contentTag = document.getElementById('contentAfterTitle');
      results = contentTag.getElementsByTagName(tag);
      Array.prototype.push.apply(headingNodes, results);
    });

    headingNodes.forEach(function(node){
      link = document.createElement('a');
      link.className = 'deepLink';
      link.textContent = ' ¶';
      link.href = '#' + node.getAttribute('id');
      node.appendChild(link);
    });

  })();
</script>
</body>
</html>

