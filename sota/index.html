<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Martin Thoma" />
        <meta name="copyright" content="Martin Thoma" />
        <link title = "Martin Thoma"
              type  = "application/opensearchdescription+xml"
              rel   = "search"
              href  = "../opensearch.xml">

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="Machine Learning, Datasets, Machine Learning, " />

<meta property="og:title" content="State of the Art in ML "/>
<meta property="og:url" content="../sota/" />
<meta property="og:description" content="It is difficult to keep track of the current state of the art (SotA). Also, it might not be directly clear which datasets are relevant. The following list should help. If you think some datasets / problems / SotA results are missing, let me know in the comments or via E-mail (info …" />
<meta property="og:site_name" content="Martin Thoma" />
<meta property="og:article:author" content="Martin Thoma" />
<meta property="og:article:published_time" content="2017-02-06T20:00:00+01:00" />
<meta name="twitter:title" content="State of the Art in ML ">
<meta name="twitter:description" content="It is difficult to keep track of the current state of the art (SotA). Also, it might not be directly clear which datasets are relevant. The following list should help. If you think some datasets / problems / SotA results are missing, let me know in the comments or via E-mail (info …">
<meta property="og:image" content="logos/ml.png" />
<meta name="twitter:image" content="logos/ml.png" >

        <title>State of the Art in ML  · Martin Thoma
</title>
        <!-- Latest compiled and minified CSS -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="../theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../static/custom.css" media="screen">

        <!-- MathJax -->
<script type="text/x-mathjax-config">
<!--
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [ ['$$', '$$'], ['\\[','\\]']],
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    processEscapes: true
  }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
// -->
</script>
<script type="text/javascript" async
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

        <link href="https://martin-thoma.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Martin Thoma - Full Atom Feed" />
        <link href="https://martin-thoma.com/feeds/index.xml" type="application/rss+xml" rel="alternate" title="Martin Thoma - Full RSS Feed" />
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top navbar-default">
            <div class="container">
                <div class="container-fluid">
                    <div class="collapse navbar-collapse">
                        <ul class="nav pull-left top-menu navbar-nav">
                            <li><a href=".." style="font-family: 'Monaco', 'Inconsolata', 'Andale Mono', 'Lucida Console', 'Bitstream Vera Sans Mono', 'Courier New', Courier, Monospace;
                        font-size: 20px;" class="navbar-brand">Martin Thoma</a>
                            </li>
                        </ul>
                        <ul class="nav pull-right top-menu navbar-nav">
                            <li ><a href="..">Home</a></li>
                            <li ><a href="../categories.html">Categories</a></li>
                            <li ><a href="../tags.html">Tags</a></li>
                            <li ><a href="../archives.html">Archives</a></li>
                            <li><a href="../support-me/">Support me</a></li>
                            <li><form class="navbar-form" action="../search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="search" class="search-query form-control" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row">
                <div class="col-sm-1 col-md-1"></div>
                <div class="col-sm-10 col-md-10">
<article>
<div class="row">
    <header class="page-header col-sm-10 col-md-10 col-md-offset-2">
    <h1><a href="../sota/"> State of the Art in ML  </a></h1>
    </header>
</div>

<div class="row">
    <div class="col-sm-2 col-md-2 table-of-content">
        <nav>
        <h4>Contents</h4>
        <div id="toc"><ul><li><a class="toc-href" href="#computer-vision" title="Computer Vision">Computer Vision</a><ul><li><a class="toc-href" href="#image-classification" title="Image Classification">Image Classification</a></li><li><a class="toc-href" href="#detection-images" title="Detection (Images)">Detection (Images)</a></li><li><a class="toc-href" href="#detection-videos" title="Detection (Videos)">Detection (Videos)</a></li><li><a class="toc-href" href="#person-re-identitification" title="Person Re-Identitification">Person Re-Identitification</a></li><li><a class="toc-href" href="#semantic-segmentation" title="Semantic Segmentation">Semantic Segmentation</a></li><li><a class="toc-href" href="#instance-segmentation" title="Instance Segmentation">Instance Segmentation</a></li><li><a class="toc-href" href="#action-recognition" title="Action Recognition">Action Recognition</a></li><li><a class="toc-href" href="#super-resolution" title="Super Resolution">Super Resolution</a></li><li><a class="toc-href" href="#lip-reading" title="Lip Reading">Lip Reading</a></li><li><a class="toc-href" href="#other-datasets" title="Other Datasets">Other Datasets</a></li></ul></li><li><a class="toc-href" href="#asr_1" title="ASR">ASR</a><ul><li><a class="toc-href" href="#sentence-level" title="Sentence-Level">Sentence-Level</a></li><li><a class="toc-href" href="#phoneme-level" title="Phoneme-Level">Phoneme-Level</a></li></ul></li><li><a class="toc-href" href="#language_1" title="Language">Language</a></li><li><a class="toc-href" href="#translation" title="Translation">Translation</a></li><li><a class="toc-href" href="#matrix-completion" title="Matrix completion">Matrix completion</a></li><li><a class="toc-href" href="#reinforcment-learning" title="Reinforcment Learning">Reinforcment Learning</a></li><li><a class="toc-href" href="#control" title="Control">Control</a></li><li><a class="toc-href" href="#see-also" title="See also">See also</a></li></ul></div>
        </nav>
    </div>
    <div class="col-sm-8 col-md-8 article-content" id="contentAfterTitle">

            
            <p>It is difficult to keep track of the current state of the art (SotA). Also, it
might not be directly clear which datasets are relevant. The following list
should help. If you think some datasets / problems / SotA results are missing,
let me know in the comments or via E-mail (<code>info@martin-thoma.de</code>).
I will update it.</p>
<p>Papers and blog posts which summarize a topic or give a good introduction are
always welcome.</p>
<p>In the following, a <code>+</code> will indicate "higher is better" and a
<code>-</code> will indicate "lower is better".</p>
<h2 id="computer-vision">Computer Vision</h2>
<h3 id="image-classification">Image Classification</h3>
<table class="table" id="image-classification-table">
<tr>
<th>Dataset</th>
<th>Year</th>
<th>Score</th>
<th>Type</th>
<th>Paper</th>
</tr>
<tr>
<td><a href="http://www.image-net.org/challenges/LSVRC/2012/nonpub-downloads">ImageNet 2012</a></td>
<td>2015</td>
<td style="text-align: right;">3.57 %</td>
<td>Top-5 error <span title="lower is better">-</span></td>
<td><a href="https://arxiv.org/pdf/1512.03385v1.pdf" title="Deep Residual Learning for Image Recognition">[HZRS15a]</a></td>
</tr>
<tr>
<td><a href="http://yann.lecun.com/exdb/mnist/">MNIST</a></td>
<td>2013</td>
<td style="text-align: right;">0.21 %</td>
<td>error <span title="lower is better">-</span></td>
<td><a href="http://www.matthewzeiler.com/pubs/icml2013/icml2013.pdf" title="Regularization of Neural Networks using DropConnect">[WZZ+13]</a></td>
</tr>
<tr>
<td><a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a></td>
<td>2017</td>
<td style="text-align: right;">2.72 %</td>
<td>error <span title="lower is better">-</span></td>
<td><a href="https://openreview.net/forum?id=HkO-PCmYl" title="Shake-Shake regularization of 3-branch residual networks">[G17]</a></td>
</tr>
<tr>
<td><a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-100</a></td>
<td>2016</td>
<td style="text-align: right;">16.21 %</td>
<td>error <span title="lower is better">-</span></td>
<td><a href="https://arxiv.org/abs/1608.03983" title="SGDR: Stochastic Gradient Descent with Warm Restarts">[LH16]</a></td>
</tr>
<tr>
<td><a href="http://cs.stanford.edu/~acoates/stl10/">STL-10</a></td>
<td>2015</td>
<td style="text-align: right;">74.80 %</td>
<td>accuracy <span title="higher is better">+</span></td>
<td><a href="https://arxiv.org/abs/1506.02351v1" title="Stacked What-Where Auto-encoders">[ZMGL15]</a></td>
</tr>
<tr>
<td><a href="http://ufldl.stanford.edu/housenumbers/">SVHN</a></td>
<td>2016</td>
<td style="text-align: right;">1.59 %</td>
<td>error <span title="lower is better">-</span></td>
<td><a href="https://arxiv.org/abs/1608.06993v1" title="Densely Connected Convolutional Networks">[HLW16]</a></td>
</tr>
<tr>
<td><a href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/Caltech101.html">Caltech-101</a></td>
<td>2014</td>
<td style="text-align: right;">91.4 %</td>
<td>accuracy <span title="higher is better">+</span></td>
<td><a href="https://arxiv.org/abs/1406.4729v1" title="Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition">[HZRS14]</a></td>
</tr>
<tr>
<td><a href="http://www.vision.caltech.edu/Image_Datasets/Caltech256/">Caltech-256</a></td>
<td>2014</td>
<td style="text-align: right;">74.2 %</td>
<td>accuracy <span title="higher is better">+</span></td>
<td><a href="https://arxiv.org/abs/1311.2901" title="Visualizing and Understanding Convolutional Networks">[ZF14]</a></td>
</tr>
<tr>
<td><a href="https://zenodo.org/record/259444" title="The HASYv2 dataset">HASYv2</a></td>
<td>2017</td>
<td style="text-align: right;">81.00 %</td>
<td>accuracy <span title="higher is better">+</span></td>
<td><a href="https://arxiv.org/abs/1701.08380" title="The HASYv2 dataset">[Tho17]</a></td>
</tr>
<tr>
<td><a href="http://lear.inrialpes.fr/people/marszalek/data/ig02/">Graz-02</a></td>
<td>2010</td>
<td style="text-align: right;">78.98 %</td>
<td>accuracy <span title="higher is better">+</span></td>
<td><a href="http://imagine.enpc.fr/publications/papers/ECCV2010b.pdf" title="Towards Optimal Naive Bayes Nearest Neighbor">[BMDP10]</a></td>
</tr>
<tr>
<td><a href="http://yfcc100m.appspot.com/">YFCC100m</a></td>
<td></td>
<td style="text-align: right;"></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="http://www.vision.caltech.edu/visipedia/CUB-200-2011.html">CUB-200-2011</a> Birds</td>
<td>2015</td>
<td style="text-align: right;">84.1</td>
<td>accuracy <span title="higher is better">+</span></td>
<td><a href="http://vis-www.cs.umass.edu/bcnn/docs/bcnn_iccv15.pdf">[LRM15]</a></td>
</tr>
<tr>
<td><a href="http://mohammadmahoor.com/databases/denver-intensity-of-spontaneous-facial-action/">DISFA</a></td>
<td>2017</td>
<td style="text-align: right;">48.5</td>
<td>accuracy <span title="higher is better">+</span></td>
<td><a href="https://arxiv.org/abs/1702.02925" title="EAC-Net: A Region-based Deep Enhancing and Cropping Approach for Facial Action Unit Detection">[LAZY17]</a></td>
</tr>
<tr>
<td>BP4D</td>
<td></td>
<td style="text-align: right;"></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://arxiv.org/abs/1702.05373" title="EMNIST: an extension of MNIST to handwritten letters">EMNIST</a></td>
<td>2017</td>
<td style="text-align: right;">50.93</td>
<td>accuracy <span title="higher is better">+</span></td>
<td><a href="https://arxiv.org/abs/1702.05373" title="EMNIST: an extension of MNIST to handwritten letters">[CATS17]</a></td>
</tr>
<tr>
<td><a href="http://megaface.cs.washington.edu/">Megaface</a></td>
<td>2015</td>
<td style="text-align: right;">74.6%</td>
<td>accuracy <span title="higher is better">+</span></td>
<td>Google - <a href="https://arxiv.org/abs/1503.03832">FaceNet</a> v8</td>
</tr>
<tr>
<td><a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CelebA</a></td>
<td>?</td>
<td style="text-align: right;">?</td>
<td>?</td>
<td>?</td>
</tr>
<tr>
<td><a href="http://benchmark.ini.rub.de/?section=gtsrb&amp;subsection=news">GTSRB</a></td>
<td>2012</td>
<td style="text-align: right;">99.46%</td>
<td>accuracy <span title="higher is better">+</span></td>
<td><a href="http://ieeexplore.ieee.org/document/6033589" title="Multi-column deep neural network for traffic sign classification">[CMMS12]</a></td>
</tr>
</table>
<p>State of the art in this category are CNN models which use skip connections
in the form of residual connections or dense connections.</p>
<p>The evaluation metrics are straight-forward:</p>
<ul>
<li><strong>Accuracy</strong>: Count how many elements of the test dataset you got right,
  divided by the total number of elements in the test dataset. The accuracy is
  in <span class="math">\([0, 1]\)</span>. Higher is better.</li>
<li><strong>Error</strong> = 1 - accuracy. The error is in <span class="math">\([0, 1]\)</span>. Lower is better.</li>
<li><strong>Top-k accuracy</strong>: Sometimes, there are either extremely similar classes or
  the application allows having multiple guesses. Hence not the Top-1 guess
  of the network has to be right, but the correct label has to be within the
  top <span class="math">\(k\)</span> guesses. The top-<span class="math">\(k\)</span> accuracy is in <span class="math">\([0, 1]\)</span>. Higher is better.</li>
</ul>
<h3 id="detection-images">Detection (Images)</h3>
<p>Face recognition is a special case of detection.</p>
<p>Common metrics are:
<ul>
<li>mAP (Mean Average Precision): A detection is successfull, if the bounding
        box prediction and the true bounding box <span class="math">\(\frac{intersection}{union}\)</span> (IU, IoU)
        ratio is at least 0.5. Then the average precision = <span class="math">\(\frac{TP}{TP + FP}\)</span> is
        calculated for each class and the mean is calculated of those (see <a href="http://stackoverflow.com/q/36274638/562769">Explanation</a>, <a href="http://datascience.stackexchange.com/q/16797/8820">What does the notation mAP@[.5:.95] mean?</a>)</li>
<li>MR (miss rate)</li>
</ul></p>
<table class="table">
<tr>
<th>Dataset</th>
<th>Year</th>
<th>Score</th>
<th>Type</th>
<th>Paper</th>
</tr>
<tr>
<td><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html">PASCAL VOC 2012</a></td>
<td>2015</td>
<td style="text-align: right;">75.9</td>
<td>mAP@.5 <span title="higher is better">+</span></td>
<td><a href="https://arxiv.org/abs/1506.01497" title="Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks">[RHGS15]</a></td>
</tr>
<tr>
<td><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2011/index.html">PASCAL VOC 2011</a></td>
<td>2014</td>
<td style="text-align: right;">62.7</td>
<td>mean IU <span title="higher is better">+</span></td>
<td><a href="https://arxiv.org/abs/1411.4038" title="Fully Convolutional Networks for Semantic Segmentation">[LSD14]</a></td>
</tr>
<tr>
<td><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2010/index.html">PASCAL VOC 2010</a></td>
<td>2011</td>
<td style="text-align: right;">30.2</td>
<td>mean accuracy <span title="higher is better">+</span></td>
<td><a href="https://papers.nips.cc/paper/4296-efficient-inference-in-fully-connected-crfs-with-gaussian-edge-potentials.pdf" title="Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials">[Kol11]</a></td>
</tr>
<tr>
<td><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2007/index.html">PASCAL VOC 2007</a></td>
<td>2015</td>
<td style="text-align: right;">71.6</td>
<td>mAP@.5 <span title="higher is better">+</span></td>
<td><a href="https://arxiv.org/abs/1512.02325" title="SSD: Single Shot MultiBox Detector">[LAES+15]</a></td>
</tr>
<tr>
<td><a href="http://mscoco.org/">MS COCO</a></td>
<td>2015</td>
<td style="text-align: right;">46.5</td>
<td>mAP@.5 <span title="higher is better">+</span></td>
<td><a href="https://arxiv.org/abs/1512.02325" title="SSD: Single Shot MultiBox Detector">[LAES+15]</a></td>
</tr>
<tr>
<td><a href="https://arxiv.org/abs/1702.05693" title="CityPersons: A Diverse Dataset for Pedestrian Detection">CityPersons</a></td>
<td>2017</td>
<td style="text-align: right;">33.10</td>
<td><abbr title="log miss-rate">MR</abbr> <span title="lower is better">-</span></td>
<td><a href="https://arxiv.org/abs/1702.05693" title="CityPersons: A Diverse Dataset for Pedestrian Detection">[ZBS17]</a></td>
</tr>
</table>
<h3 id="detection-videos">Detection (Videos)</h3>
<table>
<tr>
<th>Dataset</th>
<th>Year</th>
<th>Score</th>
<th>Type</th>
<th>Paper</th>
</tr>
<tr>
<td><a href="https://research.google.com/youtube-bb/">YouTube-BoundingBoxes</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</table>
<h3 id="person-re-identitification">Person Re-Identitification</h3>
<p>Person Re-ID is the task of identifying a person again which was already seen
in a video stream. Person following and <abbr title="Multi Target Multi
Camera">MTMCT</abbr> seems to be very similar if not identical.</p>
<table class="table">
<tr>
<th>Dataset</th>
<th>Year</th>
<th>Score</th>
<th>Type</th>
<th>Paper</th>
</tr>
<tr>
<td><a href="http://www.liangzheng.org/Project/project_reid.html">Market-1501</a></td>
<td>2017</td>
<td>62.1</td>
<td>mAP <span title="higher is better">+</span></td>
<td><a href="https://arxiv.org/abs/1703.05693">[SZDW17]</a></td>
</tr>
<tr>
<td><a href="http://www.ee.cuhk.edu.hk/~xgwang/CUHK_identification.html">CUHK03</a></td>
<td>2017</td>
<td>84.8</td>
<td>mAP <span title="higher is better">+</span></td>
<td><a href="https://arxiv.org/abs/1703.05693">[SZDW17]</a></td>
</tr>
<tr>
<td><a href="http://vision.cs.duke.edu/DukeMTMC/">DukeMTMC</a></td>
<td>2017</td>
<td>56.8</td>
<td>mAP <span title="higher is better">+</span></td>
<td><a href="https://arxiv.org/abs/1703.05693">[SZDW17]</a></td>
</tr>
</table>
<h3 id="semantic-segmentation">Semantic Segmentation</h3>
<p>A summary of classical methods for semantic segmentation, more information
to several datasets and metrics for evaluation can be found in <a href="https://arxiv.org/abs/1602.06541">A Survey of Semantic Segmentation</a>.</p>
<table class="table">
<tr>
<th>Dataset</th>
<th>Year</th>
<th>Score</th>
<th>Type</th>
<th>Paper</th>
</tr>
<tr>
<td><a href="https://www.microsoft.com/en-us/research/project/image-understanding/">MSRC-21</a></td>
<td>2011</td>
<td style="text-align: right;">84.7</td>
<td>mean accuracy <span title="higher is better">+</span></td>
<td><a href="https://papers.nips.cc/paper/4296-efficient-inference-in-fully-connected-crfs-with-gaussian-edge-potentials.pdf" title="Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials">[Kol11]</a></td>
</tr>
<tr>
<td><a href="http://www.cvlibs.net/datasets/kitti/eval_road.php">KITTI Road</a></td>
<td></td>
<td style="text-align: right;">96.69</td>
<td>Max F1 <span title="higher is better">+</span></td>
<td></td>
</tr>
<tr>
<td><a href="http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html" title="NYU Depth Dataset V2">NYUDv2</a></td>
<td>2014</td>
<td style="text-align: right;">34.0</td>
<td>mean IO <span title="higher is better">+</span></td>
<td><a href="https://papers.nips.cc/paper/4296-efficient-inference-in-fully-connected-crfs-with-gaussian-edge-potentials.pdf" title="Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials">[Kol11]</a></td>
</tr>
<tr>
<td><a href="https://people.csail.mit.edu/celiu/SIFTflow/">SIFT Flow</a></td>
<td>2014</td>
<td style="text-align: right;">39.5</td>
<td>mean IU <span title="higher is better">+</span></td>
<td><a href="https://arxiv.org/abs/1411.4038" title="Fully Convolutional Networks for Semantic Segmentation">[LSD14]</a></td>
</tr>
<tr>
<td><a href="http://www.it.lut.fi/project/imageret/diaretdb1/">DIARETDB1</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://www2.warwick.ac.uk/fac/sci/dcs/research/combi/research/bic/glascontest/download/">Warwick-QU</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://dataverse.scholarsportal.info/dataset.xhtml?persistentId=doi:10.5683/SP/NTUOK9">Ciona17</a></td>
<td>2017</td>
<td>51.36 %</td>
<td>mean IoU <span title="higher is better">+</span></td>
<td><a href="https://arxiv.org/pdf/1702.05564.pdf" title="The Ciona17 Dataset for Semantic Segmentation of Invasive Species in a Marine Aquaculture Environment">[GTRM17]</a></td>
</tr>
</table>
<h3 id="instance-segmentation">Instance Segmentation</h3>
<p>See <a href="https://arxiv.org/abs/1512.04412" title="Instance-aware Semantic Segmentation via Multi-task Network Cascades">[DHS15]</a></p>
<table class="table">
<tr>
<th>Dataset</th>
<th>Year</th>
<th>Score</th>
<th>Type</th>
<th>Paper</th>
</tr>
<tr>
<td><a href="https://www.cityscapes-dataset.com/benchmarks/">CityScapes</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</table>
<h3 id="action-recognition">Action Recognition</h3>
<p>Action recognition is a classification problem over a short video clip.</p>
<table class="table">
<tr>
<th>Dataset</th>
<th>Year</th>
<th>Score</th>
<th>Type</th>
<th>Paper</th>
</tr>
<tr>
<td><a href="https://research.google.com/youtube8m/">YouTube-8M</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://github.com/gtoderici/sports-1m-dataset/blob/wiki/ProjectHome.md">Sports-1M</a></td>
<td>2015</td>
<td>68.7 %</td>
<td>Clip Hit@1 accuracy <span title="higher is better">+</span></td>
<td><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ng_Beyond_Short_Snippets_2015_CVPR_paper.pdf" title="Beyond Short Snippets: Deep Networks for Video Classification">[NHV+15]</a></td>
</tr>
<tr>
<td>UCF-101</td>
<td>2015</td>
<td>70.8 %</td>
<td>Clip Hit@1 accuracy <span title="higher is better">+</span></td>
<td><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ng_Beyond_Short_Snippets_2015_CVPR_paper.pdf" title="Beyond Short Snippets: Deep Networks for Video Classification">[NHV+15]</a></td>
</tr>
<tr>
<td><a href="http://www.nada.kth.se/cvap/actions/">KTH</a></td>
<td>2015</td>
<td style="text-align: right;">95.6 %</td>
<td>EER <span title="higher is better">+</span></td>
<td><a href="https://arxiv.org/pdf/1512.03980.pdf" title="Action Recognition with Image Based CNN Features">[RMRMD15]</a></td>
</tr>
<tr>
<td><a href="http://crcv.ucf.edu/data/UCF_Sports_Action.php">UCF Sport</a></td>
<td>2015</td>
<td style="text-align: right;">97.8 %</td>
<td>EER <span title="higher is better">+</span></td>
<td><a href="https://arxiv.org/pdf/1512.03980.pdf" title="Action Recognition with Image Based CNN Features">[RMRMD15]</a></td>
</tr>
<tr>
<td><a href="http://crcv.ucf.edu/data/UCF_YouTube_Action.php">UCF-11 Human Action</a></td>
<td>2015</td>
<td style="text-align: right;">89.5 %</td>
<td>EER <span title="higher is better">+</span></td>
<td><a href="https://arxiv.org/pdf/1512.03980.pdf" title="Action Recognition with Image Based CNN Features">[RMRMD15]</a></td>
</tr>
</table>
<h3 id="super-resolution">Super Resolution</h3>
<p>See <a href="https://github.com/huangzehao/Super-Resolution.Benckmark">github.com/huangzehao</a></p>
<table class="table">
<tr>
<th>Dataset</th>
<th>Year</th>
<th>Score</th>
<th>Type</th>
<th>Paper</th>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</table>
<p>I'm not sure how super resolution is benchmarked. One way to do it would be
to get high resolution images, scale them down, feed them to the network and
measure the mean squared error for each pixel:</p>
<div class="math">$$\frac{1}{|I|} \sum_{t \in I} {(t - \hat{t})}^2$$</div>
<p>However, this might be sensitive to the way the images were downsampled.</p>
<h3 id="lip-reading">Lip Reading</h3>
<table class="table">
<tr>
<th>Dataset</th>
<th>Year</th>
<th>Score</th>
<th>Type</th>
<th>Paper</th>
</tr>
<tr>
<td><a href="http://spandh.dcs.shef.ac.uk/gridcorpus/" title="The GRID audiovisual sentence corpus

">GRID</a></td>
<td>2016</td>
<td>95.2 %</td>
<td>accuracy <span title="higher is better">+</span></td>
<td><a href="https://openreview.net/forum?id=BkjLkSqxg" title="LipNet: End-to-End Sentence-level Lipreading">[ASWF16]</a></td>
</tr>
</table>
<h3 id="other-datasets">Other Datasets</h3>
<p>For the following datasets, I was not able to find where to download them</p>
<ul>
<li>Mapping global urban areas using MODIS 500-m data: New methods and datasets
  based on urban ecoregions</li>
<li><a href="https://arxiv.org/abs/1612.00423">TorontoCity: Seeing the World with a Million Eyes</a></li>
</ul>
<h2 id="asr_1">ASR</h2>
<p>Automatic Speech Recognition (ASR).</p>
<h3 id="sentence-level">Sentence-Level</h3>
<table class="table">
<tr>
<th>Dataset</th>
<th>Year</th>
<th>Score</th>
<th>Type</th>
<th>Paper</th>
</tr>
<tr>
<td><a href="" title="Wall Street Journal">WSJ</a> (eval92)</td>
<td>2015</td>
<td>3.47</td>
<td>WER <span title="lower is better">-</span></td>
<td><a href="https://arxiv.org/pdf/1504.01482v1.pdf" title="Deep Recurrent Neural Networks for Acoustic Modelling">[CL15]</a></td>
</tr>
<tr>
<td>Switchboard Hub5'00</td>
<td>2016</td>
<td>6.3%</td>
<td>WER <span title="lower is better">-</span></td>
<td><a href="https://arxiv.org/pdf/1609.03528v1.pdf" title="The Microsoft 2016 conversational speech recognition system">[XDSS+16]</a></td>
</tr>
</table>
<p>See <a href="https://martin-thoma.com/word-error-rate-calculation/">Word Error Rate</a> (WER)
for an explanation of the metric.</p>
<p>Relevant papers might be</p>
<ul>
<li><a href="https://arxiv.org/abs/1512.02595v1">Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</a></li>
</ul>
<h3 id="phoneme-level">Phoneme-Level</h3>
<table class="table">
<tr>
<th>Dataset</th>
<th>Year</th>
<th>Score</th>
<th>Type</th>
<th>Paper</th>
</tr>
<tr>
<td><a href="https://catalog.ldc.upenn.edu/ldc93s1" title="TIMIT Acoustic-Phonetic Continuous Speech Corpus">TIMIT</a></td>
<td>2013</td>
<td>17.7 %</td>
<td>error rate <span title="lower is better">-</span></td>
<td><a href="http://ieeexplore.ieee.org/abstract/document/6638947/" title="Speech recognition with deep recurrent neural networks">[GMH13]</a></td>
</tr>
</table>
<h2 id="language_1">Language</h2>
<p>Natural Language Processing (NLP) deals with how to represent language. It is
related and often a part of ASR.</p>
<table class="table">
<tr>
<th>Dataset</th>
<th>Year</th>
<th>Score</th>
<th>Type</th>
<th>Paper</th>
</tr>
<tr>
<td><a href="https://metamind.io/research/the-wikitext-long-term-dependency-language-modeling-dataset/">WikiText-103</a></td>
<td>2016</td>
<td style="text-align: right;">48.7</td>
<td>Perplexity <span title="lower is better">-</span></td>
<td><a href="https://openreview.net/pdf?id=B184E5qee" title="Improving Neural Language Models with a Continuous Cache">[GJU16]</a></td>
</tr>
<tr>
<td>Penn Treebank (PTB)</td>
<td>2016</td>
<td style="text-align: right;">62.4</td>
<td>Perplexity <span title="lower is better">-</span></td>
<td><a href="https://arxiv.org/abs/1611.01578" title="Neural Architecture Search with Reinforcement Learning">[ZL16]</a> (<a href="http://www.shortscience.org/paper?bibtexKey=journals/corr/1611.01578#martinthoma">summary</a>)</td>
</tr>
<tr>
<td><a href="https://www.reddit.com/r/MachineLearning/comments/5s6ixw/d_what_are_the_current_benchmark_datasets_and_the/ddcwoay/">Stanford Sentiment Treebank</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</table>
<p>NLP benchmarks use <a href="https://en.wikipedia.org/wiki/Perplexity">perplexity</a> to
measure how good a result is.</p>
<h2 id="translation">Translation</h2>
<table class="table">
<tr>
<th>Dataset</th>
<th>Year</th>
<th>Score</th>
<th>Type</th>
<th>Paper</th>
</tr>
<tr>
<td>MT03</td>
<td>2003</td>
<td>35.76</td>
<td>BLEU</td>
<td><a href="https://www.cs.sfu.ca/~anoop/papers/pdf/jhu-ws03-report.pdf" title="Syntax for Statistical Machine Translation">[OGKS+03]</a></td>
</tr>
</table>
<p>The <a href="https://en.wikipedia.org/wiki/BLEU" title="bilingual evaluation understudy">BLEU</a>
score is used to measure how good a translation system is.</p>
<p>Another score is the <em>Translation Edit Rate</em> (TER) introduced by
<a href="http://mt-archive.info/AMTA-2006-Snover.pdf">Snover et al., 2006</a>.</p>
<h2 id="matrix-completion">Matrix completion</h2>
<p>Collaborative filtering is an application of matrix completion.
More datasets are on <a href="https://gist.github.com/entaroadun/1653794">entaroadun/gist:1653794</a>.</p>
<table class="table">
<tr>
<th>Dataset</th>
<th>Year</th>
<th>Score</th>
<th>Type</th>
<th>Paper</th>
</tr>
<tr>
<td><a href="https://grouplens.org/datasets/movielens/">MovieLens</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="http://eigentaste.berkeley.edu/dataset/">Jester</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</table>
<h2 id="reinforcment-learning">Reinforcment Learning</h2>
<p>The <a href="https://gym.openai.com/">OpenAI Gym</a> offers many environments
for testing RL algorithms.</p>
<table class="table">
<tr>
<th>Challenge</th>
<th>Year</th>
<th>Score</th>
<th>Type</th>
<th>Paper</th>
</tr>
<tr>
<td>Chess</td>
<td></td>
<td>3395</td>
<td></td>
<td><a href="https://stockfishchess.org/">Stockfishchess</a></td>
</tr>
<tr>
<td>Go</td>
<td>2015</td>
<td>3,168</td>
<td>ELO <span title="higher is better">+</span></td>
<td><a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html" title="Mastering the game of Go with deep neural networks and tree search">AlphaGo</a></td>
</tr>
<tr>
<td>Star Craft</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</table>
<h2 id="control">Control</h2>
<table class="table">
<tr>
<th>Dataset</th>
<th>Year</th>
<th>Score</th>
<th>Type</th>
<th>Paper</th>
</tr>
<tr>
<td><a href="CartPole-v0">Cart Pole</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</table>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="http://rodrigob.github.io/are_we_there_yet">Are we there yet ?</a></li>
<li><a href="https://www.reddit.com/r/MachineLearning/comments/4dkrw1/some_stateofthearts_in_natural_language/">Some state-of-the-arts in natural language processing and their discussion</a></li>
<li>aclweb.org: <a href="https://www.aclweb.org/aclwiki/index.php?title=State_of_the_art">State of the art</a> - NLP tasks</li>
<li><a href="https://github.com/syhw/wer_are_we/tree/master">wer_are_we</a>: SotA in ASR</li>
<li><a href="https://github.com/michalwols/ml-sota">github.com/michalwols/ml-sota</a></li>
</ul>
<p>More datasets</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research">List of datasets for machine learning research</a></li>
<li><a href="http://www.cvl.isy.liu.se/research/datasets/traffic-signs-dataset/download/">traffic-signs-dataset</a></li>
<li><a href="http://vision.stanford.edu/aditya86/ImageNetDogs/">Stanford Dogs</a></li>
<li><a href="https://github.com/caesar0301/awesome-public-datasets">Awesome Public Datasets</a></li>
<li><a href="https://archive.ics.uci.edu/ml/datasets.html">archive.ics.uci.edu/ml/datasets.html</a></li>
<li><a href="https://tiny-imagenet.herokuapp.com/">Tiny ImageNet Visual Recognition Challenge</a></li>
</ul>
            
            <div id="disqus_thread"></div>
<script>
    /*
    var disqus_config = function () {
        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW
        var d = document, s = d.createElement('script');

        s.src = '//martinthoma.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

            <hr/>
        </div>
        <section>
        <div class="col-sm-2 col-md-2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time pubdate="pubdate" datetime="2017-02-06T20:00:00+01:00">Feb 6, 2017</time>
            <br/>
            by <a rel="author" class="vcard author post-author" itemprop="author" href="../author/martin-thoma/"><span class="fn" itemscope="" itemtype="https://schema.org/Person"><span itemprop="name">Martin Thoma</span></span></a>
            <h4>Category</h4>
            <a class="category-link" href="../categories.html#machine-learning-ref">Machine Learning</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="../tags.html#datasets-ref">Datasets
                    <span>1</span>
</a></li>
                <li><a href="../tags.html#machine-learning-ref">Machine Learning
                    <span>45</span>
</a></li>
            </ul>
<h4>Contact</h4>
    <a href="https://twitter.com/themoosemind" title="My Twitter Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-twitter sidebar-social-links"></i></a>
    <a href="mailto:info@martin-thoma.de" title="My Email Address" class="sidebar-social-links" target="_blank">
    <i class="fa fa-envelope sidebar-social-links"></i></a>
    <a href="https://github.com/MartinThoma" title="My Github Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-github sidebar-social-links"></i></a>
    <a href="http://stackoverflow.com/users/562769/martin-thoma" title="My Stackoverflow Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-stackoverflow sidebar-social-links"></i></a>
        </div>
        </section>
</div>
</article>
                </div>
                <div class="col-sm-1 col-md-1"></div>
            </div>
        </div>
        <div id="push"></div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-subtitle"><span class="site-name">Martin Thoma</span> - A blog about Code, the Web and Cyberculture</li>
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>        <script src="//code.jquery.com/jquery.min.js"></script>
        <!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>
        <script>
(function(){
    'use strict';

    /*
    Create intra-page links
    Requires that your headings already have an `id` attribute set (because that's what jekyll does)
    For every heading in your page, this adds a little anchor link `#` that you can click to get a permalink to the heading.
    Ignores `h1`, because you should only have one per page.
    The text content of the tag is used to generate the link, so it will fail "gracefully-ish" if you have duplicate heading text.

    Credit: https://gist.github.com/SimplGy/a229d25cdb19d7f21231
     */

    var headingNodes = [], results, link,
        tags = ['h2', 'h3', 'h4', 'h5', 'h6'];

    tags.forEach(function(tag){
        var contentTag = document.getElementById('contentAfterTitle');
      results = contentTag.getElementsByTagName(tag);
      Array.prototype.push.apply(headingNodes, results);
    });

    headingNodes.forEach(function(node){
      link = document.createElement('a');
      link.className = 'deepLink';
      link.textContent = ' ¶';
      link.href = '#' + node.getAttribute('id');
      node.appendChild(link);
    });

  })();
</script>
    </body>
    <!-- Theme: Elegant built for Pelican
    License : http://oncrashreboot.com/pelican-elegant -->
</html>