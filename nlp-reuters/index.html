<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Martin Thoma" />
        <meta name="copyright" content="Martin Thoma" />
        <link title = "Martin Thoma"
              type  = "application/opensearchdescription+xml"
              rel   = "search"
              href  = "../opensearch.xml">

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="NLP, Reuters, Classification, Machine Learning, " />

<meta property="og:title" content="The Reuters Dataset "/>
<meta property="og:url" content="../nlp-reuters/" />
<meta property="og:description" content="Reuters is a benchmark dataset for document classification. To be more precise, it is a multi-class (e.g. there are multiple classes), multi-label (e.g. each document can belong to many classes) dataset. It has 90 classes, 7769 training documents and 3019 testing documents. It is the ModApte (R(90 …" />
<meta property="og:site_name" content="Martin Thoma" />
<meta property="og:article:author" content="Martin Thoma" />
<meta property="og:article:published_time" content="2017-07-27T20:00:00+02:00" />
<meta name="twitter:title" content="The Reuters Dataset ">
<meta name="twitter:description" content="Reuters is a benchmark dataset for document classification. To be more precise, it is a multi-class (e.g. there are multiple classes), multi-label (e.g. each document can belong to many classes) dataset. It has 90 classes, 7769 training documents and 3019 testing documents. It is the ModApte (R(90 …">
<meta property="og:image" content="logos/nlp.png" />
<meta name="twitter:image" content="logos/nlp.png" >

        <title>The Reuters Dataset  · Martin Thoma
</title>
        <!-- Latest compiled and minified CSS -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="../theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../static/custom.css" media="screen">

        <!-- MathJax -->
<script type="text/x-mathjax-config">
<!--
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [ ['$$', '$$'], ['\\[','\\]']],
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    processEscapes: true
  }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
// -->
</script>
<script type="text/javascript" async
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

        <link href="https://martin-thoma.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Martin Thoma - Full Atom Feed" />
        <link href="https://martin-thoma.com/feeds/index.xml" type="application/rss+xml" rel="alternate" title="Martin Thoma - Full RSS Feed" />
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top navbar-default">
            <div class="container">
                <div class="container-fluid">
                    <div class="collapse navbar-collapse">
                        <ul class="nav pull-left top-menu navbar-nav">
                            <li><a href=".." style="font-family: 'Monaco', 'Inconsolata', 'Andale Mono', 'Lucida Console', 'Bitstream Vera Sans Mono', 'Courier New', Courier, Monospace;
                        font-size: 20px;" class="navbar-brand">Martin Thoma</a>
                            </li>
                        </ul>
                        <ul class="nav pull-right top-menu navbar-nav">
                            <li ><a href="..">Home</a></li>
                            <li ><a href="../categories.html">Categories</a></li>
                            <li ><a href="../tags.html">Tags</a></li>
                            <li ><a href="../archives.html">Archives</a></li>
                            <li><a href="../support-me/">Support me</a></li>
                            <li><form class="navbar-form" action="../search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="search" class="search-query form-control" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row">
                <div class="col-sm-1 col-md-1"></div>
                <div class="col-sm-10 col-md-10">
<article>
<div class="row">
    <header class="page-header col-sm-10 col-md-10 col-md-offset-2">
    <h1><a href="../nlp-reuters/"> The Reuters Dataset  </a></h1>
    </header>
</div>

<div class="row">
    <div class="col-sm-2 col-md-2 table-of-content">
        <nav>
        <h4>Contents</h4>
        <div id="toc"><ul><li><a class="toc-href" href="#classes-and-labels" title="Classes and Labels">Classes and Labels</a></li><li><a class="toc-href" href="#multi-label-scoring" title="Multi-label Scoring">Multi-label Scoring</a></li><li><a class="toc-href" href="#classifier-comparison-tf-idf" title="Classifier comparison (tf-idf)">Classifier comparison (tf-idf)</a><ul><li><a class="toc-href" href="#multilayer-perceptron" title="Multilayer Perceptron">Multilayer Perceptron</a></li></ul></li><li><a class="toc-href" href="#code_1" title="Code">Code</a><ul><li><a class="toc-href" href="#data-loading" title="Data loading">Data loading</a></li><li><a class="toc-href" href="#experimenting-with-classifiers" title="Experimenting with classifiers">Experimenting with classifiers</a></li></ul></li><li><a class="toc-href" href="#see-also_1" title="See also">See also</a></li></ul></div>
        </nav>
    </div>
    <div class="col-sm-8 col-md-8 article-content" id="contentAfterTitle">

            
            <p>Reuters is a benchmark dataset for <a href="https://martin-thoma.com/document-classification/">document classification</a>.
To be more precise, it is a multi-class (e.g. there are multiple classes),
multi-label (e.g. each document can belong to many classes) dataset.
It has <strong>90 classes</strong>, <strong>7769 training documents</strong> and <strong>3019 testing documents</strong>.
It is the ModApte (R(90)) subest of the Reuters-21578 benchmark (<a href="https://stackoverflow.com/a/25149714/562769">source</a>).</p>
<p>The mean number of words per document, grouped by class, is between 93 and 1263
on the training set.</p>
<p>The training set has a <strong>vocabulary size of 35247</strong>. Even if you restrict it to
words which appear at least 5 times and at most 12672 times in the training
set, there are still 12017 words. Those 12017 words were used as features in
the following.</p>
<h2 id="classes-and-labels">Classes and Labels</h2>
<div class="highlight"><pre><span></span>                              documents
        class name            train   test    mean number of words in train set
     1: earn                : 2877    1087    104.4
     2: acq                 : 1650     719    150.1
     3: money-fx            :  538     179    219.0
     4: grain               :  433     149    223.6
     5: crude               :  389     189    247.3
     6: trade               :  368     117    294.3
     7: interest            :  347     131    198.0
     8: wheat               :  212      71    225.6
     9: ship                :  197      89    203.7
    10: corn                :  181      56    259.1
    11: money-supply        :  140      34    170.5
    12: dlr                 :  131      44    230.4
    13: sugar               :  126      36    247.2
    14: oilseed             :  124      47    277.9
    15: coffee              :  111      28    264.1
    16: gnp                 :  101      35    372.8
    17: gold                :   94      30    188.5
    18: veg-oil             :   87      37    291.0
    19: soybean             :   78      33    347.9
    20: livestock           :   75      24    222.9
    21: nat-gas             :   75      30    257.7
    22: bop                 :   75      30    288.1
    23: cpi                 :   69      28    235.4
    24: cocoa               :   55      18    266.4
    25: reserves            :   55      18    216.1
    26: carcass             :   50      18    259.7
    27: copper              :   47      18    201.6
    28: jobs                :   46      21    232.7
    29: yen                 :   45      14    282.8
    30: ipi                 :   41      12    232.1
    31: iron-steel          :   40      14    220.5
    32: cotton              :   39      20    366.3
    33: barley              :   37      14    272.9
    34: gas                 :   37      17    209.4
    35: rubber              :   37      12    274.5
    36: alum                :   35      23    180.5
    37: rice                :   35      24    359.8
    38: palm-oil            :   30      10    234.5
    39: meal-feed           :   30      19    387.1
    40: sorghum             :   24      10    511.3
    41: retail              :   23       2    324.8
    42: zinc                :   21      13    189.9
    43: silver              :   21       8    221.0
    44: pet-chem            :   20      12    204.9
    45: wpi                 :   19      10    200.9
    46: tin                 :   18      12    322.1
    47: rapeseed            :   18       9    168.6
    48: orange              :   16      11    169.6
    49: strategic-metal     :   16      11    205.6
    50: housing             :   16       4    207.2
    51: hog                 :   16       6    162.3
    52: lead                :   15      14    216.7
    53: soy-oil             :   14      11    568.7
    54: heat                :   14       5    190.4
    55: fuel                :   13      10    194.6
    56: soy-meal            :   13      13    551.0
    57: lei                 :   12       3    134.7
    58: sunseed             :   11       5    425.1
    59: dmk                 :   10       4    212.1
    60: lumber              :   10       6    242.1
    61: tea                 :    9       4    365.1
    62: income              :    9       7    286.4
    63: nickel              :    8       1    193.6
    64: oat                 :    8       6    648.4
    65: l-cattle            :    6       2    298.0
    66: instal-debt         :    5       1    134.3
    67: platinum            :    5       7    174.8
    68: groundnut           :    5       4    258.0
    69: rape-oil            :    5       3    167.2
    70: sun-oil             :    5       2    201.9
    71: coconut-oil         :    4       3    471.4
    72: jet                 :    4       1    109.6
    73: coconut             :    4       2    264.5
    74: propane             :    3       3    352.0
    75: potato              :    3       3    161.2
    76: cpu                 :    3       1    133.2
    77: rand                :    2       1    345.7
    78: palmkernel          :    2       1    326.3
    79: copra-cake          :    2       1    495.0
    80: dfl                 :    2       1    764.7
    81: naphtha             :    2       4    206.7
    82: palladium           :    2       1    93.0
    83: nzdlr               :    2       2    508.8
    84: groundnut-oil       :    1       1    277.5
    85: castor-oil          :    1       1    194.0
    86: sun-meal            :    1       1    153.0
    87: lin-oil             :    1       1    262.5
    88: cotton-oil          :    1       2    1262.7
    89: rye                 :    1       1    383.0
    90: nkr                 :    1       2    122.3
</pre></div>
<p>By far most documents have either one or two labels, but some have up to 15:</p>
<div class="highlight"><pre><span></span>labelcount= 1, documentcount=9160
labelcount= 2, documentcount=1173
labelcount= 3, documentcount= 255
labelcount= 4, documentcount=  91
labelcount= 5, documentcount=  52
labelcount= 6, documentcount=  27
labelcount= 7, documentcount=   9
labelcount= 8, documentcount=   7
labelcount= 9, documentcount=   5
labelcount=10, documentcount=   3
labelcount=11, documentcount=   2
labelcount=14, documentcount=   2
labelcount=12, documentcount=   1
labelcount=15, documentcount=   1
</pre></div>
<p>Let's look at the relationship between the classes. Which classes occur often
together? Are there classes which can be used to predict the presence of other
classes? For example, <code>wheat</code> should imply <code>grain</code>.</p>
<p>Here are the 50 strongest predictors:</p>
<div class="highlight"><pre><span></span>castor-oil =&gt; cotton-oil (0.999742566611)
castor-oil =&gt; groundnut-oil (0.999742566611)
castor-oil =&gt; lin-oil (0.999742566611)
castor-oil =&gt; nkr (0.999742566611)
castor-oil =&gt; rye (0.999742566611)
castor-oil =&gt; sun-meal (0.999742566611)
copra-cake =&gt; palmkernel (0.999742566611)
cotton-oil =&gt; castor-oil (0.999742566611)
cotton-oil =&gt; groundnut-oil (0.999742566611)
cotton-oil =&gt; lin-oil (0.999742566611)
cotton-oil =&gt; nkr (0.999742566611)
cotton-oil =&gt; rye (0.999742566611)
cotton-oil =&gt; sun-meal (0.999742566611)
groundnut-oil =&gt; castor-oil (0.999742566611)
groundnut-oil =&gt; cotton-oil (0.999742566611)
groundnut-oil =&gt; lin-oil (0.999742566611)
groundnut-oil =&gt; nkr (0.999742566611)
groundnut-oil =&gt; rye (0.999742566611)
groundnut-oil =&gt; sun-meal (0.999742566611)
lin-oil =&gt; castor-oil (0.999742566611)
lin-oil =&gt; cotton-oil (0.999742566611)
lin-oil =&gt; groundnut-oil (0.999742566611)
lin-oil =&gt; nkr (0.999742566611)
lin-oil =&gt; rye (0.999742566611)
lin-oil =&gt; sun-meal (0.999742566611)
nkr =&gt; castor-oil (0.999742566611)
nkr =&gt; cotton-oil (0.999742566611)
nkr =&gt; groundnut-oil (0.999742566611)
nkr =&gt; lin-oil (0.999742566611)
nkr =&gt; rye (0.999742566611)
nkr =&gt; sun-meal (0.999742566611)
palmkernel =&gt; copra-cake (0.999742566611)
rye =&gt; castor-oil (0.999742566611)
rye =&gt; cotton-oil (0.999742566611)
rye =&gt; groundnut-oil (0.999742566611)
rye =&gt; lin-oil (0.999742566611)
rye =&gt; nkr (0.999742566611)
rye =&gt; sun-meal (0.999742566611)
sun-meal =&gt; castor-oil (0.999742566611)
sun-meal =&gt; cotton-oil (0.999742566611)
sun-meal =&gt; groundnut-oil (0.999742566611)
sun-meal =&gt; lin-oil (0.999742566611)
sun-meal =&gt; nkr (0.999742566611)
sun-meal =&gt; rye (0.999742566611)
castor-oil =&gt; copra-cake (0.999613849916)
castor-oil =&gt; dfl (0.999613849916)
castor-oil =&gt; naphtha (0.999613849916)
castor-oil =&gt; nzdlr (0.999613849916)
castor-oil =&gt; palladium (0.999613849916)
castor-oil =&gt; palmkernel (0.999613849916)
</pre></div>
<h2 id="multi-label-scoring">Multi-label Scoring</h2>
<p>I've never been in a multi-label context, so it was not directly clear to me
which scoring is used. Thanks to Chirag Nagpal who pointed me in the right
direction:</p>
<ul>
<li><strong>Accuracy</strong>: For each document, one has to make a decision for each possible
  category. As most documents belong to one or two categories, the simplest
  classifier simply decides all the time that the document does not belong to
  any category. For the used dataset, this leads to an accuracy of 0.986. Hence
  accuracy is not suitable.</li>
<li>Micro/macro averaged ROC or Precision/Recall curve:<ul>
<li>Micro: Calculate metrics globally by counting the total true positives,
  false negatives and false positives.</li>
<li>Macro: Calculate metrics for each label, and find their unweighted mean.
  This does not take label imbalance into account.</li>
</ul>
</li>
<li>F1 score: See <a href="http://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics">user manual</a></li>
<li>Coverage error: See <a href="http://scikit-learn.org/stable/modules/model_evaluation.html#coverage-error">user manual</a></li>
</ul>
<p>A nice overview is given by <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.364.5612&amp;rep=rep1&amp;type=pdf#page=13">A Literature Survey on Algorithms for Multi-label Learning</a>.</p>
<h2 id="classifier-comparison-tf-idf">Classifier comparison (tf-idf)</h2>
<p>The following are the accuracies as well as the training and test times. All
classifiers got the same tf-idf features.</p>
<div class="highlight"><pre><span></span>vocabulary size = 26147
</pre></div>
<div class="highlight"><pre><span></span>Classifier                      Acc     F1
-------------------------------------------------------------------------------
LinearSVC                     : 99.66% 86.61% in   31.00s train /    9.72s test
Logistic Regression (C=1000)  : 99.65% 86.33% in   38.70s train /    9.75s test
MLP (3 Layer)                 : 99.63% 86.00% in  439.13s train /    1.37s test
MLP (2 Layer)                 : 99.58% 83.02% in  328.98s train /    1.44s test
Logistic Regression (C=1)     : 99.44% 75.25% in   32.62s train /    9.44s test
k nn 5                        : 99.44% 78.43% in    8.80s train / 1087.94s test
k nn 3                        : 99.41% 77.32% in    9.34s train / 1174.55s test
Random Forest (200 estimators): 99.40% 72.73% in   56.71s train /    6.49s test
Random Forest (50 estimators) : 99.39% 72.56% in   13.55s train /    1.18s test
Decision Tree                 : 99.21% 63.86% in   12.42s train /    0.11s test
Naive Bayes                   : 98.75% 50.04% in  236.15s train /  102.17s test
SVM, linear                   :   (bad)       in 6875.84s train / 2501.81s test
</pre></div>
<p>There are a couple of things to notice here:</p>
<ul>
<li><strong>Speed</strong>:<ul>
<li>Naive Bayes and k-nn is slow</li>
<li>Random forests and MLP are fast</li>
<li>SVM depends extremely on the implementation (see <a href="https://stackoverflow.com/q/45384185/562769">What is the difference between LinearSVC and SVC(kernel=&ldquo;linear&rdquo;)?</a>)</li>
</ul>
</li>
<li><strong>Prediction Quality</strong>:<ul>
<li>LinearSVC, logistic regression and MLP are accurate</li>
<li>Achieving high accuracy seems to be easier than achieving high F1 scores.</li>
</ul>
</li>
</ul>
<p>The MLP has a reasonable prediction quality and test time.</p>
<h3 id="multilayer-perceptron">Multilayer Perceptron</h3>
<p>When training a multilayer perceptron for a multi-label classification task, there
are two important things to look for:</p>
<ul>
<li><strong>Output layer</strong>: Do not  use softmax, as the normalization does not make
  sense in this case.</li>
<li><strong>Loss</strong>: Use <a href="https://keras.io/losses/#binary_crossentropy"><code>`binary_crossentropy</code></a></li>
</ul>
<p>When you print precision, recall, F1-score and accuracy you note the following:</p>
<ul>
<li>Accuracy gets to 98% in the first epoch and over 99% in the second. It stays
  that high.</li>
<li>Precision is at about 4% in the first epoch and over 97% in the second. It
  stays that high.</li>
<li>Recall needs about 15 epochs of steady progress to get over 98%.</li>
</ul>
<p>As a consequence, the F1 score steadily increases. Hence by using other metrics
one can see that the classifier makes great improvements, although the accuracy
is pretty high from the beginning.</p>
<h2 id="code_1">Code</h2>
<p>See <a href="https://github.com/MartinThoma/algorithms/blob/master/ML/nlp/">GitHub</a>.</p>
<p>If you use it, please cite this article or link to this blog post:</p>
<div class="highlight"><pre><span></span>@Misc{Thom2017-reuters,
  Title                    = {The Reuters Dataset},

  Author                   = {Martin Thoma},
  Month                    = jul,
  Year                     = {2017},

  Url                      = {https://martin-thoma.com/nlp-reuters}
}
</pre></div>
<h3 id="data-loading">Data loading</h3>
<div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>
<span class="c1"># -*- coding: utf-8 -*-</span>

<span class="sd">"""</span>
<span class="sd">Utility file for the Reuters text categorization benchmark dataset.</span>

<span class="sd">See also</span>
<span class="sd">--------</span>
<span class="sd">http://www.vision.caltech.edu/Image_Datasets/Caltech101/</span>
<span class="sd">"""</span>

<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">reuters</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MultiLabelBinarizer</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>

<span class="n">n_classes</span> <span class="o">=</span> <span class="mi">90</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">reuters</span><span class="o">.</span><span class="n">categories</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">load_data</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="p">{}):</span>
    <span class="sd">"""</span>
<span class="sd">    Load the Reuters dataset.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.</span>
<span class="sd">    """</span>
    <span class="n">stop_words</span> <span class="o">=</span> <span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s2">"english"</span><span class="p">)</span>
    <span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="n">stop_words</span><span class="p">)</span>
    <span class="n">mlb</span> <span class="o">=</span> <span class="n">MultiLabelBinarizer</span><span class="p">()</span>

    <span class="n">documents</span> <span class="o">=</span> <span class="n">reuters</span><span class="o">.</span><span class="n">fileids</span><span class="p">()</span>
    <span class="n">test</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">documents</span> <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">'test/'</span><span class="p">)]</span>
    <span class="n">train</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">documents</span> <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">'training/'</span><span class="p">)]</span>

    <span class="n">docs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">docs</span><span class="p">[</span><span class="s1">'train'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">reuters</span><span class="o">.</span><span class="n">raw</span><span class="p">(</span><span class="n">doc_id</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc_id</span> <span class="ow">in</span> <span class="n">train</span><span class="p">]</span>
    <span class="n">docs</span><span class="p">[</span><span class="s1">'test'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">reuters</span><span class="o">.</span><span class="n">raw</span><span class="p">(</span><span class="n">doc_id</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc_id</span> <span class="ow">in</span> <span class="n">test</span><span class="p">]</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'train'</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">'test'</span><span class="p">:</span> <span class="p">[]}</span>
    <span class="n">xs</span><span class="p">[</span><span class="s1">'train'</span><span class="p">]</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">[</span><span class="s1">'train'</span><span class="p">])</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
    <span class="n">xs</span><span class="p">[</span><span class="s1">'test'</span><span class="p">]</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">docs</span><span class="p">[</span><span class="s1">'test'</span><span class="p">])</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'train'</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">'test'</span><span class="p">:</span> <span class="p">[]}</span>
    <span class="n">ys</span><span class="p">[</span><span class="s1">'train'</span><span class="p">]</span> <span class="o">=</span> <span class="n">mlb</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([</span><span class="n">reuters</span><span class="o">.</span><span class="n">categories</span><span class="p">(</span><span class="n">doc_id</span><span class="p">)</span>
                                     <span class="k">for</span> <span class="n">doc_id</span> <span class="ow">in</span> <span class="n">train</span><span class="p">])</span>
    <span class="n">ys</span><span class="p">[</span><span class="s1">'test'</span><span class="p">]</span> <span class="o">=</span> <span class="n">mlb</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="n">reuters</span><span class="o">.</span><span class="n">categories</span><span class="p">(</span><span class="n">doc_id</span><span class="p">)</span>
                                <span class="k">for</span> <span class="n">doc_id</span> <span class="ow">in</span> <span class="n">test</span><span class="p">])</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'x_train'</span><span class="p">:</span> <span class="n">xs</span><span class="p">[</span><span class="s1">'train'</span><span class="p">],</span> <span class="s1">'y_train'</span><span class="p">:</span> <span class="n">ys</span><span class="p">[</span><span class="s1">'train'</span><span class="p">],</span>
            <span class="s1">'x_test'</span><span class="p">:</span> <span class="n">xs</span><span class="p">[</span><span class="s1">'test'</span><span class="p">],</span> <span class="s1">'y_test'</span><span class="p">:</span> <span class="n">ys</span><span class="p">[</span><span class="s1">'test'</span><span class="p">],</span>
            <span class="s1">'labels'</span><span class="p">:</span> <span class="nb">globals</span><span class="p">()[</span><span class="s2">"labels"</span><span class="p">]}</span>
    <span class="k">return</span> <span class="n">data</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">'__main__'</span><span class="p">:</span>
    <span class="n">config</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"len(data['x_train'])={}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">'x_train'</span><span class="p">])))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"data['x_train'].shape={}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">'x_train'</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
<h3 id="experimenting-with-classifiers">Experimenting with classifiers</h3>
<h2 id="see-also_1">See also</h2>
<ul>
<li>NLTK: <a href="http://www.nltk.org/book/ch02.html">Other datasets</a></li>
<li><a href="http://www.daviddlewis.com/resources/testcollections/reuters21578/">Reuters-21578</a></li>
<li>Publications:<ul>
<li><a href="http://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf">Text Categorization with Support Vector Machines: Learning with Many Relevant Features</a></li>
</ul>
</li>
<li>Blog posts:<ul>
<li>Miguel Malvarez: <a href="https://miguelmalvarez.com/2015/03/20/classifying-reuters-21578-collection-with-python-representing-the-data/">Classifying Reuters-21578 collection with Python: Representing the data</a></li>
<li>Miguel Malvarez: <a href="https://miguelmalvarez.com/2016/11/07/classifying-reuters-21578-collection-with-python/">Classifying Reuters-21578 collection with Python</a></li>
</ul>
</li>
</ul>
            
            <div id="disqus_thread"></div>
<script>
    /*
    var disqus_config = function () {
        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW
        var d = document, s = d.createElement('script');

        s.src = '//martinthoma.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

            <hr/>
        </div>
        <section>
        <div class="col-sm-2 col-md-2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time pubdate="pubdate" datetime="2017-07-27T20:00:00+02:00">Jul 27, 2017</time>
            <br/>
            by <a rel="author" class="vcard author post-author" itemprop="author" href="../author/martin-thoma/"><span class="fn" itemscope="" itemtype="https://schema.org/Person"><span itemprop="name">Martin Thoma</span></span></a>
            <h4>Category</h4>
            <a class="category-link" href="../categories.html#machine-learning-ref">Machine Learning</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="../tags.html#classification-ref">Classification
                    <span>6</span>
</a></li>
                <li><a href="../tags.html#nlp-ref">NLP
                    <span>3</span>
</a></li>
                <li><a href="../tags.html#reuters-ref">Reuters
                    <span>1</span>
</a></li>
            </ul>
<h4>Contact</h4>
    <a href="https://twitter.com/themoosemind" title="My Twitter Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-twitter sidebar-social-links"></i></a>
    <a href="mailto:info@martin-thoma.de" title="My Email Address" class="sidebar-social-links" target="_blank">
    <i class="fa fa-envelope sidebar-social-links"></i></a>
    <a href="https://github.com/MartinThoma" title="My Github Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-github sidebar-social-links"></i></a>
    <a href="http://stackoverflow.com/users/562769/martin-thoma" title="My Stackoverflow Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-stackoverflow sidebar-social-links"></i></a>
        </div>
        </section>
</div>
</article>
                </div>
                <div class="col-sm-1 col-md-1"></div>
            </div>
        </div>
        <div id="push"></div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-subtitle"><span class="site-name">Martin Thoma</span> - A blog about Code, the Web and Cyberculture</li>
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>        <script src="//code.jquery.com/jquery.min.js"></script>
        <!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>
        <script>
(function(){
    'use strict';

    /*
    Create intra-page links
    Requires that your headings already have an `id` attribute set (because that's what jekyll does)
    For every heading in your page, this adds a little anchor link `#` that you can click to get a permalink to the heading.
    Ignores `h1`, because you should only have one per page.
    The text content of the tag is used to generate the link, so it will fail "gracefully-ish" if you have duplicate heading text.

    Credit: https://gist.github.com/SimplGy/a229d25cdb19d7f21231
     */

    var headingNodes = [], results, link,
        tags = ['h2', 'h3', 'h4', 'h5', 'h6'];

    tags.forEach(function(tag){
        var contentTag = document.getElementById('contentAfterTitle');
      results = contentTag.getElementsByTagName(tag);
      Array.prototype.push.apply(headingNodes, results);
    });

    headingNodes.forEach(function(node){
      link = document.createElement('a');
      link.className = 'deepLink';
      link.textContent = ' ¶';
      link.href = '#' + node.getAttribute('id');
      node.appendChild(link);
    });

  })();
</script>
    </body>
    <!-- Theme: Elegant built for Pelican
    License : http://oncrashreboot.com/pelican-elegant -->
</html>