<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Martin Thoma" />
        <meta name="copyright" content="Martin Thoma" />
        <link title = "Martin Thoma"
              type  = "application/opensearchdescription+xml"
              rel   = "search"
              href  = "../opensearch.xml">

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="Klausur, German posts, " />

<meta property="og:title" content="Probabilistische Planung "/>
<meta property="og:url" content="../probabilistische-planung/" />
<meta property="og:description" content="Dieser Artikel beschäftigt sich mit der Vorlesung „Probabilistische Planung“ am KIT. Er dient als Prüfungsvorbereitung. Ich habe die Vorlesungen bei Herrn Dr.-Ing. Marco Huber im Sommersemester 2015 und 2016 gehört. Der Artikel dient als Prüfungsvorbereitung und ist noch am Entstehen. In der Vorlesung &#39;Probabilistische Planung ..." />
<meta property="og:site_name" content="Martin Thoma" />
<meta property="og:article:author" content="Martin Thoma" />
<meta property="og:article:published_time" content="2016-05-11T20:00:00+02:00" />
<meta name="twitter:title" content="Probabilistische Planung ">
<meta name="twitter:description" content="Dieser Artikel beschäftigt sich mit der Vorlesung „Probabilistische Planung“ am KIT. Er dient als Prüfungsvorbereitung. Ich habe die Vorlesungen bei Herrn Dr.-Ing. Marco Huber im Sommersemester 2015 und 2016 gehört. Der Artikel dient als Prüfungsvorbereitung und ist noch am Entstehen. In der Vorlesung &#39;Probabilistische Planung ...">
<meta property="og:image" content="logos/klausur.png" />
<meta name="twitter:image" content="logos/klausur.png" >

        <title>Probabilistische Planung  · Martin Thoma
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="../theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../static/custom.css" media="screen">

        <!-- MathJax -->
<script type="text/x-mathjax-config">
<!--
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [ ['$$', '$$'], ['\\[','\\]']],
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    processEscapes: true
  }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
// -->
</script>
<script type="text/javascript" async
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

        <link href="https://martin-thoma.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Martin Thoma - Full Atom Feed" />
        <link href="https://martin-thoma.com/feeds/index.xml" type="application/rss+xml" rel="alternate" title="Martin Thoma - Full RSS Feed" />
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container-fluid">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="../"><span class=site-name>Martin Thoma</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="..">Home</a></li>
                            <li ><a href="../categories.html">Categories</a></li>
                            <li ><a href="../tags.html">Tags</a></li>
                            <li ><a href="../archives.html">Archives</a></li>
                            <li><form class="navbar-search" action="../search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="search" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article>
<div class="row-fluid">
    <header class="page-header span10 offset2">
    <h1><a href="../probabilistische-planung/"> Probabilistische Planung  </a></h1>
    </header>
</div>

<div class="row-fluid">
    <div class="span2 table-of-content">
        <nav>
        <h4>Contents</h4>
        <div id="toc"><ul><li><a class="toc-href" href="#behandelter-stoff" title="Behandelter Stoff">Behandelter Stoff</a><ul><li><a class="toc-href" href="#ubersicht" title="&Uuml;bersicht">&Uuml;bersicht</a></li><li><a class="toc-href" href="#grundlagen" title="Grundlagen">Grundlagen</a></li><li><a class="toc-href" entscheidungs­probleme'="" href="#markovsche-entscheidungsprobleme" sche="" title="Markov">Markov'sche Entscheidungs&shy;probleme</a></li><li><a class="toc-href" href="#pomdps" title="POMDPs">POMDPs</a></li><li><a class="toc-href" href="#reinforcement-learning" title="Reinforcement Learning">Reinforcement Learning</a></li></ul></li><li><a class="toc-href" href="#prufungsfragen_1" title="Pr&uuml;fungsfragen">Pr&uuml;fungsfragen</a><ul><li><a class="toc-href" href="#mdp" title="MDP">MDP</a></li><li><a class="toc-href" href="#pomdp" title="POMDP">POMDP</a></li><li><a class="toc-href" href="#rl" title="RL">RL</a></li></ul></li><li><a class="toc-href" href="#notation_1" title="Notation">Notation</a></li><li><a class="toc-href" href="#material-und-links" title="Material und Links">Material und Links</a></li><li><a class="toc-href" href="#vorlesungsempfehlungen" title="Vorlesungs&shy;empfehlungen">Vorlesungs&shy;empfehlungen</a></li><li><a class="toc-href" href="#termine-und-klausurablauf" title="Termine und Klausurablauf">Termine und Klausurablauf</a></li></ul></div>
        </nav>
    </div>
    <div class="span8 article-content" id="contentAfterTitle">

            
            <div class="info">Dieser Artikel besch&auml;ftigt sich mit der Vorlesung &bdquo;Probabilistische Planung&ldquo; am KIT. Er dient als Pr&uuml;fungsvorbereitung. Ich habe die Vorlesungen bei <a href="http://ies.anthropomatik.kit.edu/mitarbeiter.php?person=huber">Herrn Dr.-Ing. Marco Huber</a> im Sommersemester 2015 und 2016 geh&ouml;rt. Der Artikel dient als Pr&uuml;fungsvorbereitung und ist noch am Entstehen.</div>
<p>In der Vorlesung 'Probabilistische Planung' werden drei Themen besprochen:</p>
<ul>
<li>Markov'sche Entscheidungsprobleme (MDPs)</li>
<li>Planung bei Messunsicherheiten</li>
<li>Reinforcement Learning (RL)</li>
</ul>
<h2 id="behandelter-stoff">Behandelter Stoff</h2>
<h3 id="ubersicht">&Uuml;bersicht</h3>
<table>
<tr>
<th>Datum</th>
<th>Kapitel</th>
<th>Inhalt</th>
</tr>
<tr>
<td>26.04.2016</td>
<td>Grundlagen</td>
<td>Wahrscheinlichkeitsraum, Grundraum, Ereignis&shy;raum, Resultate,
        Elementar&shy;ereignis, $\sigma$-Algebra, Wahrscheinlichkeits&shy;ma&szlig;,
        Bedingte Wahrscheinlichkeit, Ziegenproblem, Dichtefunktion</td>
</tr>
<tr>
<td>28.04.2016</td>
<td>Grundlagen</td>
<td>Allais-Paradoxon, Nutzentheorie, Pr&auml;ferenzrelation, Nutzenfunktion</td>
</tr>
<tr>
<td>06.05.2016</td>
<td>Grundlagen</td>
<td>Einf&uuml;hrung in die Optimierungstheorie: Notwendige und Hinreichende
        Bedingungen, Konvexe Optimierung, Numerische Methoden</td>
</tr>
<tr>
<td>11.05.2016</td>
<td>MDPs</td>
<td>Definition eines MDP, Plan vs. Strategie, <abbr title="Dynamische Programmierung">DP</abbr></td>
</tr>
<tr>
<td id="2016-05-18">18.05.2016</td>
<td>MDPs</td>
<td>Endliche Planungsprobleme, Value- und Policy-Iteration</td>
</tr>
<tr>
<td id="2016-05-25">25.05.2016</td>
<td>MDPs</td>
<td>K&uuml;rzeste-Wege Suche (Tiefensuche, Breitensuche, Dijkstra, A*, Branch &amp; Bound; Label-Korrektur-Algorithmus); Trellis-Diagramm; Differentialantrieb; Pontryagin's Minimumprinzip</td>
</tr>
<tr>
<td id="2016-06-01">01.06.2016</td>
<td>MDPs</td>
<td>Pontryagin's Minimumprinzip, Hamilton-Funktion; LQR; Sicherheits&auml;quivalenz</td>
</tr>
<tr>
<td id="2016-06-08">08.06.2016</td>
<td>POMDPs</td>
<td>Motivation und Definition von POMDP; Hinreichende Statistik; Bayes-Sch&auml;tzer</td>
</tr>
<tr>
<td id="2016-06-15">15.06.2016</td>
<td>POMDPs</td>
<td>Lineare Planungsprobleme (Kalman-Filter); Sperationsproblem</td>
</tr>
<tr>
<td id="2016-06-22">22.06.2016</td>
<td>POMDPs</td>
<td>Endliche Planungsprobleme (Optimale Strategie); <a href="#ol-planung">OL</a>, <a href="#olf-planung">OLF</a>, Modellpr&auml;diktive Planung</td>
</tr>
<tr>
<td id="2016-06-29">29.06.2016</td>
<td>POMDPs</td>
<td>Parametrische / Nichtparametrische approximative Planung (Sicherheits&auml;quivalenz bei deterministischen Problemen); Funktionsapproximatoren f&uuml;r Wertefunktion / Strategie; Sensoreinsatzplanung</td>
</tr>
<tr>
<td id="2016-07-06">06.07.2016</td>
<td>POMDPs, <abbr title="Reinforcement Learning">RL</abbr></td>
<td>POMDPs: Sensoreinsatzplanung</td>
</tr>
</table>
<p>Folien:</p>
<ul>
<li>25.05.2016: Folie 4 - Die Knoten sind Zust&auml;nde und die Kanten sind Aktionen</li>
<li><span class="math">\(g_{ij}^k = \infty\)</span>: Kein &Uuml;bergang von <span class="math">\(i\)</span> nach <span class="math">\(j\)</span> in Schritt <span class="math">\(k\)</span>.</li>
</ul>
<h3 id="grundlagen">Grundlagen</h3>
<p>Slides: <code>ProPlan-1-Anschrieb.pdf</code></p>
<dl>
<dt><dfn>$\sigma$-Algebra</dfn></dt>
<dd>Sei $S$ eine Menge und $\mathcal{A}$ ein Menge aus Teilmengen von $S$.
      $\mathcal{A}$ hei&szlig;t eine $\sigma$-Algebra &uuml;ber $S$, genau dann, wenn
      gilt:

      <ul>
<li>$S \in \mathcal{A}$</li>
<li>$\forall M \in \mathcal{A} \Rightarrow (S \setminus M) \in \mathcal{A}$</li>
<li>$M_1, M_2, \dots \in \mathcal{A} \Rightarrow \bigcup_{n \in \mathbb{N}} M_n \in \mathcal{A}$</li>
</ul>
</dd>
<dt><dfn>Wahrscheinlichkeitsma&szlig;</dfn></dt>
<dd>Eine Funktion $P: A \rightarrow \mathbb{R}$ hei&szlig;t Wahrscheinlichkeitsma&szlig;,
      wenn die Kolmogorov'schen Axiome gelten:

      <ul>
<li>$\forall M \in \mathcal{A}: P(M) \geq 0$</li>
<li>$\forall P(S) = 1$</li>
<li>$M_1, M_2 \in \mathcal{A} \land M_1 \cap M_2 = \emptyset \Rightarrow P(M_1 \cup M_2) = P(M_1) + P(M_2)$</li>
</ul>
</dd>
<dt><dfn>Normalverteilung</dfn></dt>
<dd>Die Normalverteilung $\mathcal{N}(\mu, \sigma^2)$ ist eine
      kontinuierliche Verteilung mit der Dichtefunktion

      $$f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{- \frac{(x - \mu)^2}{2\sigma^2}}$$


      </dd>
</dl>
<h3 id="markovsche-entscheidungsprobleme">Markov'sche Entscheidungs&shy;probleme</h3>
<p>Slides: <code>11.05.2016</code></p>
<dl>
<dt><dfn id="mdp">Markov'sches Entscheidungsproblem</dfn> (<dfn>Markov Decision Process</dfn>, <dfn>MDP</dfn>)</dt>
<dd>Ein MDP wird durch 8&nbsp;Eigenschaften gekennzeichnet:

      <ol>
<li>Zustandsraum $X \subseteq \mathbb{R}^n$ mit Zust&auml;nden
              $x \in \mathcal{X}$.</li>
<li>Diskrete Zeitschritte $k=0, 1, \dots, N$ mit Endzeitpunkt
              $N$. Dabei ist der 0-te Schritt gegeben.</li>
<li>Initialzustand $x_o \in \mathcal{X}$ des Agenten zum Zeitpunkt $k=0$.</li>
<li>Nichtleere Aktionsmenge $A_k(x_k) \subseteq A$ mit Aktion $a_k$.
              H&auml;ufig $A_k(x_k)=A$ f&uuml;r alle $k=0, \dots, N$ (Zeit- und Zustandsinvarianz)</li>
<li>&Uuml;bergangswahrscheinlichkeit $x_{k+1} \sim P_x(\cdot | x_k, a_k)$.<br/>
              Markov-Annahme: $P_x(\cdot | x_k, a_k) = P(\cdot | x_{0:k}, a_{0:k})$,
              wobei die Notation $x_{0:k} = x_0, x_1, \dots, x_k$ bedeutet.
              Das hei&szlig;t, der Folgezustand ist nur vom Zustand $x_k$ und
              der gew&auml;hlten Aktion $a_k$ abh&auml;ngig.<br/>
              Im Fall diskreter Zust&auml;nde ist die
              &Uuml;bergangs&shy;wahrscheinlichkeit eine bedingte Z&auml;hldichte:
              $$f(x_{k+1} | x_k, a_k) = P_x(x=x_{k+1} | x_k, a_k)$$<br/>
              Bei kontinuierlichen Zust&auml;nden eine bedingte Wahrscheinlichkeits&shy;dichte:
              $$f(x_{k+1} | x_k, a_k) = \frac{\partial F(x | x_k, a_k)}{\partial x} |_{x=x_{k+1}}$$</li>
<li>Additive Kostenfunktion
              $$g_N (x_N) + \sum_{k=0}^{N-1} g_k(x_k, a_k)$$
              wobei $g_N$ die terminalen Kosten und $g_k$ Schrittkosten genannt
              werden.</li>
<li>Der Zustand ist f&uuml;r jedes $k$ <strong>direkt beobachtbar</strong>.

              <ul>
<li><strong>Vor</strong> Anwendung bzw Auswahl einer Aktion
                      $a_k$ zum Zeitpunkt $k$
                      $$x_{k+1} \sim P_x(\cdot | x_k, a_k)$$
                      wobei $x_k, a_k$ exakt bekannt sind.</li>
<li><strong>Nach</strong> Anwendung der Aktion $a_k$ zum
                      Zeitpunkt $k+1$ ist $x_{k+1}$ exakt bekannt.</li>
</ul>
</li>
<li><strong>Ziel</strong>: Minimierung der erwarteten Kosten
              $$J_{\pi_{0:N-1}}(x_0) := \mathbb{E} \left (g_N(x_k) + \sum_{k=0}^{N-1} g_k (x_k, \pi_k(x_k)) \right )$$
              bzgl. einer Strategie $\pi_{0:N-1} = (\pi_0, \pi_1, \dots, \pi_{N-1})$
              mit Funktionen $\pi_k(x_k) = a_k \in A_k(x_k)$.</li>
</ol>
</dd>
<dt><dfn id="policy">Strategie</dfn> (<dfn>policy</dfn>)</dt>
<dd>Eine Strategie ist ein Plan mit Zustandsr&uuml;ckf&uuml;hrung.</dd>
<dt><a href="https://de.wikipedia.org/wiki/Pr%C3%A4ferenzrelation"><dfn>Pr&auml;ferenzrelation</dfn></a></dt>
<dd>Sei $\mathcal{X}$ eine Zustandsmenge und $\geq \subseteq \mathcal{X} \times \mathcal{X}$
      eine bin&auml;re Relation auf $\mathcal{X}$. $\geq$ hei&szlig;t (schwache)
      Pr&auml;ferenzrelation, wenn gilt:
      <ul>
<li>$\geq$ ist vollst&auml;ndig: $\forall x, y \in \mathcal{X}: x \geq y \lor y \geq x$</li>
<li>$\geq$ ist transitiv: $\forall x, y, z \in \mathcal{X}: x \geq y \land y \geq z \Rightarrow x \leq z$</li>
</ul></dd>
<dt><a href="https://de.wikipedia.org/wiki/Nutzenfunktion"><dfn id="nutzenfunktion">Nutzenfunktion</dfn></a></dt>
<dd>Sei $\mathcal{X}$ eine Zustandsmenge und $u: \mathcal{X} \rightarrow \mathbb{R}$
      eine Funktion. Sei au&szlig;erdem $\geq$ eine Pr&auml;ferenzrelation. $u$ hei&szlig;t
      eine Nutzenfunktion welche $\geq$ abbildet, wenn gilt:
      $$\forall x, y \in \mathcal{X}: x \geq y \Leftrightarrow u(x) \geq u(y)$$</dd>
<dt><a href="https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem#The_axioms"><dfn>Von-Neumann-Morgenstern Axiome</dfn></a></dt>
<dd>Sei $\mathcal{X}$ eine Zustandsmenge und $\mathcal{P}$ die Menge aller
      Verteilungen $P: \mathcal{X} \rightarrow [0, 1]$.

        <ol>
<li id="VNM-1">$\geq$ ist eine Pr&auml;ferenzrelation</li>
<li id="VNM-2">Unabh&auml;ngigkeitsaxiom: Gilt f&uuml;r $P, Q \in \mathcal{P}$ die
                Beziehung $P \geq Q$, dann gilt auch:
                $$\alpha \cdot P + (1 - \alpha) R \geq \alpha Q + (1 - \alpha) R$$
                f&uuml;r beliebiges $R \in \mathcal{P}$ und beliebiges $\alpha \in [0, 1]$.
                <br/>
<u>Salopp:</u> St&ouml;rungen $R$ beeinflussen die Pr&auml;ferenz von $P$
                und $Q$ nicht.
                </li>
<li id="VNM-3">Stetigkeitsaxiom: F&uuml;r beliebige
                           $P, Q, R \in \mathcal{P}$ mit
                           $P &gt; Q &gt; R$ gibt es $\alpha, \beta \in (0, 1)$
                           derart, dass
                           $$\alpha \cdot P + (1 - \alpha) \cdot R &gt; Q &gt; \beta \cdot P + (1-\beta)R$$
                           gilt.<br/>
<u>Salopp:</u> Pr&auml;ferenzrelationen sind nicht
                           anf&auml;llig gegen&uuml;ber kleinen &Auml;nderungen.</li>
</ol>
</dd>
<dt><dfn>Optimierungsproblem</dfn></dt>
<dd>Ein allgemeines optimierungsproblem besteht aus einer Optimierungsvariable
      $x \in \mathbb{R}^n$, f&uuml;r welche ein "bester" Parameter gew&auml;hlt werden
      soll. Daf&uuml;r gibt es eine Bewertungsfunktion $f$ (Zielfunktion):

        $$
        \begin{align}
        &amp;\underset{x}{\operatorname{minimize}}&amp; &amp; f(x) \\
        &amp;\operatorname{subject\;to}
        &amp; &amp;g;_i(x) \leq 0, \quad i = 1,\dots,m \\
        &amp;&amp;&amp;h;_i(x) = 0, \quad i = 1, \dots,p
        \end{align}
        $$

        Siehe auch: <a href="../optimization-basics">Optimization Basics</a>
</dd>
<dt><dfn>Notwendige Bedingung f&uuml;r optimale L&ouml;sung</dfn></dt>
<dd>$\nabla f(x) \overset{!}{=} 0$</dd>
<dt><dfn>Konvexe Optimierungsprobleme</dfn></dt>
<dd>Ein Optimierungsproblem mit konvexer Zielfunktion $f$ hat folgende
      besonderen Eigenschaften

      <ul>
<li>Jedes lokale Optimum ist ein globales Optimum</li>
<li>Ein strikt konvexes Optimierungsproblem hat ein eindeutiges
              Optimum.</li>
<li>Die notwendige Bedingung ist auch hinreichend:

              <ul>
<li>Ohne Nebenbedingungen: $\nabla f(x) \overset{!}{=} 0$</li>
<li>Mit Nebenbedingungen: $(\nabla f(x^*))^T \cdot (x - x^*) \geq 0 \quad \forall x \in \mathcal{F}$, wobei $\mathcal{F}$ eine konvexe Menge ist.</li>
</ul>
</li>
</ul>
</dd>
<dt><a href="https://en.wikipedia.org/wiki/Bellman_equation"><dfn id="bellman-equation">Bellman-Gleichungen</dfn></a></dt>
<dd>Eine Bellman-Gleichung stellt die L&ouml;sung eines Problems rekursiv dar.
      Sie zeigt, dass und wie man die L&ouml;sung eines komplexen Problems aus
      L&ouml;sungen von Teilproblemen aufbauen kann.<br/>
<br/>
      Probleme, f&uuml;r die man eine Bellman-Gleichung aufstellen kann haben
      <b>optimale Substruktur</b>.
      <br/>
      Example with the value function:
      $$V(s) = \max_{a} (R(s, a) + \gamma \sum_{s'} T(s, a, s') V(s'))$$
      where $V(s)$ is the value of the state $s$, $R(s,a)$ is the reward
      you get when you apply action $a$ in state $s$, $\gamma \in [0, 1]$ is
      the discount factor, $T(s, a, s') \in [0, 1]$ is the transormation matrix
      which gives you the probability that you will end up in state $s'$ when
      you apply action $a$ in state $s$.</dd>
<dt><dfn id="differentiation-rules">Differentiation Rules</dfn></dt>
<dd>

      $$
      \begin{align}
          \frac{\partial x^T a}{\partial x} &amp;= \frac{\partial a^T x}{\partial x} = a\\
          \frac{\partial x^T A}{\partial x} &amp;= \frac{\partial A x}{\partial x} = A \qquad A \in \mathbb{R}^{n \times n}\\
          \frac{\partial x^T A x}{\partial x} &amp;= 2 A x \qquad A \in \mathbb{R}^{n \times n}
      \end{align}
      $$

  </dd>
<dt><dfn id="q-function">Q-Funktion</dfn></dt>
<dd>Siehe <a href="../machine-learning-1-course/#q-function">ML 1</a>.</dd>
<dt><a href="https://de.wikipedia.org/wiki/Dynamische_Programmierung"><dfn id="dynamic-programming">Dynamische Programmierung</dfn></a></dt>
<dd>Dynamische Programmierung ist eine Methode zum L&ouml;sen von
      Optimierungsproblemen. Dabei wird die Tatsache genutzt, dass f&uuml;r jeden
      initialen Zustand $x_0 \in \mathcal{X}$ die optimalen Kosten $J^*(x_0)$
      in
      $$J^*(x_0) = \min_{\pi_{0:N-1}} J_{\pi_{0:N-1}} (x_0)$$
      gleich dem Wert $J_0(x_0)$, welcher sich aus dem letzten Schritt der
      Rekursion
      $$
      \begin{align}
          J_N(x_N) &amp;= g_N (x_N)\\
          J_k(x_k) &amp;= \min_{a_k \in A_k(x_k)} \{g_k (x_k, a_k) + \mathbb{E}(J_{k+1} (x_{k+1})|x_k, a_k)\}
          \text{ f&uuml;r } k = 0, \dots, N-1
      \end{align}
      $$
      ergibt.<br/>
<br/>
      Laufzeitkomplexit&auml;t: $\mathcal{O}(N |\mathcal{X}|^2 |A|)$

  </dd>
</dl>
<p>18.05.2016</p>
<dl>
<dt><dfn>Endliche Planungsprobleme</dfn></dt>
<dd>Hat man einen endlichen Zustandsraum $\mathcal{X} = \{1, 2, \dots, n_x\} \subsetneq \mathbb{N}$ und eine endliche Aktionsmenge $A = \{1, 2, \dots, n_a\} \subsetneq \mathbb{N}$,
        in einem Planungsproblem, so spricht man von einem endlichen
        Planungsproblem.</dd>
<dt><dfn>Markov-Kette</dfn></dt>
<dd>&Uuml;bergangswahrscheinlichkeiten in einem endlichen Planungsproblem
        sind gegeben.

        Die naive L&ouml;sung mit Brute-Force ist in $\mathcal{O}(|A|^{N \cdot |X|})$.

    </dd>
<dt><dfn>Planungsprobleme nach Horizont</dfn></dt>
<dd>
<ul>
<li>$N=1$: Gierige Planung, ein einschrittiges Planungsproblem.
                       Hat geringe Komplexit&auml;t, aber zuk&uuml;nftige Effekte werden
                       nicht ber&uuml;cksichtig. Bei submodularen Kostenfunktionen
                       kann man die Kosten, die durch die gierige Planung
                       entstehen, absch&auml;tzen.</li>
<li>$N&lt;\infty$: Wurde bisher betrachtet und betrifft die meisten
                       Planungsprobleme. Nachteil ist, dass die Strategie $\pi_k$
                       zeitinvariant ist.</li>
<li>$N = \infty$: Bei Planungsproblemen mit sehr langem Horizont,
                       wenn ein Ende nicht abzulesen ist. Beispiele sind die
                       k&uuml;rzeste-Wege-Suche sowie bei Reinforcement Learning.
                       Probleme sind unendliche Kosten und die Zeitabh&auml;ngigkeit
                       der Schrittkosten und &Uuml;bergangswahrscheinlichkeiten.</li>
</ul>
</dd>
<dt><dfn>Diskontiertes Planungsproblem</dfn></dt>
<dd>
<ol>
<li>&Uuml;bergangswahrscheinlichkeiten und Schrittkosten sind
                Zeitinvariant, dh. $f_{ij}^k(a) = f_{ij}(a)$ und
                $g_k(i,a) = g(i, a) \forall k$.</li>
<li>Es gilt die optimale Wertefunktion $J^*$ zu finden, welche
                durch
                $$J^*(x_0) = \min_{\pi_0, \pi_1, \dots} (J_{\pi_0}(x_0))$$

                definiert ist. Diese minimiert die erwarteten <i>diskontierten
                Kosten</i>

                $$J_{\pi_0} (x_0) = \lim_{N \rightarrow \infty} \mathbb{E}(\alpha^N g(x_N)+ \sum_{k=0}^{N-1} \alpha^k \cdot g(x_k, \pi_k(x_k)))$$

                Dabei hei&szlig;t $\alpha \in (0, 1)$ ein <i>Diskontierungsfaktor</i>.
                Er verhindert, dass die Kosten unendlich werden.
            </li>
</ol>

        Dies kann man mit DP l&ouml;sen, indem man eine Vorw&auml;rtsrekursion macht:

        $$
        \begin{align}
        J_k(1) &amp;= \min_{a \in A(i)}(g(i, a) + \alpha \sum_{j=1}^{n_x} f_{ij}(a) \cdot J_{k-1}(j))\\
        J_0(i) &amp;= g(i)
        \end{align}
        $$

        Das ist m&ouml;glich, da das Problem zeitinvariant ist. Dies kann man durch
        Indexverschiebung zeigen.
    </dd>
<dt><dfn>Bellman-Operator</dfn></dt>
<dd>$$(T J) (i) = \min_{a \in A(i)} (g(i,a) + \alpha \cdot \sum_j f_{ij}(a) \cdot J(j))$$

        $$T^k J = \begin{cases}(T(T^{k-1} J)) &amp;\text{if } k \geq 1\\
                               J              &amp;\text{otherwise}
                  \end{cases}$$

        Es gilt: $$J^* = \lim_{N \rightarrow \infty} T^N J \text{ f&uuml;r beliebiges } J$$
    </dd>
<dt><dfn>Strategiebewertung</dfn></dt>
<dd>$$(T_\pi J)(i) = g(i, \pi(i)) + \alpha \cdot \sum_j f_{ij} (\pi(i)) \cdot J(j)$$

        F&uuml;r eine optimale Strategie $\pi^*$ gilt:

        $$(T J)(i) = (T_{\pi^*} J)(i)$$
    </dd>
<dt><dfn>Wertevektor</dfn></dt>
<dd>$$J = (J(1), \dots, J(nx))^T$$</dd>
<dt><a href="https://de.wikipedia.org/wiki/Kontraktion_(Mathematik)"><dfn>Kontraktion</dfn></a></dt>
<dd>Eine Funktion $f: M \rightarrow M$ in einem metrischen Raum $(M, d)$
        hei&szlig;t Kontraktion genau dann, wenn
        $$\exists \lambda \in [0, 1) \forall x, y \in M: d(f(x), f(y)) \leq \lambda d(x, y)$$
        gilt.</dd>
<dt><a href="https://de.wikipedia.org/wiki/Fixpunktsatz_von_Banach"><dfn>Banach'scher Fixpunktsatz</dfn></a></dt>
<dd>

        Sei $(M, d)$ ein vollst&auml;ndig metrischer Raum und $f$ eine Kontraktion,
        welche Lipschitz-Stetig ist mit Konstante $0 \leq \lambda &lt; 1$.
        Dann gilt:

        <ul>
<li>Es gibt genau einen Fixpunkt $\xi \in M$ mit $f(\xi) = \xi$.</li>
<li>A-priori-Absch&auml;tzung: $d(x_n,\xi)\le\frac{\lambda^n}{1-\lambda}d(x_0,x_1)$</li>
<li>A-posteriori-Absch&auml;tzung: $d(x_n,\xi)\le\frac{\lambda}{1-\lambda}d(x_{n-1},x_n)$</li>
</ul>
</dd>
<dt><dfn>T-Kontraktion</dfn></dt>
<dd>F&uuml;r beliebige Wertevektoren $J, J'$, eine beliebige Strategie $\pi$
        und f&uuml;r alle $k=0,1, \dots$ gilt:

        $$d(T^k J, T^k J') = \leq \alpha^k \cdot d(J, J')$$
        $$d(T^k_\pi J, T_T^k J') \leq \alpha^k \cdot d (TODO)$$
    </dd>
<dt><dfn>Werte-Iteration</dfn> (<dfn id="value-iteration">Value iteration</dfn>)</dt>
<dd>$$J^* = \lim_{N \rightarrow \infty} T^N J$$
        wobei $J^*$ die optimalen Kosten, $T$ der Bellman-Operator und $N$
        der Planungshorizont ist.

        TODO: Pseudocode

        </dd>
<dt><dfn>Satz von der Sation&auml;ren Strategie</dfn></dt>
<dd>
<ol>
<li>F&uuml;r jede station&auml;re Strategie $\pi = \pi_{0:N-1}$ erf&uuml;llt der
                dazugeh&ouml;rige Wertevektor $J_\pi$ die Fixpunktgleichung
                $J_\pi = T_\pi J_\pi$.
                Dabei ist $J_\pi$ der eindeutige Fixpunkt.</li>
<li>Eine sation&auml;re Strategie $\pi^*$ ist genau dann optimal, wenn
                $\pi^*$

                $$T J^* = T_{\pi^*} J^*$$

                erf&uuml;llt. (Also: Die optimale Strategie ist eine station&auml;re Strategie)</li>
</ol>

        Der Beweis f&uuml;r (1) folgt aus dem Banach'schen Fixpunktsatz.
    </dd>
<dt><dfn>Strategie-Iteration</dfn> (<dfn id="policy-iteration">Policy iteration</dfn>)</dt>
<dd>Man kann beobachten, dass bei der Werte-Iteration die Stategie schneller
        konvergiert als der Wertevektor. Au&szlig;erdem ist die Anzahl der
        Strategien endlich, aber es gibt unendlich viele Wertevektoren.<br/>
<br/>
        (TODO: Pseudocode)
        <br/>
        Die folgenden beiden Schritte werden alternierend ausgef&uuml;hrt:

        <ol>
<li>Strategieauswertung:
                $$V^\pi(s) \gets R(s, \pi(s)) + \gamma \sum_{s'} T(s, \pi(s), s') V^\pi(s')$$
            </li>
<li>Strategieverbesserung:
                $$\pi'(s) \gets \text{arg max}_a (R(s, a) + \gamma \sum_{s'} T(s, a, s') V^\pi(s'))$$
            </li>
</ol>

        Siehe <a href="https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume4/kaelbling96a-html/node20.html">CMU</a>
</dd>
<dt><dfn>Value iteration vs Policy iteration</dfn></dt>
<dd>
<ul>
<li>Strategieiteration konvergiert in weniger Schritten</li>
<li>Jeder Schritt der Strategieiteration ist teurer als in der
                Werteoperation, da die Strategieauswertung die L&ouml;sung eines
                LGS ist (in $\mathcal{O}(n_x^3)$). Au&szlig;erdem ist
                die Strategieiteration nie f&uuml;r $\alpha=1$ l&ouml;sbar (kann auch
                sonst passieren).</li>
</ul>
</dd>
<dt><dfn>Label-Korrektur-Algorithmus</dfn></dt>
<dd>Der Label-Korrektur-Algorithmus ist ein Meta-Algorithmus zur
        k&uuml;rzeste-Wege-Suche dient. Spezialf&auml;lle von diesem sind die
        Tiefen- und Breitensuche, der <a href="https://de.wikipedia.org/wiki/Dijkstra-Algorithmus">Dijkstra-Algorithmus</a>, der <a href="https://de.wikipedia.org/wiki/A*-Algorithmus">A*-Algorithmus</a> sowie
        Branch &amp; Bound.</dd>
<dt><a href="https://de.wikipedia.org/wiki/Trellis-Code"><dfn id="trellis">Trellis-Diagramm</dfn></a></dt>
<dd>Eine Diagramm welches anzeigt welche Zust&auml;nde &uuml;ber die Zeit
        gew&auml;hlt werden.</dd>
<dt><a href="https://en.wikipedia.org/wiki/Pontryagin%27s_maximum_principle"><dfn id="pontryagins-minimum-principle">Pontryagin's Minimum-Prinzip</dfn></a></dt>
<dd>Das Pontryagin'sche Minimum-Prinzip k&ouml;nnte als die russische
        Variante der Bellman-Gleichungen f&uuml;r deterministische MDPs bezeichnet
        werden.<br/>

        TODO</dd>
<dt><a href="https://de.wikipedia.org/wiki/Hamilton-Funktion_(Kontrolltheorie)"><dfn>Hamilton-Funktion</dfn></a></dt>
<dd>Die Hamilton-Funktion der Kontrolltheorie stellt eine notwendige
        Bedingung f&uuml;r die optimale L&ouml;sung eines Steuerungsproblems ist. Damit
        eine L&ouml;sung eines Steuerungsprobelms optimal ist, muss die L&ouml;sung
        die Hamilton-Funktion minimieren.<br/>
<br/>
        Die Steuerung $u(t)$ soll so gew&auml;hlt werden, dass
        $$J(u)=\Psi(x(T))+\int^T_0 L(x,u,t) \mathrm{d}t$$
        minimiert wird. Dabei ist $x(t)$ der Systemzustand mit
        $$\dot{x}=f(x,u,t) \qquad x(0)=x_0 \quad t \in [0,T]$$
        In diesem Fall ist die Hamilton-Funktion
        $$H(x,\lambda,u,t)=\lambda^T(t)f(x,u,t)+L(x,u,t),$$
        wobei $\lambda(t)$ Lagrange-Multiplikatoren sind.</dd>
<dt><dfn>Lineares Zustandsmodell</dfn></dt>
<dd>$$x_{k+1} = A_k + x_k + B_k \cdot a_k + r_k^{(s)}$$</dd>
<dt><a href="https://de.wikipedia.org/wiki/LQ-Regler"><dfn id="linear-quadratic-regulator">Linearer Quadratischer Regulator</dfn></a> (<dfn id="lqr">LQR</dfn>)</dt>
<dd>Der LQR ist ein Regler (Regulator) f&uuml;r einen lineareren Zustandsraum
        mit quadratischer Kostenfunktion. Ein Regel will typischerweise den
        Zustand $x = \vec{0}$ erreichen, wohingegen ein Tracker den aktuellen
        Zustand bestm&ouml;glich sch&auml;tzen will.

        TODO</dd>
<dt><dfn>Sicherheits&auml;quivalenz</dfn> (<a href="https://en.wikipedia.org/wiki/Stochastic_control#Certainty_equivalence"><dfn id="certainty-equivalence">Certainty Equivalence</dfn></a>)</dt>
<dd>Verst&auml;rkungsmatrix $l_k$ und somit die Strategie $\pi_k^*$
        sind unabh&auml;ngig vom Rauschen $r_k^{(s)}$.<br/>
<br/>
        Die selbe optimale Strategie ergibt sich bei Betrachtung des
        korrespondierendne deterministischen Zustandsraummodel

        $$x_{k+1} = A_k x_k + B_k a_k$$

        welchem das Rauschen $r_k^{(s)}$ durch dessen Erwartungswert $E(r_k^{(s)}) = 0$
        ersetzt ist.<br/>

        $\Rightarrow$ Deterministisches Problem

        TODO</dd>
</dl>
<h3 id="pomdps">POMDPs</h3>
<dl>
<dt><a href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process"><dfn id="pomdp">Partially observable Markov decision process</dfn></a> (POMDP)</dt>
<dd>Die Messungen sind unsicherheitsbehaftet.

        Das Planungsproblem ist wie folgt definiert:

        <ul>
<li>Zustand: Der Agent erh&auml;lt nur noch Beobachtungen / Messungen
                des Zustands. Probleme dabei sind:
                <ul>
<li>Rauschen von Sensoren</li>
<li>Indirekt: Position ist interessant, aber man kann
                        z.B. mit GPS nur die Laufzeiten ermitteln.</li>
<li>Niederdimensional: Messgr&ouml;&szlig;e ist niedrigdimensonaler
                        als die interessierte Gr&ouml;&szlig;e. Erst durch mehrere
                        Messungen gelangt man an die interessante Gr&ouml;&szlig;e.</li>
</ul></li>
</ul>

        Ein POMDP ist ein MDP mit folgenden Unterschieden:

        <ul>
<li>Initialzustand $x_0$ ist Zufallsvariable mit Verteilung $P(x_0)$.</li>
<li>Beobachtungen / Messungen $z_k \in Z$ gem&auml;&szlig; der bedingten
                Verteilung
                $$z_k \sim P(\cdot | x_k, a_{k-1})$$
                (Beobachtungswahrscheinlichkeit)<br/>
<ul>
<li>Diskrete Beobachtungen $\rightarrow$ bedingte Z&auml;hldichte
                $$f(z_k | x_k a_{k-1}) = P(z=z_k | x_k, a_{k-1})$$</li>
<li>Kontinuierliche Beobachtungen $\rightarrow$ bedingte Wahrscheinlichkeitsdichte
                    $$f(z_k | x_k, a_{k-1}) = \frac{\partial f(z | x_k, a_{k-1})}{\partial z} |_{z=z_k}$$</li>
</ul>
</li>
<li id="pomdp-cost-function">Minimierung der erwarteten Kosten

                $$J_{\pi_{0:N-1}}(\square) = E(g_N (x_n) + \sum_{k=0}^{N-1} g_n(x_k, \pi_k(\square)))$$</li>
</ul>

        Reformulierung als MDP:

        <ul>
<li>Problem: keine vollst&auml;ndige Information &uuml;ber den Zustand $x_k$,
                aber Zugriff auf Beobachtungen</li>
<li>Idee: Definieren eines neuen Zustands (Informationsvektor $\mathcal I$,
                engl. Information state), welcher

                <ul>
<li>direkt zug&auml;nglich ist,</li>
<li>alle verf&uuml;gbaren Informationen &uuml;ber $x_k$ zum Zeitpunkt
                        $k$ enth&auml;lt</li>
</ul>

                Der Informationsvektor enth&auml;lt alle Beobachtungen:

                $$\mathcal{I} = (z_0, \dots, z_k, a_{0}, \dots, a_{k-1}) \text{ f&uuml;r } k=0, \dots, N-1$$

                Der Informationsvektor beschreibt die zeitliche Entwicklung des
                Agenten. Mit $P(x_0)$ und $\mathcal{I}_k$ ist s&auml;mtliche
                Information gegeben um zum Zeitpunkt $k$ eine
                Planungsentscheidung zu treffen.

                Das korrespondierende MDP wird <b>Informations-MDP</b> genannt.

                Das zu l&ouml;sende dynamische Programm lautet:

                $$J_N(\mathcal{I}_N) = E(g_N(x_N) | \mathcal{I}_N)$$

                $$J_k(\mathcal{I}_k) = \min_{a_k} (E_{x,z}(g_k(x_k, a_k) + J_{k+1}(\mathcal{I}_k, z_{k+1}, a_k)(\mathcal{I}_k, a_k))) \text{ f&uuml;r } k=0, 1, \dots, N-1$$

                Die L&ouml;sung ist eine &ouml;ptimale Strategie $\pi_k^* (\mathcal{I}_k) = a_k^*$

                Nur in Ausnahmef&auml;llen geschlossen l&ouml;sbar, z.B. lineare Modelle.

            </li>
</ul>
</dd>
<dt><dfn id="statistik">Statistik</dfn></dt>
<dd>Seien $S=\{z_1, \dots, z_n\}$ Stichproben (Samples) einer Zufallsvariablen
        $z \sim P(z | \Theta)$ mit unbkanntem Parameter $\Theta$. Eine
        Statistik ist eine Funktion $T(S)=t$, welche zwar von $S$, nicht aber
        von $\Theta$ abh&auml;ngt.<br/>
<br/>
        Konstante Funktionen, minimum, maximum, durschschnitt, median, ...</dd>
<dt><dfn>Hinreichende Statistik</dfn> (engl. <dfn id="sufficient-statistic">sufficient statistic</dfn>)</dt>
<dd>Ziel: Kompression, d.h. Darstellung von $\mathcal{I}_k$ von geringer
        Dimension.

        Eine Statistik $T$ hei&szlig;t hinreichend f&uuml;r $\Theta$, wenn keine weitere
        Statstik auf $S$ existiert, welche zus&auml;tzliche Informationen &uuml;ber
        $\Theta$ liefert.

        Ist $T(S) = t$ gegeben, dann liefert die volle Kentnis von $S$ keine
        Zusatzinformation &uuml;ber $\Theta$.


        Beispiele:

        <ul>
<li>Der Stichprobenmittelwert $\hat{z}$ von $n$ unabh&auml;ngigen
                Stichproben $z_i$ einer normalverteilten Zuvallsvariabeln
                $z \sim \mathcal{N}(\mu, \sigma)$ ist eine hinreichende
                Statistik f&uuml;r $\mu$.</li>
<li></li>
</ul>
</dd>
<dt><a href="https://de.wikipedia.org/wiki/Bayes-Sch%C3%A4tzer"><dfn>Bayes'scher Sch&auml;tzer</dfn></a></dt>
<dd>Pr&auml;diktion + Filterschritt = Bayes-Sch&auml;tzer.<br/>
        TODO (z.B. in GPS arbeitet eine Variante; Extended Kalman Filter)<br/>
        Der Bayes-Sch&auml;tzer ist im Allgemeinen nciht geschlossen berechenbar.
        </dd>
<dt><dfn>Verteilungs-MDP</dfn> (<dfn>Belief-state MDP</dfn>)</dt>
<dd>Belief ist die Verteilung (TODO)</dd>
<dt><dfn>Lineare Planungsprobleme in POMDPs</dfn></dt>
<dd>Zustandsraummodell (Systemmodell):
        $$x_{k+1} = A_k \cdot x_k + B_k a_k + r_k^{(s)}$$

        Messmodell (Sensormodell):
        $$z_k = H_k x_k + v_k$$

        <ul>
<li>$r_k^{(s)}, v_k$ sind normalverteilte Rauschterme:

                $$f_k^x(x_k) = N(x_k; \hat{x}_k, C_k^x) = \frac{1}{\sqrt{|2 \pi C_k^x|}} \exp(-1/2 (x_k - \hat{x}_k)^T (C_k^x)^{-1} (x_k - \hat{x}_k))$$

                mit Mittelwert $\hat{x}_k$ und Kovarianzmatrix $C_k^x$
            </li>
<li>$X = \mathbb{R}^{n_x}, A=\mathbb{R}^{n_k}, Z=\mathbb{R}^{n_z}$</li>
<li>Ziel: &Uuml;berf&uuml;hrung des Zustandes $x_0$ in Zielzustand $x_t = [0, ..., 0]^T$
                durch Minimierung der quatratischen Kostenfunktion
                $E(x_N^T Q_N x_n + \sum_{k=0}^{N-1} (x_k^T Q_k x_k + a_k^T R_k a_k) | I_N)$

                mit symmetrisch, positiv definiten Gewichtungsmatrizen
                $Q_N, Q_k, R_k$ und Informationsvektor $I_N$.


                Dies ist ein lineares, quadratisches Gau&szlig;'sches Planungsprobelm (LQG)</li>
</ul>

        Planer besteht aus 2 Komponenten:

        <ul>
<li>Zustandssch&auml;tzer</li>
<li>Strategie</li>
</ul>

        Zustandssch&auml;tzer:

        <ul>
<li>Annahme: bel. Aktionsfolge $a_{0:N-1}$ gegeben: <a href="https://martin-thoma.com/kalman-filter/">Kalman-Filter</a> (Optimaler Sch&auml;tzer f&uuml;r lineare Modelle mit normalverteilten Gr&ouml;&szlig;en)</li>
</ul>

        Pr&auml;diktion ($k \rightarrow k+1$)
        <ul>
<li>Gegeben: A posteriori Wahrscheinlichkeitsdichte $f_a^e(x_k) = N(x_k; \hat{x}_k^e, C_k^e) = P(x_k | I_k)$</li>
<li>Gesucht: pr&auml;dizierte Wahrscheinlichkeitsdichte $f_{k+1}^p(x_{k+1}) = N(x_{k+1}; \hat{x}_k^P, C_k^P) = P(x_{k+1} | I_k, a_k)$</li>
<li>Berechnung der Parameter:

                <ul>
<li>Mittelwert: $\hat{x}_{k+1}^P = A_k \hat{x}_k^e + B_k a_k$</li>
<li>Kovarianzmatrix: $C_k^P = A_k C_k^e A_k^T + C_k^w$</li>
</ul>
</li>
</ul>

        Filterschritt ($k \overset{Z_k}{\rightarrow} k$)
        <ul>
<li>Gegeben: pr&auml;dizierte Dichte $f_k^P(x_k)$, Messung $z_k$</li>
<li>Gesucht: a-posteriori Dichte $f_k^e(x_k)$</li>
<li>Berechnung der Parameter:

            <ul>
<li>Mittelwert: $\hat{x}_k^e = \hat{x}_k^P + K_k (z_k - H_k \hat{x}_k^P)$</li>
<li>Kovarianzmatrix: $C_k^e = C_k^P - K_k H_k C_k^P$</li>
<li>Kalman-Gain: $K_k = C_k^P H_k^T (H_k C_k^P H_k^T + C_k^v)^{-1}$</li>
</ul>
</li>
</ul>

        Insgesamt:
        <ul>
<li>Geschlossene Berechnung der Zustandsverteilung</li>
<li>Kalman-Filter erf&uuml;llt <span id="blue">BLUE</span>-Eigenschaft:

            <ul>
<li>Best Linear: Optimaler Sch&auml;tzer f&uuml;r lineare Modelle und
                                 normalverteilte Gr&ouml;&szlig;en; bester linearer
                                 Sch&auml;tzer  wenn lineare Modelle aber beliebige
                                 Verteilungen (z.B. Partikelfilter ist
                                 nicht-linear); d.h. minimale Varianz</li>
<li>Unbiased Estimator: Erwartungstreuer Sch&auml;tzer</li>
</ul>
</li>
</ul>
</dd>
<dt><a href="https://de.wikipedia.org/wiki/Regelkreis"><dfn>Regelkreis</dfn></a> (<dfn>Control system</dfn>)</dt>
<dd>Ein Regelkreis ist ein technisches System, welches einen Zielzustand
        anstrebt.</dd>
<dt><dfn id="ol-planung">Open-loop Planung</dfn> (<dfn>OL Planung</dfn>)</dt>
<dd>Unter einem Open-loop Control system (offener Regelkreis) versteht man
        ein technisches System welches ohne Zustandsr&uuml;ckf&uuml;hrung, also ohne
        Messung des Zustands nachdem die Regelung begonnen wurde, arbeitet.<br/>
        Beispiele sind Sp&uuml;hlmaschinen und Rasensprenger.<br/>
        In der Open-loop Planung wird ein optimaler Plan bestimmt.
    </dd>
<dt><dfn>Closed-loop Planung</dfn> (<dfn>CL Planung</dfn>)</dt>
<dd>Unter einem Closed-loop control system (geschlossenem Regelkreis)
        versteht man ein technisches System welches mit Zustandsr&uuml;ckf&uuml;hrung, also
        mit Messung des Zustands w&auml;hrend der Regelung, arbeitet.<br/>
        Beispiele sind System im Auto zum halten der Geschwindigkeit oder
        Rasensprenger welche die Feuchtigkeit &uuml;berpr&uuml;fen.<br/>
        Closed-loop Planung kann mit dynamischer Programmierung gel&ouml;st werden.
        Geschlossene L&ouml;sung nur in Ausnahmef&auml;llen, sonst numerische
        L&ouml;sungsverfahren.<br/>
        In der closed-loop Planung wird eine optimale Strategie bestimmt.
    </dd>
<dt><dfn id="olf-planung">Open-Loop-Feedback Planung</dfn> (<dfn>OLF Planung</dfn>)</dt>
<dd>TODO</dd>
<dt><dfn>Modellpr&auml;diktive Planung</dfn></dt>
<dd>OLF-Planung &uuml;ber k&uuml;rzeren, aber wandernden Horizont $M &lt;&lt; N$

        Ablauf (on-line):

        <ol>
<li>Berechnung von $P(x_k | I_k)$</li>
<li>TODO ($E(\sum_{i=k} g_i(x_i, a_i) | I_k)$)</li>
<li>Anwendung von $a_k^*$, zur&uuml;ck zu 1.</li>
</ol>

        Eigenschaften:

        <ul>
<li>Effiziente Planung f&uuml;r gro&szlig;e $N$, insbesondere f&uuml;r $N=a$</li>
<li>Verl&auml;ngerung von $N$ f&uuml;hrt nicht notwendigerweise zu besseren
                Planungsergebnissen; d.h. $N$ ist <b>kein</b> Trade-off zwischen
                Qualit&auml;t und Komplexit&auml;t.</li>
</ul>

        Hier wird der Plan aktualisiert; OLF minimiert die Kosten garantiert
        st&auml;rker als OL-Planung (gleichheit im deterministischen Fall)
    </dd>
<dt><dfn id="linearization">Linearisierung</dfn> (<dfn>Extended Kalman Filter</dfn>, <dfn id="ekf">EKF</dfn>)</dt>
<dd>Gegeben ist ein Nichtlineares Zustandsraummodell:

        $$x_{k+1} = p_k(x_k, a_k) + w_k\tag{Systemmodell}$$
        $$z_k = h_k(x_k) + v_k\tag{Messmodell}$$

        Idee: Verwendung von LQR + Kalman Filter<br/>
        Linearisierung mittels Taylorreihenentwicklung<br/>
        Annahmen: $p_k$ ist differenzierbar und nichtlinearer Anteil in Umgebung
        ist vernachl&auml;ssigbar.<br/>
        Linearisierung um Nominalwerte $\bar{x}_k, \bar{a}_k$.
        $$p_k(x_k, a_k) \approx p_k(\bar{x}_k, \bar{a}_k) + A_k (\underbrace{x_k - \bar{x}_k}_{=: \Delta x_k}) + B_k (\underbrace{a_k - \bar{a}_k}_{\Delta a_k})$$
        mit Jakobi-Matrizen:
        $$A_k = \frac{\partial}{\partial x_k} p_k(x_k, a_k)|_{x_k = \bar{x}_k, a_k = \bar{a}_k}$$
        $$B_k = \frac{\partial}{\partial a_k} p_k(x_k, a_k)|_{x_k = \bar{x}_k, a_k = \bar{a}_k}$$
        $\Rightarrow$ Lineares Modell: $\Delta x_{k+1} \approx A_k \cdot \Delta x_k + B_k \Delta a_k$
        mit $\Delta x_{k+1} = p_k(x_k, a_k) - p_k(\bar{x}_k, \bar{a}_k)$<br/>
<br/>
        Wahl der Nominalwerte $\bar{x}_k, \bar{a}_k$:
        <ul>
<li>Strategie:
            <ul>
<li>Zielzustand $\bar{x}_k = x_+ = [0, \dots, 0]^T, \bar{a}_k = [0, \dots, 0]^T \forall k$</li>
<li>Zustandssolltrajektorien bei Verfolgungsproblem</li>
<li>Pr&auml;diktiv: $\bar{x}_{k+1} = p_k(\bar{x}_k, \bar{a}_k)$ mit $\bar{x}_0 = E(x_0)$ und beliebig $\bar{a}_{0:N-1}$</li>
<li>Iterativ: Starte mit beliebigem $\bar{a}_{0:N-1}$ und $\bar{x}_0 = E(x_0)$
                <ul>
<li>Bestimme $\bar{x}_{k+1} = p_k(\bar{x}_k, \bar{a}_k) \forall k$</li>
<li>Linearisiere und l&ouml;se LQR $\Rightarrow \bar{a}_k = \pi_k(\bar{x}_k)$</li>
<li>zur&uuml;ck zu 1.</li>
</ul>
</li>
</ul>
</li>
<li>Sch&auml;tzer:
                <ul>
<li>Linearisierung um $\bar{x}_k = \hat{x}_k^l, \bar{a}_k=\pi_k(\hat{x}_k^l)$

                        $$\hat{x}^p_{k+1} = p_k(\hat{x}_k^l, \bar{a}_k)$$

                        $$C_{k+1}^P = A_k C_k^e A_k^T + C_k^w$$
                    </li>
<li>Filterschritt: Linearisierung um $\bar{x}_k = \hat{x}_k^p$

                        $$\hat{x}_k^e = \hat{x}_k^P + K_k (z_k - h_k(\hat{x}_k^P))$$
                        $$C_k^e = C_k^P - K_k H_k C_k^P$$
                    </li>
</ul>
</li>
</ul>
</dd>
<dt><dfn>Bedingte Differentielle Entropie</dfn></dt>
<dd>

        $$H(x|z, a) = - \int_z f(z|a) \cdot \int_{\mathcal{X}} f(x|z, a) \cdot \log (f(x|z, a)) \mathrm{d}x \mathrm{d} z$$

        Die differentielle Entropie erweitert die Schannon-Entropie auf den
        kontinuierlichen Fall. Unsch&ouml;n ist, dass sie negativ werden kann.<br/>
<br/>
        Sie bewertet Unsicherheit anhand der "r&auml;umlichen" Konzentration von
        Wahrscheinlichkeitsmassen.

    </dd>
<dt><dfn id="sensoreinsatzplanung">Sensoreinsatzplanung</dfn></dt>
<dd>

        TODO

        Informationstheoretische Kosten gehen in Kovarianz-basierte Kosten
        wie z.B. Entropie &uuml;ber:

        TODO

        In der Sensoreinsatzplanung liefern Open-Loop und Closed Loop
        Verfahren, gegeben TODO, die selben Kosten. Daher wird
        Open-Loop-Planung verwendent. Das hei&szlig;t, der optimale Plan
        $a_{0:N-1}^*$ wird mittels deterministischer Planung
        (K&uuml;rzeste-Wege-Suche), bestimmt.

        <ul>
<li>$g_i(x_i, a)$: Schrittkosten</li>
</ul>

        Der Suchbaum hat $|A|^N$ Pfade. (0-1 Programme)

    </dd>
<dt><dfn>Monotonie der Riccarti-Gleichung</dfn></dt>
<dd>

        Sei
        $$V_k(\Lambda, C) := C_k^w + (A_k - \Lambda \cdot H_k) C \cdot (A_k - \Lambda H_k)^T + \Lambda C_k^v V^T$$
        mit
        $$\Lambda = K_k = A_k C H_k^T (H_k C H_k^T)^{-1} \text{ und } C = C_k^P$$
        gilt $V_k = S_k$, da

        $$
        \begin{align}
            V(K, C^P) &amp;= C^W + (A - KH) C^P (A-KH)^T + KC^V K^T\\
                      &amp;= C^W + AC^P A^T - KH C^P A^T + KHC^P H^T K^T - A C^P H^T K^T KC^v K^T\\
                      &amp;= C^W + AC^P A^T - KH C^P A^T - AC^P H^T K^T + K (HC^P H^T + C^v) K^T \cdot A^CP H {(H C^P H^T + C^V)}^{-1}\\
                      &amp;= C^W + AC^P A^T - KH C^P A^T = S_k(C^P)
        \end{align}
        $$

        Weiterhin ist $\Lambda = K_k$ das Minimum von $V_k$ f&uuml;r gegebenes $C$,
        da der Kalman-Filter der optimale Sch&auml;tzer f&uuml;r lineare Modelle ist.

        Mit $\tilde{K}_k = A_k \tilde{C} H_k^T {(H_k \bar{C} H_k^T + C_k^v)}^{-1}$
        gilt
        $$S_k(C) = V_k(K_k, C) \prec V_k(\bar{K}_k, C) \prec V_k(\bar{K}_k, \bar{C}) = S_k(\tilde{C})$$
    </dd>
<dt><dfn>Approximative Planung</dfn></dt>
<dd>

        Abbildung auf lineare Sensoreinsatzplanung mittels

        <ul>
<li>Linearisierung und</li>
<li>modellpr&auml;diktiver Planung</li>
</ul>
<u>Linearisierung</u>

        Hier werden Nominalwerte $\bar{x}_{k:N-1}$ ben&ouml;tigt. Da die Aktion nur
        die Messgleichung, nicht jedoch die Systemgleichung betrifft k&ouml;nnen die
        $$\bar{x}_k = \hat{x}_k^P; \qquad \bar{x}_{k+1} = p_k(\bar{x}_k, 0)$$
        Anschlie&szlig;end wird linearisiert.

        <u>Ablauf</u>
<ol>
<li>Nach Messung: (approximative) Berechnung ovn $P(x_k | I_k)$
                bzw. $P(x_{k+1} | I_k)$ z.B. mittels EKF.</li>
<li>Berechnung der Nominalwerte $\bar{x}_{k+1:k+M}$ mit
                $\bar{x}_{k+1} = E(x_{k+1} | I_k) = \hat{x}_{k+1}^P$</li>
<li>Linearisierung</li>
<li>Berechnung des optimalen Plans $a_{k+1:k+M}^*$ f&uuml;r
                lineares Problem.</li>
<li>Anwenden von $a_{k+1}^*$; zur&uuml;ck zu 1.</li>
</ol>
<u>Beispiel</u>: Steuerung eines mobilen Sensors
        <ul>
<li>Objekt: $x_{k+1} = \begin{pmatrix}1 &amp; T &amp; 0 &amp; 0\\
                                                  0 &amp; 1 &amp; 0 &amp; 0\\
                                                  0 &amp; 0 &amp; 1 &amp; T\\
                                                  0 &amp; 0 &amp; 0 &amp; 1\end{pmatrix} \cdot x_k + w_k$
                mit $x_k = \begin{pmatrix}x_k\\ \dot{x}_k, y_k, \dot{y}_k\end{pmatrix}$</li>
<li>Sensor: $z_k = \sqrt{(x_k - x_k^S(a_k))^2 + (y_k - y_k^S(a_k))^2} + v_k$</li>
<li>Aktion $a_k$ ist Lenkwinkel</li>
<li>Kinematisches Sensormodell:

                $$\begin{pmatrix}x_{k+1}^S\\
                                 y_{k+1}^S\\
                                 \phi_{k+1}^S\end{pmatrix}
                 = \begin{pmatrix}x_{k}^S\\
                                 y_{k}^S\\
                                 \phi_{k}^S\end{pmatrix} +
                   \begin{pmatrix}T v TODO\\
                                  TODO\\
                                  TODO\end{pmatrix}$$

            </li>
</ul>
</dd>
</dl>
<p>TODO:</p>
<ul>
<li>Ausblendeigenschaft der Dirac-Delta-Funktion</li>
</ul>
<h3 id="reinforcement-learning">Reinforcement Learning</h3>
<dl>
<dt><dfn id="rl">Reinforcement Learning</dfn> (<dfn>RL</dfn>)</dt>
<dd>Reinforcement learning ist ein Subfeld des maschinellen Lernens,
        welches sich auf Probleme der optimalen Kontrolle fokusiert.<br/>
<br/>
    Problem:

    <ul>
<li>Was ist wenn die Kostenfunktion $g_k$ unbekannt ist?</li>
<li>Was ist wenn das Modell, das hei&szlig;t die &Uuml;bergangswahrscheinlichkeiten
            $P(x_{k+1} | x_k, a_k)$ unbekannt sind?</li>
<li></li>
</ul>

    Dies wird durch ein Zusammenspiel aus lernen und Planen gel&ouml;st.

    (Agent-Umelt-Bild)

    Man lernt also aus Erfahrung und <b>Interaktion mit der Umwelt</b>.<br/>
<br/>
    Eigenschaften und Besonderheiten:
    <ul>
<li>Prinzipien des biologischen Lernens (Negatives / Positives Verst&auml;rken)

            <ul>
<li>Intrinsische Motivation etwas erreichen zu wollen:
                    Abstraktion als Kosten- / Belohnungsfunktion, die es &uuml;ber
                    die Zeit zu min. / max. gilt.</li>
<li>Exploratives Lernen</li>
</ul>
</li>
<li>Unterschied zu "klassischen" Lernverfahren:
            <ul>
<li>Lernen erfolgt un&uuml;berwacht und explorativ durch
                    aktive Interaktion mit der Umwelt.</li>
<li>RL kombiniert Aspekte der Planung mit Lernmethodik.
                    Da RL un&uuml;berwacht ist erfolgt die Entscheidung aufgrund
                    eigener Erfahrung.</li>
</ul>
</li>
</ul>
<u>Definition:</u><br/>
    MDP mit folgenden Unterschieden:
    <ul>
<li>2 Zeithorizont:

            <ul>
<li>$N = \infty$ f&uuml;r fortlaufende Aufgaben</li>
<li>$N &lt; \infty$ f&uuml;r episodische Aufgaben (diese haben einen
                     terminaler Zustand)</li>
</ul>
</li>
<li>5 Keine &Uuml;bergangswahrscheinlichkeiten gegeben</li>
<li>6 Belohnungen (reward) $r_k \in \mathbb{R}$ f&uuml;r Aktion
            $a_k$ in Zustand $x_k$ mit Nachfolgezustand $x_{k+1}$.<br/>
            $$r_k = g_k(x_k, a_k, x_{k+1})$$
            wobei $g_k$ unbekannt.</li>
<li>8 Ziel: Maximierung der erwarteten Belohnung &uuml;ber die Zeit.
            $$J(x_k) = E(R_k | x_k)$$

            <ul>
<li>Fortlaufender Zeithorizont: $R_k = \sum_{t=0}^\infty \gamma^t r_{k+t}$
                    mit Diskontierungsfaktor $\gamma \in [0, 1)$</li>
<li>Episodischer Zeithorizont: $R_k = \sum_{i=0}^N r_{k+i}$,
                    wobei $N$ unbekannt ist.</li>
</ul>
</li>
</ul>

    Dynamisches Programmieren ist nicht anwendbar, da das Modell und die Kosten
    unbekannt sind. Die optimale Strategie wird aus Erfahrung approximiert.<br/>
<br/>
<u>Unterscheidungsmerkmale</u>:
    <ul>
<li>Horizont:

            <ul>
<li>fortlaufend, z.B. in Regelungstechnik das inverse Pendel</li>
<li>episodisch in Spielen</li>
</ul>
</li>
<li>Approximation / lernen:

            <ul>
<li>on-policy: Dieselbe Strategie wird zugleich verbessert
                               und angewandt.</li>
<li>off-policy: verwendet 2 Strategien
                              <ul>
<li>Strategie 1: erzeugen von Aktionen</li>
<li>Strategie 2: wird verbessert</li>
</ul>
</li>
</ul>
</li>
<li>Zustands- und Aktionsraum:

            <ul>
<li>diskret</li>
<li>kontinuierlich</li>
</ul>
</li>
<li>&Uuml;bergangswahrscheinlichkeiten / Kosten

            <ul>
<li>Modellfreie Verfahren: Lernen nur die optimale Strategie</li>
<li>Modelllernende Verfahren: Lernen von Strategie und Modell</li>
</ul>
</li>
</ul>
<u>Grundvarianten</u>:

    <ul>
<li>Wertefunktionsbasiert: Sch&auml;tzen die Wertefunktion / Q-Funktion aus
            Lernstichproben (Monte Carlo (MC); Temporal Difference (TD),
            Verantwortlichkeitsspur (eligibility trace, credit assignment);
            Verwendung von Funktionsapproximatoren)</li>
<li>Modelllernende Methoden</li>
<li>Strategiesuche (Policy search; Funktionsapproximatoren - neuronale
            Netze)</li>
</ul>
</dd>
</dl>
<h2 id="prufungsfragen_1">Pr&uuml;fungsfragen</h2>
<ul>
<li>Welche 3 Themengebiete wurden in der Vorlesung behandelt und was sind die
  Unterschiede?<br/>
  &rarr; <a href="#mdp">MDP</a>, <a href="#pomdp">POMDP</a>, <a href="#rl">RL</a> (TODO: Agent-Umfeld-Diagram)</li>
<li>Wie ist eine Nutzenfunktion definiert?<br/>
  &rarr; Siehe <a href="#nutzenfunktion">oben</a></li>
<li>Wie l&ouml;st man Optimierungsprobleme ohne Nebenbedingungen?<br/>
  &rarr; Iterativer Abstieg (z.B. Gradientenverfahren), Dynamische Programmierung (TODO)</li>
<li>Beweisen Sie, dass der Gradient senkrecht auf die H&ouml;henlinien steht.<br/>
  &rarr; TODO</li>
<li>Wie l&ouml;st man Optimierungsprobleme mit Nebenbedingungen?<br/>
  &rarr; Lagrange (TODO)</li>
<li>Wann ist es leichter / schwerer das Optimierungsproblem zu l&ouml;sen?<br/>
  &rarr; Keine Nebenbedingungen, in <span class="math">\(\mathbb{R}^n\)</span> oder kleiner diskreter Raum (TODO)</li>
<li>Welche numerischen Methoden zur Optimierung kennen sie?<br/>
  &rarr; Iterativer Abstieg (Gradientenverfahren, Newton-Verfahren), Dynamische Programmierung(?) (TODO)</li>
</ul>
<h3 id="mdp">MDP</h3>
<ul>
<li>Wie lautet die Definition eines MDP?<br/>
  &rarr; Siehe <a href="#mdp">oben</a>.</li>
<li>Wie viele Pl&auml;ne gibt es?<br/>
  &rarr; F&uuml;r diskrete <span class="math">\(\mathcal{X}, A\)</span> und <span class="math">\(N\)</span> Zeitschritte gibt es <span class="math">\(|A|^N\)</span>
     m&ouml;gliche Pl&auml;ne. In jedem Zeitschritt gibt es eine m&ouml;gliche Aktion.</li>
<li>Wie viele Strategien gibt es?<br/>
  &rarr; F&uuml;r diskrete <span class="math">\(\mathcal{X}, A\)</span> und <span class="math">\(N\)</span> Zeitschritte gibt es <span class="math">\(|A|^{N \cdot |\mathcal{X}|}\)</span>
     Strategien, da f&uuml;r jede Kombination aus Zeitschritt und Zustand eine
     Aktion gew&auml;hlt werden muss.</li>
<li>Was versteht man unter dynamischer Programmierenung?<br/>
  &rarr; Siehe <a href="#dynamic-programming">oben</a>.</li>
<li>Wie lauten die Bellman-Gleichungen?<br/>
  &rarr; Siehe <a href="#bellman-equation">oben</a>.</li>
<li>Was ist an den Bellman-Gleichungen problematisch?<br/>
  &rarr; TODO</li>
</ul>
<h3 id="pomdp">POMDP</h3>
<ul>
<li>Wie lautet die Definition eines POMDP?<br/>
  &rarr; Siehe <a href="#pomdp">oben</a></li>
<li>Wie lautet die Kostenfunktion eines POMDP?<br/>
  &rarr; Siehe <a href="#pomdp-cost-function">oben</a></li>
<li>Was ist der Unterschied des LQR beim MDP und POMDP?<br/>
  &rarr; TODO (POMDP hat Erwartungswert)</li>
<li>Was ist PWLC?<br/>
  &rarr; Piece-wise linear and Concave / Convex</li>
</ul>
<h3 id="rl">RL</h3>
<ul>
<li>Welche Arten von RL gibt es?<br/>
  &rarr; TODO</li>
</ul>
<h2 id="notation_1">Notation</h2>
<p>Der Dozent nutzt folgende Notation:</p>
<ul>
<li><span class="math">\(J^*, \pi^*\)</span>: Das Asterisk <code>*</code> deutet an, dass die Kosten / Strategie optimal
  sind.</li>
<li><span class="math">\(\underline{x}\)</span>: Der Unterstrich deutet an, dass es sich um einen Vektor
  handelt. Diese Notation wurde in diesem Artikel <strong>nicht</strong> &uuml;bernommen.</li>
<li><span class="math">\(\hat{x}\)</span>: Der Hut zeigt an, dass der Zustand <span class="math">\(x\)</span> gesch&auml;tzt ist.</li>
</ul>
<h2 id="material-und-links">Material und Links</h2>
<ul>
<li><a href="http://ies.anthropomatik.kit.edu/lehre_proplan.php">Vorlesungswebsite</a></li>
<li>Dimitri Bertsekas: Dynamic Programming and Optimal Control: Volume 1 (POMDP)</li>
<li>Emanuel Todorov: <a href="https://homes.cs.washington.edu/~todorov/papers/TodorovChapter06.pdf">Optimal Control Theory</a> (f&uuml;r Pontryagins Minimum-Prinzip)</li>
<li>Dan Simon: Optimal State Estimation (Kalman-Filter)</li>
</ul>
<h2 id="vorlesungsempfehlungen">Vorlesungs&shy;empfehlungen</h2>
<p>Folgende Vorlesungen sind &auml;hnlich:</p>
<ul>
<li><a href="https://martin-thoma.com/analysetechniken-grosser-datenbestaende/">Analysetechniken gro&szlig;er Datenbest&auml;nde</a></li>
<li><a href="https://martin-thoma.com/informationsfusion/">Informationsfusion</a></li>
<li><a href="https://martin-thoma.com/machine-learning-1-course/">Machine Learning 1</a></li>
<li><a href="https://martin-thoma.com/machine-learning-2-course/">Machine Learning 2</a></li>
<li><a href="https://martin-thoma.com/mustererkennung-klausur/">Mustererkennung</a></li>
<li><a href="https://martin-thoma.com/neuronale-netze-vorlesung/">Neuronale Netze</a></li>
<li><a href="https://martin-thoma.com/lma/">Lokalisierung Mobiler Agenten</a></li>
<li><a href="https://martin-thoma.com/probabilistische-planung/">Probabilistische Planung</a></li>
</ul>
<h2 id="termine-und-klausurablauf">Termine und Klausurablauf</h2>
<p>Die Veranstaltung wird m&uuml;ndlich gepr&uuml;ft.</p>
            
            <div id="disqus_thread"></div>
<script>
    /*
    var disqus_config = function () {
        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW
        var d = document, s = d.createElement('script');

        s.src = '//martinthoma.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

            <hr/>
        </div>
        <section>
        <div class="span2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time pubdate="pubdate" datetime="2016-05-11T20:00:00+02:00">Mai 11, 2016</time>
            <br/>
            by <a rel="author" class="vcard author post-author" itemprop="author" href="../author/martin-thoma/"><span class="fn" itemscope="" itemtype="https://schema.org/Person"><span itemprop="name">Martin Thoma</span></span></a>
            <h4>Category</h4>
            <a class="category-link" href="../categories.html#german-posts-ref">German posts</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="../tags.html#klausur-ref">Klausur
                    <span>33</span>
</a></li>
            </ul>
<h4>Contact</h4>
    <a href="https://twitter.com/themoosemind" title="My Twitter Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-twitter sidebar-social-links"></i></a>
    <a href="mailto:info@martin-thoma.de" title="My Email Address" class="sidebar-social-links" target="_blank">
    <i class="fa fa-envelope sidebar-social-links"></i></a>
    <a href="https://github.com/MartinThoma" title="My Github Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-github sidebar-social-links"></i></a>
    <a href="http://stackoverflow.com/users/562769/martin-thoma" title="My Stackoverflow Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-stackoverflow sidebar-social-links"></i></a>
        </div>
        </section>
</div>
</article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
        <div id="push"></div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-subtitle"><span class="site-name">Martin Thoma</span> - A blog about Code, the Web and Cyberculture</li>
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>        <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>
        <script>
(function(){
    'use strict';

    /*
    Create intra-page links
    Requires that your headings already have an `id` attribute set (because that's what jekyll does)
    For every heading in your page, this adds a little anchor link `#` that you can click to get a permalink to the heading.
    Ignores `h1`, because you should only have one per page.
    The text content of the tag is used to generate the link, so it will fail "gracefully-ish" if you have duplicate heading text.

    Credit: https://gist.github.com/SimplGy/a229d25cdb19d7f21231
     */

    var headingNodes = [], results, link,
        tags = ['h2', 'h3', 'h4', 'h5', 'h6'];

    tags.forEach(function(tag){
        var contentTag = document.getElementById('contentAfterTitle');
      results = contentTag.getElementsByTagName(tag);
      Array.prototype.push.apply(headingNodes, results);
    });

    headingNodes.forEach(function(node){
      link = document.createElement('a');
      link.className = 'deepLink';
      link.textContent = ' ¶';
      link.href = '#' + node.getAttribute('id');
      node.appendChild(link);
    });

  })();
</script>
    </body>
    <!-- Theme: Elegant built for Pelican
    License : http://oncrashreboot.com/pelican-elegant -->
</html>