<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Martin Thoma" />
        <meta name="copyright" content="Martin Thoma" />
        <link title = "Martin Thoma"
              type  = "application/opensearchdescription+xml"
              rel   = "search"
              href  = "../opensearch.xml">

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="Machine Learning, NLP, Machine Learning, " />

<meta property="og:title" content="Natural Language Processing "/>
<meta property="og:url" content="../nlp/" />
<meta property="og:description" content="Natural language processing (NLP) is a scientific field which deals with language in textual form. Tasks Classification: Is an e-mail spam or not? Topic: Is it about sports, science or religion? Language: Is it English, German or French? Sentence boundary: Is a character the boundary of a sentence or not …" />
<meta property="og:site_name" content="Martin Thoma" />
<meta property="og:article:author" content="Martin Thoma" />
<meta property="og:article:published_time" content="2017-05-24T20:00:00+02:00" />
<meta name="twitter:title" content="Natural Language Processing ">
<meta name="twitter:description" content="Natural language processing (NLP) is a scientific field which deals with language in textual form. Tasks Classification: Is an e-mail spam or not? Topic: Is it about sports, science or religion? Language: Is it English, German or French? Sentence boundary: Is a character the boundary of a sentence or not …">
<meta property="og:image" content="logos/ml.png" />
<meta name="twitter:image" content="logos/ml.png" >

        <title>Natural Language Processing  · Martin Thoma
</title>
        <!-- Latest compiled and minified CSS -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="../theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../static/custom.css" media="screen">

        <!-- MathJax -->
<script type="text/x-mathjax-config">
<!--
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [ ['$$', '$$'], ['\\[','\\]']],
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    processEscapes: true
  }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
// -->
</script>
<script type="text/javascript" async
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

        <link href="https://martin-thoma.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Martin Thoma - Full Atom Feed" />
        <link href="https://martin-thoma.com/feeds/index.xml" type="application/rss+xml" rel="alternate" title="Martin Thoma - Full RSS Feed" />
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top navbar-default">
            <div class="container">
                <div class="container-fluid">
                    <div class="collapse navbar-collapse">
                        <ul class="nav pull-left top-menu navbar-nav">
                            <li><a href=".." style="font-family: 'Monaco', 'Inconsolata', 'Andale Mono', 'Lucida Console', 'Bitstream Vera Sans Mono', 'Courier New', Courier, Monospace;
                        font-size: 20px;" class="navbar-brand">Martin Thoma</a>
                            </li>
                        </ul>
                        <ul class="nav pull-right top-menu navbar-nav">
                            <li ><a href="..">Home</a></li>
                            <li ><a href="../categories.html">Categories</a></li>
                            <li ><a href="../tags.html">Tags</a></li>
                            <li ><a href="../archives.html">Archives</a></li>
                            <li><a href="../support-me/">Support me</a></li>
                            <li><form class="navbar-form" action="../search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="search" class="search-query form-control" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row">
                <div class="col-sm-1 col-md-1"></div>
                <div class="col-sm-10 col-md-10">
<article>
<div class="row">
    <header class="page-header col-sm-10 col-md-10 col-md-offset-2">
    <h1><a href="../nlp/"> Natural Language Processing  </a></h1>
    </header>
</div>

<div class="row">
    <div class="col-sm-2 col-md-2 table-of-content">
        <nav>
        <h4>Contents</h4>
        <div id="toc"><ul><li><a class="toc-href" href="#tasks" title="Tasks">Tasks</a><ul><li><a class="toc-href" href="#sentiment-analysis" title="Sentiment analysis">Sentiment analysis</a></li><li><a class="toc-href" href="#text-generation" title="Text generation">Text generation</a></li><li><a class="toc-href" href="#named-entity-recognition" title="Named Entity Recognition">Named Entity Recognition</a></li><li><a class="toc-href" href="#relation-extraction" title="Relation Extraction">Relation Extraction</a></li></ul></li><li><a class="toc-href" href="#data-sources-and-corpora_1" title="Data sources and Corpora">Data sources and Corpora</a></li><li><a class="toc-href" href="#libraries" title="Libraries">Libraries</a></li><li><a class="toc-href" href="#products" title="Products">Products</a></li><li><a class="toc-href" href="#terminology-methods" title="Terminology / Methods">Terminology / Methods</a><ul><li><a class="toc-href" href="#smoothing" title="Smoothing">Smoothing</a></li></ul></li><li><a class="toc-href" href="#data-structures_1" title="Data structures">Data structures</a></li><li><a class="toc-href" href="#resources" title="Resources">Resources</a></li></ul></div>
        </nav>
    </div>
    <div class="col-sm-8 col-md-8 article-content" id="contentAfterTitle">

            
            <p>Natural language processing (NLP) is a scientific field which deals with
language in textual form.</p>
<h2 id="tasks">Tasks</h2>
<ul>
<li><strong>Classification</strong>:<ul>
<li>Is an e-mail spam or not?</li>
<li>Topic: Is it about sports, science or religion?</li>
<li>Language: Is it English, German or French?</li>
<li>Sentence boundary: Is a character the boundary of a sentence or not?</li>
<li>Author:<ul>
<li>Identify the author from a given set of authors</li>
<li>Age of the author</li>
<li>Gender of the author</li>
</ul>
</li>
<li>Sentiment analysis (Opinion mining, opinion extraction, sentiment mining, subjectivity analysis)</li>
</ul>
</li>
<li><strong>Machine Translation</strong> (MT): Given a text in language A, return the same content in language B.</li>
<li><strong>Similarity calculation</strong>: Given a corpus of n texts and one text A as
  input, find passages of the corpus which are similar to passages of A. This
  can be used to detect if students copied content / copyright violation.<ul>
<li>Minimum Edit Distance</li>
</ul>
</li>
<li><strong>Spelling correction</strong>: Find places where the grammar / writing needs to be
  fixed.</li>
<li><strong>Word sense disambiguation</strong>: If "mouse" is in a sentence, is it about the
  computer mouse or the animal.</li>
<li><strong>POS Tagging</strong>: Detect adjectives, verbs, nouns in a sentence.</li>
<li><strong>Summarization</strong> / <strong>Paraphrasing</strong></li>
<li><strong>Information extraction</strong>: For example, find the date in a calender
  application when the user enters the name of the event. Or dates in an e-mail
  in order to allow users to create a calender date.<ul>
<li><a href="https://en.wikipedia.org/wiki/Named-entity_recognition"><strong>Named Entity Recognition</strong></a> (NER):<ul>
<li>Find names in a text</li>
<li>Classify names into names of places, people, organizations and non-names.</li>
</ul>
</li>
<li><strong>Relation Extraction</strong></li>
</ul>
</li>
<li><strong>Compound splitting</strong>: For German, "Donaudampfschiffskapit&auml;n" can be split
  into the compounds "Donau" (a river) "dampfschiff" (steam boat) and "kapit&auml;n"
  (captain).</li>
</ul>
<h3 id="sentiment-analysis">Sentiment analysis</h3>
<p>Find out how users feel about something.</p>
<ul>
<li><a href="http://www.cs.cornell.edu/home/llee/omsa/omsa.pdf">Opinion mining and sentiment analysis</a></li>
</ul>
<p>Sentiment Lexicons are compared by Christopher Potts ("Sentiment Tutorial", 2011):</p>
<ul>
<li><a href="http://www.wjh.harvard.edu/~inquirer">General Inquirer</a> is free for research use<ul>
<li><a href="http://www.wjh.harvard.edu/~inquirer/homecat.htm">List of categories</a><ul>
<li>Positive (1915 words) and negative (2291 words)</li>
<li>strong vs weak, active vs passive, overstated vs understated</li>
<li>pleasure, pain, virtue, vice, motivation, cognitive orientation, ...</li>
</ul>
</li>
<li><a href="http://www.wjh.harvard.edu/~inquirer/inquirerbasic.xls">Spreadsheet</a></li>
</ul>
</li>
<li><a href="http://www.liwc.net">LIWC</a> - Linguistic Inquiry and Word count: 30 US-Dollar or 90 US-Dollar fee<ul>
<li>2300 words, more than 70 classes</li>
<li>affective process (negative and positive emotion)</li>
<li>cognitive processes: Tentative (maybe, perhaps, guess), Inhibition (block, constraint)</li>
<li>Pronouns, Negation (no, never), Quantifiers (few, many)</li>
</ul>
</li>
<li><a href="http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar">Bing Liu Opinion Lexicon</a><ul>
<li>6786 words (2006 positive, 4783 negative)</li>
</ul>
</li>
<li><a href="http://sentiwordnet.isti.cnr.it">SentiWordNet</a></li>
</ul>
<p>Positivity of a word can be infered from reviews. Reviews with many stars
should have positive words, reviews with only one or two stars should have
negative words.</p>
<h3 id="text-generation">Text generation</h3>
<p>Generate text in a given style / tone.</p>
<ul>
<li>Andrej Karpathy: <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a></li>
</ul>
<h3 id="named-entity-recognition">Named Entity Recognition</h3>
<p>Named entities are sequences of word tokens. Each word token can either be other (O),
the beginning of a namend entity (B) or the continuation of a named entity (I):</p>
<div class="highlight"><pre><span></span>           IO-encoding   IOB-encoding
Adam       PER           B-PER
gives      O             O
Berta      PER           B-PER
Charlies   PER           B-PER
Smith      PER           I-PER
's         O             O
table      O             O
--------------------------------------
#NER       2             3
</pre></div>
<p>IOB encoding</p>
<h3 id="relation-extraction">Relation Extraction</h3>
<ul>
<li>ACE (Automated Content Extraction): 17 relations from 2007 "Relation
  Extraction Task"</li>
<li>UMLS (Unified Medical Language System): 134 entity types, 54 relations</li>
</ul>
<p>One approach is taking seed relations to find language patterns. For example, a
seed relation could be <code>BORN-IN(Albert Einstein, Ulm)</code>. Now find all sentences
in a corpus which contain "Albert Einstain" and "Ulm". You might find find
patterns like:</p>
<ul>
<li>Albert Einstein, born in Ulm, ...</li>
<li>Albert Einstein (1879, Ulm) ...</li>
<li>One son of Ulm is Albert Einstein.</li>
</ul>
<p>Now you can extract language patterns:</p>
<ul>
<li>X, born in Y, ...</li>
<li>X (?, Y), ...</li>
<li>One son of Y is X.</li>
</ul>
<p>Unsupervised Information Extraction (or Open Information Extraction) does not
start with given relations or training data. The textrunner algorithm is one
way to do it.</p>
<h2 id="data-sources-and-corpora_1">Data sources and Corpora</h2>
<p>Thesaurus: WordNet</p>
<ul>
<li>Twitter</li>
<li>Wikipedia</li>
<li>News websites</li>
<li>Amazon Reviews</li>
<li>AP Newswire</li>
<li>IMDB: Polarity data 2.0 (sentiment analysis)</li>
<li>Reuters newswire dataset</li>
<li>DBPedia: 1 billion RDF triples</li>
<li>Freebase: many relations</li>
</ul>
<table class="table">
<tr>
<th>Name</th>
<th>Tokens</th>
<th>Types</th>
</tr>
<tr>
<td>Switchboard phone conversations</td>
<td class="text-right">2&thinsp;400&thinsp;000</td>
<td class="text-right">20&thinsp;000</td>
</tr>
<tr>
<td>Shakespeare</td>
<td class="text-right">884&thinsp;000</td>
<td class="text-right">31&thinsp;000</td>
</tr>
<tr>
<td>Google N-Grams</td>
<td class="text-right">1&thinsp;000&thinsp;000&thinsp;000&thinsp;000</td>
<td class="text-right">13&thinsp;000&thinsp;000</td>
</tr>
<tr>
<td><a href="https://research.fb.com/downloads/babi/">bAbI</a></td>
<td>?</td>
<td>?</td>
</tr>
</table>
<h2 id="libraries">Libraries</h2>
<ul>
<li><a href="http://www.nltk.org/"><strong>NLTK</strong></a>: The natural language toolkit. Written in Python, for Python. (<a href="http://www.nltk.org/book/ch01.html">Book</a>)</li>
<li><a href="https://spacy.io/">SpaCy</a>: According to <a href="https://www.reddit.com/r/LanguageTechnology/comments/69xbkc/question_spacy_or_nltk/">reddit</a>, it is cleaner than NLTK but less complete.</li>
<li><a href="https://textblob.readthedocs.io/en/dev/">TextBlob</a>: A simple to start
  toolkit for Python.</li>
<li>CoreNLP: Faster than NLTK (source?), written in Java, Python wrappers available</li>
<li><a href="https://radimrehurek.com/gensim/">gensim</a>: topic modeling and document similarity analysis</li>
<li><a href="https://github.com/facebookresearch/fastText">fasttext</a>: a classifier on top of a sentence2vec model</li>
<li>DeepText: an NLP engine</li>
<li>Tensorflow<ul>
<li><a href="https://github.com/tensorflow/models/tree/master/syntaxnet">syntaxnet</a> and Parsey McParseface (<a href="https://arxiv.org/pdf/1603.06042v1.pdf">paper</a>)</li>
</ul>
</li>
</ul>
<h2 id="products">Products</h2>
<ul>
<li><a href="https://github.com/GNUAspell/aspell">aspell</a></li>
<li><a href="https://books.google.com/ngrams">Google n-gram Viewer</a></li>
<li><a href="https://translate.google.com/">Google Translate</a></li>
<li>Quora: <a href="https://www.kaggle.com/c/quora-question-pairs">Duplication detection</a></li>
<li><a href="https://en.wikipedia.org/wiki/Watson_(computer)">IBM Watson</a></li>
</ul>
<h2 id="terminology-methods">Terminology / Methods</h2>
<ul>
<li>Backoff: Use trigram if possible. If not, backoff to bigram (or unigram). Alternatively, use interpolation of trigram, bigram and unigram</li>
<li>Filled pauses: "uh" in English or "&auml;hm" in German</li>
<li>Fragment: A part of a word (e.g. if you transcribe spoken text and somebody stutters)</li>
<li>Lemma: Two words belong to the same lemma if they have the same stem, belong to the same POS and have the same meaning.</li>
<li>Lexer: One type of tokenizer</li>
<li>Maxent classifiers</li>
<li>n-gram model: Model language by counting word-tuples of length n.</li>
<li>Naive Bayes</li>
<li>OOV: Out of vocabulary, <code>&lt;UNK&gt;</code> token</li>
<li>Porters Algorithms</li>
<li>Regular expressions (see <a href="http://regexpal.com/">regexpal.com</a> to test)</li>
<li>sentence2vec: Similar to word2vec.</li>
<li>Statistical parsing</li>
<li>Stemming: Bring a word in a normed form (the stem). Mostly for verbs.</li>
<li>Tokenization: Segment the text into tokens.</li>
<li>Tokenizer: Splits a text into tokens.</li>
<li>Viterbi Algorithm</li>
<li>word2vec: Embedd any word in a (high-dimensional) vector space. Allows vector arithmetic.</li>
</ul>
<p>More might me in <a href="https://martin-thoma.com/ml-glossary/">my ML Glossary</a>.</p>
<h3 id="smoothing">Smoothing</h3>
<p>A common task in NLP is estimating the probability of a word given some other
words: <span class="math">\(P(w_i | w_{i-1}, w_{i-2})\)</span>. You can do that by counting n-grams <span class="math">\((w_{i-2}, w_{i-1}, w_{i})\)</span>:</p>
<div class="math">$$P(w_i | w_{i-1}, w_{i-2}) = \frac{N((w_{i-2}, w_{i-1}, w_i))}{N((w_{i-2}, w_{i-1}))}$$</div>
<p>But you will quite often have the case that you did not see a 3-gram. How do you deal with that?</p>
<p>Smoothing is the answer. The simplest method is Laplace Smoothing (aka Add-one smoothing).</p>
<p>How do you deal with words you've never seen? The Good-Turing smoothing method
uses things you've seen once to estimate things you've never seen:</p>
<div class="math">$$P^*_{GT}(\text{things never seen}) = \frac{N_1}{N}$$</div>
<div class="math">$$P^*_{GT}(\text{thing seen}) = (\frac{(c+1) N_{c+1}}{N_c}) / N$$</div>
<p>when <span class="math">\(c\)</span> bekomes "large" (depends on the dataset), just replace <span class="math">\(N_c\)</span> by a
best-fit power law.</p>
<p>Other smoothing methods</p>
<ul>
<li>Interpolated Kneser-Ney</li>
<li>Good-Turing Smoothing</li>
<li>Stupid backoff: For very large N-grams</li>
</ul>
<p>Another important concept is the continuation probability. While some words (like "a", "to", "the", ...)
can be followed / preceeded by many different words, others (like "San", "Angelo", "D.C.", "United States of", ...).
The continuation probability quantifies how likely it is that a word is continued by something novel. Putting this together with absolute discounting gives the Kneser-Ney Smoothing algorithm:</p>
<div class="math">$$P_{KN}(w_i | w_{i-1}) = \frac{\max( c(w_{i-1}, w_i) - d, 0)}{c(w_{i-1})} + \lambda (w_{i-1}) P_{continuation}(w_i)$$</div>
<p>where <span class="math">\(\lambda \in \mathbb{R}\)</span> weights how important the continuation probability is,</p>
<h2 id="data-structures_1">Data structures</h2>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom filter</a>: a space-efficient probabilistic data structure that is used to test whether an element is a member of a set</li>
<li><a href="https://en.wikipedia.org/wiki/Trie">Trie</a>: A prefix-tree</li>
</ul>
<h2 id="resources">Resources</h2>
<ul>
<li>KIT: The ASR course has some NLP content</li>
<li>Reddit: <a href="https://www.reddit.com/r/LanguageTechnology/">/r/LanguageTechnology</a></li>
<li>StackExchange: <a href="https://datascience.stackexchange.com/questions/tagged/nlp">datascience.stackexchange.com</a></li>
<li>Online Courses:<ul>
<li>Coursera: <a href="https://www.coursera.org/learn/natural-language-processing">Introduction to Natural Language Processing</a></li>
<li>Stanford: <a href="http://web.stanford.edu/class/cs224n/">Natural Language Processing with Deep Learning</a></li>
<li>Dan Jurafsky and Chris Manning on YouTube: <a href="https://www.youtube.com/watch?v=nfoudtpBV68&amp;list=PL6397E4B26D00A269">Stanford NLP</a></li>
<li>Oxford: <a href="https://github.com/oxford-cs-deepnlp-2017/lectures">Lecture Notes</a></li>
<li><a href="http://mt-class.org/">Machine Translation</a></li>
</ul>
</li>
<li><a href="http://nlpforhackers.io/">NLP for Hackers</a></li>
</ul>
            
            <div id="disqus_thread"></div>
<script>
    /*
    var disqus_config = function () {
        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW
        var d = document, s = d.createElement('script');

        s.src = '//martinthoma.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

            <hr/>
        </div>
        <section>
        <div class="col-sm-2 col-md-2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time pubdate="pubdate" datetime="2017-05-24T20:00:00+02:00">Mai 24, 2017</time>
            <br/>
            by <a rel="author" class="vcard author post-author" itemprop="author" href="../author/martin-thoma/"><span class="fn" itemscope="" itemtype="https://schema.org/Person"><span itemprop="name">Martin Thoma</span></span></a>
            <h4>Category</h4>
            <a class="category-link" href="../categories.html#machine-learning-ref">Machine Learning</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="../tags.html#machine-learning-ref">Machine Learning
                    <span>58</span>
</a></li>
                <li><a href="../tags.html#nlp-ref">NLP
                    <span>3</span>
</a></li>
            </ul>
<h4>Contact</h4>
    <a href="https://twitter.com/themoosemind" title="My Twitter Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-twitter sidebar-social-links"></i></a>
    <a href="mailto:info@martin-thoma.de" title="My Email Address" class="sidebar-social-links" target="_blank">
    <i class="fa fa-envelope sidebar-social-links"></i></a>
    <a href="https://github.com/MartinThoma" title="My Github Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-github sidebar-social-links"></i></a>
    <a href="http://stackoverflow.com/users/562769/martin-thoma" title="My Stackoverflow Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-stackoverflow sidebar-social-links"></i></a>
        </div>
        </section>
</div>
</article>
                </div>
                <div class="col-sm-1 col-md-1"></div>
            </div>
        </div>
        <div id="push"></div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-subtitle"><span class="site-name">Martin Thoma</span> - A blog about Code, the Web and Cyberculture</li>
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>        <script src="//code.jquery.com/jquery.min.js"></script>
        <!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>
        <script>
(function(){
    'use strict';

    /*
    Create intra-page links
    Requires that your headings already have an `id` attribute set (because that's what jekyll does)
    For every heading in your page, this adds a little anchor link `#` that you can click to get a permalink to the heading.
    Ignores `h1`, because you should only have one per page.
    The text content of the tag is used to generate the link, so it will fail "gracefully-ish" if you have duplicate heading text.

    Credit: https://gist.github.com/SimplGy/a229d25cdb19d7f21231
     */

    var headingNodes = [], results, link,
        tags = ['h2', 'h3', 'h4', 'h5', 'h6'];

    tags.forEach(function(tag){
        var contentTag = document.getElementById('contentAfterTitle');
      results = contentTag.getElementsByTagName(tag);
      Array.prototype.push.apply(headingNodes, results);
    });

    headingNodes.forEach(function(node){
      link = document.createElement('a');
      link.className = 'deepLink';
      link.textContent = ' ¶';
      link.href = '#' + node.getAttribute('id');
      node.appendChild(link);
    });

  })();
</script>
    </body>
    <!-- Theme: Elegant built for Pelican
    License : http://oncrashreboot.com/pelican-elegant -->
</html>