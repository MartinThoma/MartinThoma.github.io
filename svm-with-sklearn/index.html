<!doctype html>
<html lang="en">
  <!-- type: head.html -->
  <head>
    <meta charset="utf-8">
    
    

    
        <meta name="thumbnail" content="//martin-thoma.com/images/logos/ai.png" />
        <meta property="og:image" content="//martin-thoma.com/images/logos/ai.png" />
    

    <meta property="og:type" content="blog"/>

    <title>Using SVMs with sklearn</title>
    <meta property="og:title" content="Using SVMs with sklearn" />
    <meta property="og:url" content="//martin-thoma.com/svm-with-sklearn" />
    <link rel="stylesheet" href="//martin-thoma.com/css/screen.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="//martin-thoma.com/css/style.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="//martin-thoma.com/css/pygments.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="//martin-thoma.com/css/tocplus-screen.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="//martin-thoma.com/css/print.css" type="text/css" media="print" />
    <link rel="stylesheet" href="//martin-thoma.com/css/handheld.css" type="text/css" media="only screen and (max-device-width: 480px)" />

    <link rel="alternate" type="application/rss+xml" title="Martin Thoma RSS Feed" href="//martin-thoma.com/feed/" /><!--TODO-->
    <link rel="shortcut icon" href="//martin-thoma.com/favicon.ico" type="image/x-icon" />

    <link rel="canonical" href="//martin-thoma.com/svm-with-sklearn" />
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:site" content="@themoosemind"/>
<meta name="twitter:creator" content="@themoosemind"/>
<meta name="twitter:title" content="Using SVMs with sklearn"/>

    <meta name="twitter:description" content="A blog about Code, the Web and Cyberculture" />


    <meta name="twitter:image" content="//martin-thoma.com/images/logos/ai.png"/>



<meta name="twitter:url" content="//martin-thoma.com/svm-with-sklearn"/>
<meta name="twitter:domain" content="Martin Thoma.com"/>


    <script type='text/javascript' src="//martin-thoma.com/js/jquery.js"></script>
    <script type='text/javascript' src="//martin-thoma.com/js/jquery-migrate.min.js"></script>
    <style type="text/css">div#toc_container {width: 275px;}</style>
    <style type="text/css" id="syntaxhighlighteranchor"></style>

<!-- MathJax -->
<script type="text/x-mathjax-config">
<!--
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [ ['$$', '$$'], ['\\[','\\]']],
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    processEscapes: true
  }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
// -->
</script>
<script type="text/javascript" async
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<!-- Latest compiled and minified CSS bootstrap -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">
</head>

<!-- type: post.html -->
<body>
    <div id="wrapper">
        <div id="container" class="container">
            <div class="span-16">
                <!-- type: header.html -->
<div id="header" role="banner">
    <h1><a href="//martin-thoma.com">Martin Thoma</a></h1>
    <h2 style="margin-top: 0;">A blog about Code, the Web and Cyberculture.</h2>
</div>
<nav class="navcontainer" role="navigation">
    <ul id="nav">
        <li class=""><a href="//martin-thoma.com">Home</a></li>
        <li class="page_item page-item-41 "><a href="//martin-thoma.com/author/martin-thoma/">About Me</a></li>
        <li class="page_item page-item-91 "><a href="//martin-thoma.com/imprint/">Imprint</a></li>
    </ul>
</nav>

                <div id="content">
                    <article class="post type-post format-standard hentry clearfix" itemscope itemType="http://schema.org/BlogPosting">
                        <h2 class="title entry-title" itemprop="name headline">Using SVMs with sklearn</h2>
                        
                            <meta name="image" itemprop="image" content="https://martin-thoma.com/images/logos/ai.png" />
                        
                        <link itemprop="mainEntityOfPage" href="//martin-thoma.com/svm-with-sklearn" />
                        <div class="postdate entry-date date-header">
                            <time datetime="2016-01-14T12:25:00+01:00" itemprop="datePublished">
                                January
                                14th,
                                  
                                2016
                            </time>
                        </div>

                        <div class="entry post-body" id="contentAfterTitle" itemprop="articleBody">
                            <p>Support Vector Machines (SVMs) is a group of powerful classifiers. In this
article, I will give a short impression of how they work. I continue
with an example how to use SVMs with sklearn.</p>

<div id="toc_container" class="toc_light_blue no_bullets">
   <p class="toc_title">Contents</p>
   <ul class="toc_list">
      <li class="toc_level-1 toc_section-1">
         <a href="#tocAnchor-1-1"><span class="tocnumber">1</span> <span class="toctext">SVM theory</span></a>
      </li>
      <li class="toc_level-1 toc_section-2">
         <a href="#tocAnchor-1-2"><span class="tocnumber">2</span> <span class="toctext">sklearn</span></a>
      </li>
      <li class="toc_level-1 toc_section-3">
         <a href="#tocAnchor-1-3"><span class="tocnumber">3</span> <span class="toctext">Results</span></a>
      </li>
      <li class="toc_level-1 toc_section-4">
         <a href="#tocAnchor-1-4"><span class="tocnumber">4</span> <span class="toctext">References</span></a>
      </li>
      <li class="toc_level-1 toc_section-5">
         <a href="#tocAnchor-1-5"><span class="tocnumber">5</span> <span class="toctext">See also</span></a>
      </li>
   </ul>
</div><h2 id="tocAnchor-1-1">SVM theory</h2>

<p><abbr title="Support Vector Machines">SVMs</abbr> can be described with
5 ideas in mind:</p>

<ol>
    <li><b>Linear, binary classifiers</b>: If data is linearly separable, it
        can be separated by a hyperplane. There is one hyperplane which
        maximizes the distance to the next datapoints (support vectors). This
        hyperplane should be taken:<br />
        <div>
          \[
          \begin{aligned}
              \text{minimize}_{\mathbf{w}, b}\,&amp;\frac{1}{2} \|\mathbf{w}\|^2\\
              \text{s.t. }&amp; \forall_{i=1}^m y_i \cdot \underbrace{(\langle \mathbf{w}, \mathbf{x}_i\rangle + b)}_{\text{Classification}} \geq 1
          \end{aligned}\]</div></li>
    <li><b>Slack variables</b>: Even if the underlying process which generates
          the features for the two classes is linearly separable, noise can
          make the data not separable. The introduction of <i>slack variables</i>
          to relax the requirement of linear separability solves
          this problem. The trade-off between accepting some errors and a more
          complex model is weighted by a parameter \(C \in \mathbb{R}_0^+\). The
          bigger \(C\), the more errors are accepted. The new optimization
          problem is:
          \[
          \begin{aligned}
              \text{minimize}_{\mathbf{w}, b}\,&amp;\frac{1}{2} \|\mathbf{w}\|^2 + C \cdot \sum_{i=1}^m \xi_i\\
              \text{s.t. }&amp; \forall_{i=1}^m y_i \cdot (\langle \mathbf{w}, \mathbf{x}_i\rangle + b) \geq 1 - \xi_i
          \end{aligned}\]

          Note that \(0 \le \xi_i \le 1\) means that the data point is within
          the margin, whereas \(\xi_i \ge 1\) means it is misclassified. An
          SVM with \(C &gt; 0\) is also called a <i>soft-margin SVM</i>.</li>
    <li><b>Dual Problem</b>: The primal problem is to find the normal vector \(\mathbf{w}\) and the
          bias \(b\). The dual problem is to express \(\mathbf{w}\) as a linear
          combination of the training data \(\mathbf{x}_i\):
          \[\mathbf{w} = \sum_{i=1}^m \alpha_i y_i \mathbf{x}_i\]
          where \(y_i \in \{-1, 1\}\) represents the class of the training
          example and \(\alpha_i\) are Lagrange multipliers. The usage of
          Lagrange multipliers is explained with some examples
          in [<a href="#ref-smi04" name="ref-smi04-anchor" id="ref-smi04-anchor">Smi04</a>]. The usage of the Lagrange multipliers
          \(\alpha_i\) changes the optimization problem depend on the
          \(\alpha_i\) which are weights for the feature vectors. It turns
          out that most \(\alpha_i\) will be zero. The non-zero weighted vectors
          are called <i>support vectors</i>.

          The optimization problem is now, according to [<a href="#ref-bur98" name="ref-bur98-anchor" id="ref-bur98-anchor">Bur98</a>] (a great read; if you really want to understand it I can recommend it!):
          \[
          \begin{aligned}
              \text{maximize}_{\alpha_i}\,&amp; \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j \langle \mathbf{x}_i, \mathbf{x}_j \rangle\\
              \text{s.t. } &amp; \forall_{i=1}^m 0 \leq \alpha_i \leq C\\
              \text{s.t. } &amp; \sum_{i=1}^m \alpha_i y_i = 0
          \end{aligned}\]</li>
    <li><b>Kernel-Trick</b>: Not every dataset is linearly separable. This problem is approached
          by transforming the feature vectors \(\mathbf{x}\) with a non-linear
          mapping \(\Phi\) into a higher dimensional (probably
          \(\infty\)-dimensional) space. As the feature vectors \(\mathbf{x}\)
          are only used within scalar product
          \(\langle \mathbf{x}_i, \mathbf{x}_j \rangle\), it is not necessary to
          do the transformation. It is enough to do the calculation
          \[K(\mathbf{x}_i, \mathbf{x}_j) = \langle \mathbf{x}_i, \mathbf{x}_j \rangle\]

          This function \(K\) is called a <i>kernel</i>. The idea of never
          explicitly transforming the vectors \(\mathbf{x}_i\) to the higher
          dimensional space is called the <i>kernel trick</i>. Common kernels
          include the polynomial kernel
          \[K_P(\mathbf{x}_i, \mathbf{x}_j) = (\langle \mathbf{x}_i, \mathbf{x}_j \rangle + r)^p\]
          of degree \(p\) and coefficient \(r\), the Gaussian <abbr title="Radial Basis Function">RBF</abbr> kernel
          \[K_{\text{Gauss}}(\mathbf{x}_i, \mathbf{x}_j) = e^{\frac{-\gamma\|\mathbf{x}_i - \mathbf{x}_j\|^2}{2 \sigma^2}}\]
          and the sigmoid kernel
          \[K_{\text{tanh}}(\mathbf{x}_i, \mathbf{x}_j) = \tanh(\gamma \langle \mathbf{x}_i, \mathbf{x}_j \rangle - r)\]
          where the parameter \(\gamma\) determines how much influence single
          training examples have.</li>
    <li><b>Multiple Classes</b>: By using the <i>one-vs-all</i> or the
        <i>one-vs-one</i> strategy it is possible to get a classifying system
        which can distinguish many classes.</li>
</ol>

<p>A nice visualization of the transformation of the data in a higher-dimensional
space was done by</p>

<p>TeamGrizzly’s channel: <a href="https://youtu.be/9NrALgHFwTo">Performing nonlinear classification via linear separation in higher dimensional space</a> on YouTube. 22.11.2010.</p>

<p>See also:</p>

<ul>
  <li><a href="http://math.stackexchange.com/a/1620256/6876">What is an example of a SVM kernel, where one implicitly uses an infinity-dimensional space?</a></li>
  <li><a href="http://stackoverflow.com/a/4630731/562769">SVM - hard or soft margins?</a></li>
</ul>

<h2 id="tocAnchor-1-2">sklearn</h2>

<p><code>sklearn</code> is the machine learning toolkit to get started for Python. It has
a very good documentation and many functions. You can find <a href="http://scikit-learn.org/stable/install.html">installation
instructions</a> on their website.</p>

<p>It also includes <a href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC"><code>sklearn.svm.SVC</code></a>.
SVC is short for <em>support vector classifier</em> and this is how you use it for
the MNIST dataset.</p>

<p>Parameters for which you might want a further explanation:</p>

<ul>
  <li><code>cache_size</code>: <a href="http://datascience.stackexchange.com/a/996/8820">datascience.stackexchange.com</a></li>
</ul>

<div class="highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span style="color:#777">#!/usr/bin/env python</span>

<span style="color:#D42"><span style="color:black">"""</span><span>
</span><span>Train a SVM to categorize 28x28 pixel images into digits (MNIST dataset).</span><span>
</span><span style="color:black">"""</span></span>

<span style="color:#080;font-weight:bold">import</span> <span style="color:#B44;font-weight:bold">numpy</span> <span style="color:#080;font-weight:bold">as</span> np


<span style="color:#080;font-weight:bold">def</span> <span style="color:#06B;font-weight:bold">main</span>():
    <span style="color:#D42"><span style="color:black">"""</span><span>Orchestrate the retrival of data, training and testing.</span><span style="color:black">"""</span></span>
    data = get_data()

    <span style="color:#777"># Get classifier</span>
    <span style="color:#080;font-weight:bold">from</span> <span style="color:#B44;font-weight:bold">sklearn.svm</span> <span style="color:#080;font-weight:bold">import</span> <span style="color:#B44;font-weight:bold">SVC</span>
    clf = SVC(probability=<span style="color:#069">False</span>,  <span style="color:#777"># cache_size=200,</span>
              kernel=<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">"</span><span style="color:#D20">rbf</span><span style="color:#710">"</span></span>, C=<span style="color:#60E">2.8</span>, gamma=<span style="color:#60E">.0073</span>)

    print(<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">"</span><span style="color:#D20">Start fitting. This may take a while</span><span style="color:#710">"</span></span>)

    <span style="color:#777"># take all of it - make that number lower for experiments</span>
    examples = <span style="color:#369;font-weight:bold">len</span>(data[<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">train</span><span style="color:#710">'</span></span>][<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">X</span><span style="color:#710">'</span></span>])
    clf.fit(data[<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">train</span><span style="color:#710">'</span></span>][<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">X</span><span style="color:#710">'</span></span>][:examples], data[<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">train</span><span style="color:#710">'</span></span>][<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">y</span><span style="color:#710">'</span></span>][:examples])

    analyze(clf, data)


<span style="color:#080;font-weight:bold">def</span> <span style="color:#06B;font-weight:bold">analyze</span>(clf, data):
    <span style="color:#D42"><span style="color:black">"""</span><span>
</span><span>    Analyze how well a classifier performs on data.</span><span>
</span><span>
</span><span>    Parameters</span><span>
</span><span>    ----------</span><span>
</span><span>    clf : classifier object</span><span>
</span><span>    data : dict</span><span>
</span><span>    </span><span style="color:black">"""</span></span>
    <span style="color:#777"># Get confusion matrix</span>
    <span style="color:#080;font-weight:bold">from</span> <span style="color:#B44;font-weight:bold">sklearn</span> <span style="color:#080;font-weight:bold">import</span> <span style="color:#B44;font-weight:bold">metrics</span>
    predicted = clf.predict(data[<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">test</span><span style="color:#710">'</span></span>][<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">X</span><span style="color:#710">'</span></span>])
    print(<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">"</span><span style="color:#D20">Confusion matrix:</span><span style="color:#b0b">\n</span><span style="color:#D20">%s</span><span style="color:#710">"</span></span> %
          metrics.confusion_matrix(data[<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">test</span><span style="color:#710">'</span></span>][<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">y</span><span style="color:#710">'</span></span>],
                                   predicted))
    print(<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">"</span><span style="color:#D20">Accuracy: %0.4f</span><span style="color:#710">"</span></span> % metrics.accuracy_score(data[<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">test</span><span style="color:#710">'</span></span>][<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">y</span><span style="color:#710">'</span></span>],
                                                     predicted))

    <span style="color:#777"># Print example</span>
    try_id = <span style="color:#00D">1</span>
    out = clf.predict(data[<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">test</span><span style="color:#710">'</span></span>][<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">X</span><span style="color:#710">'</span></span>][try_id])  <span style="color:#777"># clf.predict_proba</span>
    print(<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">"</span><span style="color:#D20">out: %s</span><span style="color:#710">"</span></span> % out)
    size = <span style="color:#369;font-weight:bold">int</span>(<span style="color:#369;font-weight:bold">len</span>(data[<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">test</span><span style="color:#710">'</span></span>][<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">X</span><span style="color:#710">'</span></span>][try_id])**(<span style="color:#60E">0.5</span>))
    view_image(data[<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">test</span><span style="color:#710">'</span></span>][<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">X</span><span style="color:#710">'</span></span>][try_id].reshape((size, size)),
               data[<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">test</span><span style="color:#710">'</span></span>][<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">y</span><span style="color:#710">'</span></span>][try_id])


<span style="color:#080;font-weight:bold">def</span> <span style="color:#06B;font-weight:bold">view_image</span>(image, label=<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">"</span><span style="color:#710">"</span></span>):
    <span style="color:#D42"><span style="color:black">"""</span><span>
</span><span>    View a single image.</span><span>
</span><span>
</span><span>    Parameters</span><span>
</span><span>    ----------</span><span>
</span><span>    image : numpy array</span><span>
</span><span>        Make sure this is of the shape you want.</span><span>
</span><span>    label : str</span><span>
</span><span>    </span><span style="color:black">"""</span></span>
    <span style="color:#080;font-weight:bold">from</span> <span style="color:#B44;font-weight:bold">matplotlib.pyplot</span> <span style="color:#080;font-weight:bold">import</span> <span style="color:#B44;font-weight:bold">show</span>, <span style="color:#B44;font-weight:bold">imshow</span>, <span style="color:#B44;font-weight:bold">cm</span>
    print(<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">"</span><span style="color:#D20">Label: %s</span><span style="color:#710">"</span></span> % label)
    imshow(image, cmap=cm.gray)
    show()


<span style="color:#080;font-weight:bold">def</span> <span style="color:#06B;font-weight:bold">get_data</span>():
    <span style="color:#D42"><span style="color:black">"""</span><span>
</span><span>    Get data ready to learn with.</span><span>
</span><span>
</span><span>    Returns</span><span>
</span><span>    -------</span><span>
</span><span>    dict</span><span>
</span><span>    </span><span style="color:black">"""</span></span>
    simple = <span style="color:#069">False</span>
    <span style="color:#080;font-weight:bold">if</span> simple:  <span style="color:#777"># Load the simple, but similar digits dataset</span>
        <span style="color:#080;font-weight:bold">from</span> <span style="color:#B44;font-weight:bold">sklearn.datasets</span> <span style="color:#080;font-weight:bold">import</span> <span style="color:#B44;font-weight:bold">load_digits</span>
        <span style="color:#080;font-weight:bold">from</span> <span style="color:#B44;font-weight:bold">sklearn.utils</span> <span style="color:#080;font-weight:bold">import</span> <span style="color:#B44;font-weight:bold">shuffle</span>
        digits = load_digits()
        x = [np.array(el).flatten() <span style="color:#080;font-weight:bold">for</span> el <span style="color:#080;font-weight:bold">in</span> digits.images]
        y = digits.target

        <span style="color:#777"># Scale data to [-1, 1] - This is of mayor importance!!!</span>
        <span style="color:#777"># In this case, I know the range and thus I can (and should) scale</span>
        <span style="color:#777"># manually. However, this might not always be the case.</span>
        <span style="color:#777"># Then try sklearn.preprocessing.MinMaxScaler or</span>
        <span style="color:#777"># sklearn.preprocessing.StandardScaler</span>
        x = x/<span style="color:#60E">255.0</span>*<span style="color:#00D">2</span> - <span style="color:#00D">1</span>

        x, y = shuffle(x, y, random_state=<span style="color:#00D">0</span>)

        <span style="color:#080;font-weight:bold">from</span> <span style="color:#B44;font-weight:bold">sklearn.cross_validation</span> <span style="color:#080;font-weight:bold">import</span> <span style="color:#B44;font-weight:bold">train_test_split</span>
        x_train, x_test, y_train, y_test = train_test_split(x, y,
                                                            test_size=<span style="color:#60E">0.33</span>,
                                                            random_state=<span style="color:#00D">42</span>)
        data = {<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">train</span><span style="color:#710">'</span></span>: {<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">X</span><span style="color:#710">'</span></span>: x_train,
                          <span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">y</span><span style="color:#710">'</span></span>: y_train},
                <span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">test</span><span style="color:#710">'</span></span>: {<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">X</span><span style="color:#710">'</span></span>: x_test,
                         <span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">y</span><span style="color:#710">'</span></span>: y_test}}
    <span style="color:#080;font-weight:bold">else</span>:  <span style="color:#777"># Load the original dataset</span>
        <span style="color:#080;font-weight:bold">from</span> <span style="color:#B44;font-weight:bold">sklearn.datasets</span> <span style="color:#080;font-weight:bold">import</span> <span style="color:#B44;font-weight:bold">fetch_mldata</span>
        <span style="color:#080;font-weight:bold">from</span> <span style="color:#B44;font-weight:bold">sklearn.utils</span> <span style="color:#080;font-weight:bold">import</span> <span style="color:#B44;font-weight:bold">shuffle</span>
        mnist = fetch_mldata(<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">MNIST original</span><span style="color:#710">'</span></span>)

        x = mnist.data
        y = mnist.target

        <span style="color:#777"># Scale data to [-1, 1] - This is of mayor importance!!!</span>
        x = x/<span style="color:#60E">255.0</span>*<span style="color:#00D">2</span> - <span style="color:#00D">1</span>

        x, y = shuffle(x, y, random_state=<span style="color:#00D">0</span>)

        <span style="color:#080;font-weight:bold">from</span> <span style="color:#B44;font-weight:bold">sklearn.cross_validation</span> <span style="color:#080;font-weight:bold">import</span> <span style="color:#B44;font-weight:bold">train_test_split</span>
        x_train, x_test, y_train, y_test = train_test_split(x, y,
                                                            test_size=<span style="color:#60E">0.33</span>,
                                                            random_state=<span style="color:#00D">42</span>)
        data = {<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">train</span><span style="color:#710">'</span></span>: {<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">X</span><span style="color:#710">'</span></span>: x_train,
                          <span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">y</span><span style="color:#710">'</span></span>: y_train},
                <span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">test</span><span style="color:#710">'</span></span>: {<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">X</span><span style="color:#710">'</span></span>: x_test,
                         <span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">y</span><span style="color:#710">'</span></span>: y_test}}
    <span style="color:#080;font-weight:bold">return</span> data


<span style="color:#080;font-weight:bold">if</span> __name__ == <span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">__main__</span><span style="color:#710">'</span></span>:
    main()

</pre></div>
</div>
</div>

<h2 id="tocAnchor-1-3">Results</h2>

<p>The script from above gives the following results:</p>

<table>
    <caption>Confusion matrix for an SVM classifier on the MNIST dataset</caption>
    <thead>
    <tr>
        <th></th>
        <th>0</th>
        <th>1</th>
        <th>2</th>
        <th>3</th>
        <th>4</th>
        <th>5</th>
        <th>6</th>
        <th>7</th>
        <th>8</th>
        <th>9</th>
    </tr>
    </thead>
    <tbody>
    <tr>
        <th>0</th>
        <td>2258</td>
        <td>1</td>
        <td>4</td>
        <td>1</td>
        <td>2</td>
        <td>2</td>
        <td>3</td>
        <td>1</td>
        <td>4</td>
        <td>2</td>
    </tr>
    <tr>
        <th>1</th>
        <td>1</td>
        <td>2566</td>
        <td>9</td>
        <td>1</td>
        <td>1</td>
        <td>0</td>
        <td>0</td>
        <td>7</td>
        <td>3</td>
        <td>0</td>
    </tr>
    <tr>
        <th>2</th>
        <td>4</td>
        <td>1</td>
        <td>2280</td>
        <td>5</td>
        <td>4</td>
        <td>0</td>
        <td>1</td>
        <td>9</td>
        <td>8</td>
        <td>2</td>
    </tr>
    <tr>
        <th>3</th>
        <td>0</td>
        <td>0</td>
        <td>14</td>
        <td>2304</td>
        <td>1</td>
        <td>13</td>
        <td>0</td>
        <td>6</td>
        <td>8</td>
        <td>2</td>
    </tr>
    <tr>
        <th>4</th>
        <td>2</td>
        <td>2</td>
        <td>2</td>
        <td>0</td>
        <td>2183</td>
        <td>0</td>
        <td>7</td>
        <td>5</td>
        <td>0</td>
        <td>10</td>
    </tr>
    <tr>
        <th>5</th>
        <td>4</td>
        <td>0</td>
        <td>0</td>
        <td>16</td>
        <td>3</td>
        <td>2026</td>
        <td>12</td>
        <td>1</td>
        <td>4</td>
        <td>3</td>
    </tr>
    <tr>
        <th>6</th>
        <td>7</td>
        <td>5</td>
        <td>3</td>
        <td>0</td>
        <td>5</td>
        <td>2</td>
        <td>2245</td>
        <td>0</td>
        <td>4</td>
        <td>0</td>
    </tr>
    <tr>
        <th>7</th>
        <td>1</td>
        <td>6</td>
        <td>11</td>
        <td>2</td>
        <td>5</td>
        <td>1</td>
        <td>0</td>
        <td>2373</td>
        <td>5</td>
        <td>13</td>
    </tr>
    <tr>
        <th>8</th>
        <td>3</td>
        <td>9</td>
        <td>4</td>
        <td>9</td>
        <td>4</td>
        <td>10</td>
        <td>2</td>
        <td>3</td>
        <td>2166</td>
        <td>5</td>
    </tr>
    <tr>
        <th>9</th>
        <td>3</td>
        <td>2</td>
        <td>2</td>
        <td>6</td>
        <td>19</td>
        <td>6</td>
        <td>0</td>
        <td>12</td>
        <td>10</td>
        <td>2329</td>
    </tr>
    </tbody>
</table>

<ul>
  <li>Accuracy: 98.40%</li>
  <li>Error: 1.60%</li>
</ul>

<p>Looks pretty good to me. However, note that there are much better results.
The best on <a href="http://yann.lecun.com/exdb/mnist/">the official website</a> has an
error of 0.23% and is a committee of 35 convolutional neural networks.</p>

<p>The best SVM I could find has an error of 0.56% and applies a polynomial kernel
of degree 9 as well as some preprocessing.</p>

<h2 id="tocAnchor-1-4">References</h2>

<ul>
  <li>[<a href="#ref-smi04-anchor" name="ref-smi04" id="ref-smi04">Smi04</a>] B. T. Smith, “Lagrange multipliers tutorial in the context of support
vector machines,” Memorial University of Newfoundland St. John’s,
Newfoundland, Canada, Jun. 2004.</li>
  <li>[<a href="#ref-bur98-anchor" name="ref-bur98" id="ref-bur98">Bur98</a>] C. J. Burges, “<a href="http://research.microsoft.com/pubs/67119/svmtutorial.pdf">A tutorial on support vector machines for pattern recognition</a>”, Data mining and knowledge discovery, vol. 2, no. 2, pp.
121–167, 1998.</li>
</ul>

<h2 id="tocAnchor-1-5">See also</h2>

<ul>
  <li><a href="http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html">Recognizing hand-written digits</a></li>
  <li>Trung Huynh’s tech blog: <a href="http://www.trungh.com/2013/04/digit-recognition-using-svm-in-python/">Digit Recognition using SVM in Python</a></li>
  <li><a href="http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html">Classifier comparison</a></li>
  <li><a href="http://scikit-learn.org/stable/supervised_learning.html">Supervised learning</a></li>
  <li><a href="http://stats.stackexchange.com/questions/80398/how-can-svm-find-an-infinite-feature-space-where-linear-separation-is-always-p">How can SVM ‘find’ an infinite feature space where linear separation is always possible?</a></li>
</ul>

                        </div>
                        <div class="postmeta">Posted in
                            
                                <a href="//martin-thoma.com/category/machine-learning/">machine learning</a><!--TODO: Displayed category name should be upper case! -->
                             | Tags:
                            
                                
                                    <a href="//martin-thoma.com/tag/python/">Python</a>
                                
                            
                                , <a href="//martin-thoma.com/tag/machine-learning/">Machine Learning</a>
                                
                            
                                , <a href="//martin-thoma.com/tag/svm/">SVM</a>
                                
                            
                                , <a href="//martin-thoma.com/tag/classification/">Classification</a>
                                
                             by <a rel="author" class="vcard author post-author" itemprop="author" href="//martin-thoma.com/author/martin-thoma/"><span class="fn" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Martin Thoma</span></span></a> on <span class="updated"><span class="value-title" title="2016-01-14 12:25:00 +0100">
                                January
                                14th
                                  ,
                                2016</span></span></div>

                            <div class="navigation clearfix">
                                <div class="alignleft">
                                
                                    &laquo; <a href="//martin-thoma.com/explaining-away/" rel="prev">Explaining Away</a>
                                
                                </div>
                                <div class="alignright">
                                
                                    <a href="//martin-thoma.com/function-approximation/" rel="next">Function Approximation</a> &raquo;
                                
                                </div>
                            </div>

                        </article>
                        <div id="respond">
                            <h3>Leave a Reply</h3>
                                <!-- comment discuss code -->
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'martinthoma'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="//disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    <!-- comment discuss code -->

                        </div>
                    </div>
                </div>
            <div class="span-8 last">
                <div id="subscriptions">
<a href="//martin-thoma.com/feed/"><img src="//martin-thoma.com/css/images/rss.png" alt="Subscribe to RSS Feed" title="Subscribe to RSS Feed" width="72" height="47" /></a>
<a href="https://twitter.com/#!/themoosemind" title="Follow me on Twitter!"><img src="//martin-thoma.com/css/images/twitter.png" title="Follow me on Twitter!" alt="Follow me on Twitter!"  width="76" height="47" /></a>
</div>

                <div id="sidebar">
                <!-- type: searchbox.html - TODO-->
<ul>
    <li id="search">
        <div class="searchlayout">
            <form method="get" id="searchform" action="//google.com/cse" role="search">
                <input type="hidden" name="cx" value="017345337424948206369:qrnnnentkkk" />
                <input type="search" value="" name="q" id="s" placeholder="Search with Google"/>
                <input type="image" src="//martin-thoma.com/css/images/search.gif" style="border:0; vertical-align: top;" alt="search"/>
            </form>
        </div>
    </li>
</ul>

                <div class="addthis_toolbox">
    <div class="custom_images">
            <a href="//twitter.com/share?url=//martin-thoma.com/svm-with-sklearn&amp;hashtags=python,machine-learning,svm,classification,&amp;via=themoosemind" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/twitter.png" width="32" height="32" alt="Twitter" /></a>
            <a href="//del.icio.us/post?url=//martin-thoma.com/svm-with-sklearn&amp;title=Using%20SVMs%20with%20sklearn" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/delicious.png" width="32" height="32" alt="Delicious" /></a>
            <a href="//www.facebook.com/sharer.php?u=//martin-thoma.com/svm-with-sklearn" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/facebook.png" width="32" height="32" alt="Facebook" /></a>
            <a href="//digg.com/submit?phase=2&amp;url=//martin-thoma.com/svm-with-sklearn&amp;title=Using%20SVMs%20with%20sklearn" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/digg.png" width="32" height="32" alt="Digg" /></a>
            <a href="//www.stumbleupon.com/submit?url=//martin-thoma.com/svm-with-sklearn&amp;title=Using%20SVMs%20with%20sklearn" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/stumbleupon.png" width="32" height="32" alt="Stumbleupon" /></a>
            <a href="//plusone.google.com/_/+1/confirm?hl=en&amp;url=//martin-thoma.com/svm-with-sklearn" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/gplus.png" width="32" height="32" alt="Google Plus" /></a>
            <a href="//reddit.com/submit?url=//martin-thoma.com/svm-with-sklearn&amp;title=Using%20SVMs%20with%20sklearn" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/reddit.png" width="32" height="32" alt="Reddit" /></a>
    </div>
</div>

                <ul>
                    <li id="categories-3" class="widget widget_categories">
                        <!-- type: categories -->
<h2 class="widgettitle">Categories</h2>
    <ul>
        <li class="cat-item cat-item-11"><a href="//martin-thoma.com/category/code/" title="Tipps for coding in different languages like Python oder C++.">Code</a></li>
        <li class="cat-item cat-item-21"><a href="//martin-thoma.com/category/web/" title="New emerging websites and technologies.">The Web</a></li>
        <li class="cat-item cat-item-31"><a href="//martin-thoma.com/category/cyberculture/" title="Lolcats, planking, Trollfaces, ...">Cyberculture</a></li>
        <li class="cat-item cat-item-3404"><a href="//martin-thoma.com/category/maths/" title="View all posts filed under Mathematics">Mathematics</a></li>
        <li class="cat-item cat-item-881"><a href="//martin-thoma.com/category/bits-and-bytes/" title="Sometimes posts don&#039;t fit in any category.">My bits and bytes</a></li>
        <li class="cat-item cat-item-41"><a href="//martin-thoma.com/category/deutschland/" title="[All Posts here are written in German about German topics] - Die Bahn, unsere Politik und Europa.">German posts</a></li>
    </ul>

                    </li>
                </ul>
                </div>
            </div>
        </div><!--/container-->
            <footer id="footer">
                <a href="//martin-thoma.com"><strong itemprop="publisher">Martin Thoma</strong></a> -  A blog about Code, the Web and Cyberculture. <br />
                <div class="footer-credits">
                    <a href="http://flexithemes.com/themes/modern-style/">Modern Style</a> theme by <a href="http://flexithemes.com/">FlexiThemes</a>
                </div>
            </footer><!--/footer-->

    </div><!--/wrapper-->
<!-- type: footer -->
<!-- TOC Plus -->
<script type='text/javascript'>
/* <![CDATA[ */
var tocplus = {"visibility_show":"show","visibility_hide":"hide","width":"275px"};
/* ]]> */
</script>
<script type='text/javascript' src="//martin-thoma.com/js/tocplus-front.js"></script>
<script>
(function(){
    'use strict';

    /*
    Create intra-page links
    Requires that your headings already have an `id` attribute set (because that's what jekyll does)
    For every heading in your page, this adds a little anchor link `#` that you can click to get a permalink to the heading.
    Ignores `h1`, because you should only have one per page.
    The text content of the tag is used to generate the link, so it will fail "gracefully-ish" if you have duplicate heading text.

    Credit: https://gist.github.com/SimplGy/a229d25cdb19d7f21231
     */

    var headingNodes = [], results, link,
        tags = ['h2', 'h3', 'h4', 'h5', 'h6'];

    tags.forEach(function(tag){
        var contentTag = document.getElementById('contentAfterTitle');
      results = contentTag.getElementsByTagName(tag);
      Array.prototype.push.apply(headingNodes, results);
    });

    headingNodes.forEach(function(node){
      link = document.createElement('a');
      link.className = 'deepLink';
      link.textContent = ' ¶';
      link.href = '#' + node.getAttribute('id');
      node.appendChild(link);
    });

  })();
</script>
</body>
</html>

