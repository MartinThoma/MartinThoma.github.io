<!DOCTYPE html>
<html lang="en">
  <!-- type: head.html -->
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    
    

    
        <meta name="thumbnail" content="//martin-thoma.com/images/logos/klausur.png" />
        <meta property="og:image" content="//martin-thoma.com/images/logos/klausur.png" />
    

    <meta property="og:type" content="blog"/>

    <title>Machine Learning 2</title>
    <link rel="stylesheet" href="//martin-thoma.com/css/screen.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="//martin-thoma.com/css/style.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="//martin-thoma.com/css/pygments.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="//martin-thoma.com/css/tocplus-screen.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="//martin-thoma.com/css/print.css" type="text/css" media="print" />
    <link rel="stylesheet" href="//martin-thoma.com/css/handheld.css" type="text/css" media="only screen and (max-device-width: 480px)" />

    <link rel="alternate" type="application/rss+xml" title="Martin Thoma RSS Feed" href="//martin-thoma.com/feed/" /><!--TODO-->
    <link rel="shortcut icon" href="//martin-thoma.com/favicon.ico" type="image/x-icon" />

    <link rel="canonical" href="//martin-thoma.com/machine-learning-2-course" />
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:site" content="@themoosemind"/>
<meta name="twitter:creator" content="@themoosemind"/>
<meta name="twitter:title" content="Machine Learning 2"/>

    <meta name="twitter:description" content="A blog about Code, the Web and Cyberculture" />


    <meta name="twitter:image" content="//martin-thoma.com/images/logos/klausur.png"/>



<meta name="twitter:url" content="//martin-thoma.com/machine-learning-2-course"/>
<meta name="twitter:domain" content="Martin Thoma.com"/>


    <script type='text/javascript' src="//martin-thoma.com/js/jquery.js"></script>
    <script type='text/javascript' src="//martin-thoma.com/js/jquery-migrate.min.js"></script>
    <style type="text/css">div#toc_container {width: 275px;}</style>
    <style type="text/css" id="syntaxhighlighteranchor"></style>

<!-- Latest compiled and minified CSS bootstrap -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">
</head>

<!-- type: post.html -->
<body>
    <div id="wrapper">
        <div id="container" class="container">
            <div class="span-16">
                <!-- type: header.html -->
<div id="header" role="banner">
    <h1><a href="//martin-thoma.com">Martin Thoma</a></h1>
    <h2>A blog about Code, the Web and Cyberculture.</h2>
</div>
<nav class="navcontainer" role="navigation">
    <ul id="nav">
        <li class=""><a href="//martin-thoma.com">Home</a></li>
        <li class="page_item page-item-41 "><a href="//martin-thoma.com/author/martin-thoma/">About Me</a></li>
        <li class="page_item page-item-91 "><a href="//martin-thoma.com/imprint/">Imprint</a></li>
    </ul>
</nav>

                <div id="content">
                    <article class="post type-post format-standard hentry clearfix ">
                        <h2 class="title entry-title">Machine Learning 2</h2>
                        <div class="postdate entry-date">
                            <time datetime="2015-05-11T11:00:00+02:00">
                                May
                                11th,
                                  
                                2015
                            </time>
                        </div>

                        <div class="entry">
                            <div class="info">Dieser Artikel beschäftigt sich mit der Vorlesung „Machine Learning 2“ am KIT. Er dient als Prüfungsvorbereitung. Ich habe die Vorlesungen bei <a href="http://tks.anthropomatik.kit.edu/21_52.php">Herrn Prof. Dr. Marius Zöllner</a> im Sommersemester 2015 gehört. Der Artikel wird bis zur mündlichen Prüfung laufend erweitert.<br />Es gibt auch einen Artikel zu <a href="http://martin-thoma.com/machine-learning-1-course/">Machine Learning 1</a></div>

<div id="toc_container" class="toc_light_blue no_bullets">
   <p class="toc_title">Contents</p>
   <ul class="toc_list">
      <li class="toc_level-1 toc_section-1">
         <a href="#tocAnchor-1-1"><span class="tocnumber">1</span> <span class="toctext">Behandelter Stoff</span></a>
         <ul>
            <li class="toc_level-2 toc_section-2">
               <a href="#tocAnchor-1-1-1"><span class="tocnumber">1.1</span> <span class="toctext">Vorlesung</span></a>
            </li>
            <li class="toc_level-2 toc_section-3">
               <a href="#tocAnchor-1-1-2"><span class="tocnumber">1.2</span> <span class="toctext">Einführung</span></a>
            </li>
            <li class="toc_level-2 toc_section-4">
               <a href="#tocAnchor-1-1-3"><span class="tocnumber">1.3</span> <span class="toctext">Semi-Supervised Learning</span></a>
            </li>
            <li class="toc_level-2 toc_section-5">
               <a href="#tocAnchor-1-1-4"><span class="tocnumber">1.4</span> <span class="toctext">Semi-Supervised and Active Learning</span></a>
            </li>
            <li class="toc_level-2 toc_section-6">
               <a href="#tocAnchor-1-1-5"><span class="tocnumber">1.5</span> <span class="toctext">Reinforcement Learning</span></a>
            </li>
            <li class="toc_level-2 toc_section-7">
               <a href="#tocAnchor-1-1-6"><span class="tocnumber">1.6</span> <span class="toctext">Dynamische Bayessche Netze</span></a>
            </li>
            <li class="toc_level-2 toc_section-8">
               <a href="#tocAnchor-1-1-7"><span class="tocnumber">1.7</span> <span class="toctext">Probablistisch Relationale Modelle</span></a>
            </li>
            <li class="toc_level-2 toc_section-9">
               <a href="#tocAnchor-1-1-8"><span class="tocnumber">1.8</span> <span class="toctext">Gaussche Prozesse</span></a>
            </li>
            <li class="toc_level-2 toc_section-10">
               <a href="#tocAnchor-1-1-9"><span class="tocnumber">1.9</span> <span class="toctext">Deep Learning</span></a>
            </li>
            <li class="toc_level-2 toc_section-11">
               <a href="#tocAnchor-1-1-10"><span class="tocnumber">1.10</span> <span class="toctext">Convolutional Neural Networks</span></a>
            </li>
            <li class="toc_level-2 toc_section-12">
               <a href="#tocAnchor-1-1-11"><span class="tocnumber">1.11</span> <span class="toctext">Spiking Neural Nets</span></a>
            </li>
            <li class="toc_level-2 toc_section-13">
               <a href="#tocAnchor-1-1-12"><span class="tocnumber">1.12</span> <span class="toctext">Evaluation</span></a>
            </li>
         </ul>
      </li>
      <li class="toc_level-1 toc_section-14">
         <a href="#tocAnchor-1-14"><span class="tocnumber">2</span> <span class="toctext">Material und Links</span></a>
      </li>
      <li class="toc_level-1 toc_section-15">
         <a href="#tocAnchor-1-15"><span class="tocnumber">3</span> <span class="toctext">Übungsbetrieb</span></a>
      </li>
      <li class="toc_level-1 toc_section-16">
         <a href="#tocAnchor-1-16"><span class="tocnumber">4</span> <span class="toctext">Kontakt</span></a>
      </li>
      <li class="toc_level-1 toc_section-17">
         <a href="#tocAnchor-1-17"><span class="tocnumber">5</span> <span class="toctext">Termine und Klausurablauf</span></a>
      </li>
   </ul>
</div><h2 id="tocAnchor-1-1">Behandelter Stoff</h2>

<h3 id="tocAnchor-1-1-1">Vorlesung</h3>

<table>
<tr>
    <th>Datum</th>
    <th>Kapitel</th>
    <th>Inhalt</th>
</tr>
<tr>
    <td>15.04.2015</td>
    <td><a href="https://ilias.studium.kit.edu/ilias.php?ref_id=429607&amp;cmd=sendfile&amp;cmdClass=ilrepositorygui&amp;cmdNode=ed&amp;baseClass=ilRepositoryGUI">Einleitung</a></td>
    <td>Wiederholung ML1; Definition 'Machine Learning'</td>
</tr>
<tr>
    <td>24.04.2015</td>
    <td><a href="https://ilias.studium.kit.edu/ilias.php?ref_id=432731&amp;cmd=sendfile&amp;cmdClass=ilrepositorygui&amp;cmdNode=ed&amp;baseClass=ilRepositoryGUI">Semi Supervised Learning (SSL)</a></td>
    <td>Transduktives Lernen; Self-Learning; Co-Training; Generative Models; EM-Algorithmus; Low-Density seperation; Transductive SVM</td>
</tr>
<tr>
    <td>01.05.2015</td>
    <td>-</td>
    <td>Vorlesung fällt aus (vgl. erster Foliensatz, Folie 49)</td>
</tr>
<tr>
    <td>08.05.2015</td>
    <td><abbr title="Semi-Supervised Learning">SSL</abbr></td>
    <td>Transductive SVM, Aktives Lernen</td>
</tr>
<tr>
    <td>15.05.2015</td>
    <td>-</td>
    <td>Vorlesung fällt aus (vgl. erster Foliensatz, Folie 49)</td>
</tr>
<tr>
    <td>22.05.2015</td>
    <td>Reinforcement Learning</td>
    <td>Beschreibung als MDP, Strategielernen, Bellman-Gleichungen; Q-Learning, <abbr title="Hierarchische Abstrakte Maschinen">HAM</abbr>, MAXQ</td>
</tr>
<tr>
    <td>05.06.2015</td>
    <td>-</td>
    <td>Vorlesung fällt aus (vgl. erster Foliensatz, Folie 49)</td>
</tr>
</table>

<h3 id="tocAnchor-1-1-2">Einführung</h3>

<p>Slides: <code>01_Einfu__hrung_MLII.pdf</code></p>

<p>Rückblick auf <a href="//martin-thoma.com/machine-learning-1-course/">ML 1</a>.</p>

<p>Meine Fragen (TODO):</p>

<ul>
<li>Folie 27: Was heißt MLNN?</li>
</ul>

<h3 id="tocAnchor-1-1-3">Semi-Supervised Learning</h3>

<p>Slides: <code>02_Semi-supervised-learning.pdf</code></p>

<dl>
  <dt><dfn>Überwachtes Lernen</dfn> (engl. <dfn>Supervised Learning</dfn>)</dt>
  <dd>Alle Trainingsdaten liegen mit Labels vor.</dd>
  <dt><dfn>Unüberwachtes Lernen</dfn> (engl. <dfn>Unsupervised Learning</dfn>)</dt>
  <dd>Alle Trainingsdaten liegen ohne Labels vor.</dd>
  <dt><a href="https://en.wikipedia.org/wiki/Semi-supervised_learning"><dfn>Semi-Supervised Learning</dfn></a> (<dfn>SSL</dfn>)</dt>
  <dd>Die meisten Trainingsdaten liegen ohne Labels vor, jedoch gibt es für
      jede Klasse auch gelabelte Daten.</dd>
  <dt><dfn>Self-Learning</dfn> (<dfn>Self-Training</dfn>, <dfn>Self-Labeling</dfn>, <dfn>Decision-directed learning</dfn>)</dt>
  <dd><i>Self-Training</i> ist ein Algorithmus zum Semi-supervised Learning.
      Er geht wie folgt vor:
      <ol>
          <li>Trainiere mit gelabelten Daten.</li>
          <li>Werte ungelabelte Daten aus.</li>
          <li>Füge Daten, bei denen sich der Klassifizierer sicher ist, zu
              den Trainingsdaten hinzu.</li>
          <li>Zurück zu Schritt 1.</li>
      </ol>

      Dabei sind folgende Variationen vorstellbar:
      <ul>
          <li>Füge alle Daten hinzu.</li>
          <li>Füge nur Daten hinzu, bei denen sich der Klassifizierer sicher ist.</li>
          <li>Gewichte Daten mit der Sicherheit.</li>
      </ul>
  </dd>
  <dt><dfn>Co-Training</dfn> (<dfn>Mit-Lernen</dfn>)</dt>
  <dd><i>Co-Training</i> ist ein Algorithmus zum Semi-supervised Learning.
      Er geht wie folgt vor:
      <ol>
          <li>Splitte jeden Feature-Vektor auf die gleiche Art in zwei
              Feature-Vektoren mit disjunkten Features auf.</li>
          <li>Trainiere zwei unterschiedliche Klassifizierer auf den beiden
              unterschiedlichen Feature-Mengen der gelabelten Daten.</li>
          <li>Label mit den beiden Klassifizieren die ungelabelten Daten.</li>
          <li>Füge ungelabelte Daten dem Trainingsdatensatz (also den
              gelabelten Daten) hinzu, falls die Klassifizierer für diese eine
              hohe Konfidenz aufweisen.</li>
          <li>Zurück zu Schritt 2.</li>
      </ol>

      Dabei sind folgende Variationen vorstellbar:
      <ul>
          <li>Demokratisches Voting: Bei mehr als 2 Klassifizierern.</li>
          <li>Schwellwert: Nur hinzufügen, wenn alle Klassifizierer jeweils
              eine Schwelle überschreiten.</li>
          <li>Gewichtes Voting: Alle Klassifizierer zusammen müssen eine
              Schwelle überschreiten.</li>
      </ul>
  </dd>
  <dt><dfn>Low Density Separation</dfn></dt>
  <dd>Methoden, welche Low Density separation benutze versuchen die
      Entscheidungsgrenze in eine Region niedriger Dichte zu legen. Ein Beispiel
      ist die <i>Transductive SVM</i>.</dd>
</dl>

<p>Weiteres:</p>

<p>Hier könnte ich mir gut vorstellen, dass man eine Bachelor / Master-Arbeit
macht. Man könnte sich große gelabelte Datensätze suchen, einen gewissen Teil
der Labels weglassen (also einige Trainingsdaten als "ungelabelt" behandeln)
und die verschiedenen <abbr title="Semi-Supervised Learning">SSL</abbr>-Methoden
untersuchen. Bis zu 20% gelabelte Daten hoch wäre es interessant; also z.B.
(0.5%, 1%, 2%, 3%, 5%, 10%, 15%, 20% gelabelte Daten). Mit mehr gelabelten
Daten könnte man argumentieren, dass man es sich vermutlich leisten könnte auch
den Rest noch zu labeln. Siehe Folie 28-31.</p>

<h3 id="tocAnchor-1-1-4">Semi-Supervised and Active Learning</h3>

<p>Slides: <code>03_Semi-supervised+Active-learning.pdf</code></p>

<dl>
  <dt><a href="https://de.wikipedia.org/wiki/Lagrange-Multiplikator"><dfn>Lagrange-Multiplikator</dfn></a></dt>
  <dd>TODO</dd>
  <dt><dfn>Active Learning</dfn></dt>
  <dd>Die Lernmaschine wählt die zu lernenden Daten selbst aus.</dd>
  <dt><dfn>Query Synthesis</dfn> (siehe <a href="http://burrsettles.com/pub/settles.activelearning.pdf">Active Learning Literature Survey</a>)</dt>
  <dd>Der Lerner kann Feature-Vektoren (Querys)
      <a href="https://en.wikipedia.org/wiki/De_novo">de novo</a>, also von
      Grund auf neu / selbst erzeugen. Er kann für diesen neuen Query ein
      Orakel befragen, was das Label ist.</dd>
  <dt><dfn>Selective Sampling</dfn> (Selektive Entnahme, siehe <a href="http://dl.acm.org/citation.cfm?id=2503327">Selective sampling and active learning from single and multiple teachers</a>)</dt>
  <dd>Selective Sampling ist eine Methode des aktiven Lernens. Dabei wird
      jede Runde \(t\) dem Lerner ein Feature-Vektor \(x_t \in \mathbb{R}^n\)
      präsentiert. Der Lerner muss sich jede Runde entscheiden, ob er einen
      Preis bezahlt um das Label zu sehen. Der Lerner hat also zwei Ziele, die
      miteinander in Konflikt stehen: Er will alles richtig klassifizieren,
      aber zugleich die Kosten so niedrig wie möglich halten.</dd>
  <dt><dfn>Pool-Based Active Learning</dfn></dt>
  <dd>Pool-Based Active Learning ist eine Methode des aktiven Lernens. Dabei
      wird von einem Pool an ungelabelten Daten \(\mathcal{U}\) ausgegangen
      und einem deutlich kleineren Pool \(\mathcal{L}\) an gelabelten Daten.
      Queries werden aus \(\mathcal{U}\) gezogen. Dabei wird ganz
      \(\mathcal{U}\) evaluiert und für den hilfreichsten Feature-Vektor
      \(x \in \mathcal{U}\) nach einem Label gefragt.</dd>
  <dt><dfn>Hinge-Funktion</dfn></dt>
  <dd>\[f(x) = \max(x, 0)\]</dd>
  <dt>Query-by-Committee (<dfn>QBC</dfn>)</dt>
  <dd>Es wird ein Committee \(\mathcal{C}\) an Klassifikatoren trainiert,
      welches gemeinsam (z.B. durch majority vote) eine Klassifikation trifft.

    Allgemeiner Ansatz:
    <ul>
        <li>Trainiere eine Menge \(\mathcal{C}\) an Klassifikatoren</li>
        <li>Wähle neue Daten, wenn die Hypothesen Wiedersprüchlich sind</li>
    </ul>

    Selektive Entnahme:
    <ol>
        <li>Beobachte neue Instanz \(x\) und werte diese mit \(\mathcal{C}\) aus</li>
        <li>Frage das Label ab, falls es einen Wiederspruch in den Hypothesen
            von \(\mathcal{C}\) für \(x\) gibt.</li>
        <li>Neu trainiren, zurück zu 1</li>
    </ol>

    Pool-based Active Learning:
    <ol>
        <li>Messung des Wiederspruchs der Hypothesen für alle Instanzen \(x\)</li>
        <li>Ranking (z.B. Entropie)</li>
        <li>Abfrage der Labels für die \(k\) widersprüchlichsten Instanzen</li>
        <li>Neu trainiren, zurück zu 1</li>
    </ol>
  </dd>
</dl>

<p>Weiteres:</p>

<ul>
<li>Ausreißerproblem: Ausreißer sollten im QBC nicht genommen werden. Dazu könnte
die Dichte im Datenraum gemessen werden.</li>
</ul>

<h3 id="tocAnchor-1-1-5">Reinforcement Learning</h3>

<p>Slides: <code>04_Reinforcement_Learning_II.pdf</code></p>

<dl>
  <dt><dfn>Options</dfn></dt>
  <dd>Eine <i>Option</i> ist wohl-definiertes Verhalten, welches im
      hierarchischen <abbr title="Reinforcement Learning">RL</abbr> eingesetzt
      werden kann. Es ist ein Baustein für komplexe Pläne.</dd>
  <dt><dfn>Hierarchien Abstrakter Maschinen</dfn> (<dfn>HAM</dfn>)</dt>
  <dd>Ein <abbr title="Markov Decision Process">MDP</abbr> wird mit
      Maschinen \(\{M_i\}\) kombiniert. Jede Maschine repräsentiert einen
      Teil der Policy. Jede Maschine verwendet eigene Zustände \(m_t^i\)
      und globale Zustände \(s_t\). Maschinen werden durch Zustandsautomaten
      abgebildet.</dd>
  <dt><dfn>MAXQ</dfn></dt>
  <dd>TODO</dd>
</dl>

<p>Folie 35:</p>

<ul>
<li>Was heißt hier "mit festen Knoten"?</li>
</ul>

<h3 id="tocAnchor-1-1-6">Dynamische Bayessche Netze</h3>

<p>Slides: <code>05_DynamischeBayesscheNetze.pdf</code></p>

<dl>
  <dt><a href="https://de.wikipedia.org/wiki/Bedingte_Wahrscheinlichkeit#Multiplikationssatz"><dfn>Multiplikationssatz</dfn></a></dt>
  <dd>Seien \(A, B, X_i\) Ereignisse. Dann gilt:
      \[P(X_1, \dots, X_n) = P(X_1) \cdot \prod_{k=2}^n P(X_k | X_{k-1}, \dots, X_1)\]
      und insbesondere
      \[P(A\cap B) = P(A, B) = P(A\mid B) \cdot P(B)\]</dd>
  <dt><a href="https://de.wikipedia.org/wiki/Bedingte_Wahrscheinlichkeit#Gesetz_der_totalen_Wahrscheinlichkeit"><dfn>Gesetz der totalen Wahrscheinlichkeit</dfn></a></dt>
  <dd>Seien \(A_1, \dots, A_n\) paarweise disjunkte Ereignisse mit
      \(\sum_{i=1}^n\). Dann gilt für jedes beliebige Ereignis \(B\):
      \[P(B) = \sum_{i=1}^n P(B | A_i) \cdot P(A_i) = P(A_i, B)\]</dd>
  <dt><a href="https://de.wikipedia.org/wiki/Satz_von_Bayes"><dfn>Satz von Bayes</dfn></a></dt>
  <dd>Seinen \(A, B\) Ereignisse mit \(P(B) &gt; 0\). Dann gilt
      \[P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}\]

      Hierbei heißt \(P(A|B)\) die <i>a posteriori Wahrscheinlichkeit</i>,
      \(P(B|A)\) die <i>likelihood</i>, \(P(A)\) die
      <i>a priori Verteilung über \(A\)</i> und \(P(B)\) die
      <i>a priori Verteilung über \(B\)</i>.</dd>
  <dt><a href="https://de.wikipedia.org/wiki/Bayessches_Netz"><dfn>Bayessches Netz</dfn></a></dt>
  <dd>Ein <i>Bayessches Netz</i> ist ein <abbr title="Directed Acyclical Graph">DAG</abbr> TODO
      Bayessche Netze sind zur Modellierung kausaler Zusammenhänge geeignet.</dd>
  <dt><a href="https://de.wikipedia.org/wiki/Markov_Random_Field"><dfn>Markov Random Field</dfn></a></dt>
  <dd>Ein <i>MRF</i> ist ein ungerichteter Graph TODO.
      MRFs sind zur Modellierung von Korrelation geeignet.</dd>
  <dt><dfn>Dynamisches Bayessches Netz</dfn></dt>
  <dd><i>Dynamische Bayessche Netze</i> sind Bayessche Netze zur Beschreibung
      dynamischer Prozesse.</dd>
  <dt><a href="https://en.wikipedia.org/wiki/Markov_blanket"><dfn>Markov Blanket</dfn></a></dt>
  <dd>Sei \(G=(V,E)\) ein DAG zu einem Bayesschen Netz und \(v_S \in V\).
      Dann ist der Markov Blanket die folgende Knotenmenge \(B \subseteq V \setminus \{v_S\}\):

      <ul>
          <li>Die Elternknoten von \(v_S\) sind in \(B\).</li>
          <li>Die Kindknoten \(K = \{v_{K_1}, \dots, v_{K_n}\}\) sind in \(B\)</li>
          <li>Die Elternknoten von \(K\), ausgenommen von \(v_S\), sind in
              \(B\)</li>
      </ul>

      Diese Knotenmenge macht \(v_S\) unabhängig von anderen Knoten.</dd>
  <dt><a href="https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering"><dfn>Naive Bayes Spam Filter</dfn></a></dt>
  <dd>Ein naiver Bayes Spamfilter nutzt häufig Bag-of-Words Features. Man berechnet die Wahrscheinlichkeit,
      dass eine gegebene E-Mail Spam ist. Dazu geht man davon aus, dass die
      Wörter in einer E-Mail unabhängig von einander sind und nutzt den
      Satz von Bayes.
      Siehe <a href="https://de.wikipedia.org/wiki/Bayes-Klassifikator#Beispiel">Bayes-Klassifikator</a>
      für eine detailiertere Beschreibung.</dd>
  <dt>Bayes Filter</dt>
  <dd>Filter + Predict (TODO)</dd>
  <dt><a href="https://de.wikipedia.org/wiki/Kalman-Filter"><dfn>Kalman-Filter</dfn></a></dt>
  <dd>Der Kalman-Filter ist ein Bayes-Filter. Er wird z.B. zum Schätzen einer Fahrzeugtrajektorie eingesetzt. TODO</dd>
</dl>

<p>Typische Fragestellungen:</p>

<ul>
<li>Gegeben ist die Struktur eines Bayesschen Netzes: Wie lautet die Verteilung?

<ul>
<li>Dies wird üblicherweise mit dem <abbr title="Expectation Maximimization">EM</abbr>-Algorithmus
gelöst.</li>
</ul></li>
</ul>

<p>Anwendungsfälle:</p>

<ul>
<li>Automatische Diagnose, gegeben die Symptome (Bayessches Netz)</li>
<li>Fahrzeugverfolgung: Vorhersage von Routen, welche die Fahrzeuge nehmen werden
(Dynamisches Bayessches Netz)</li>
</ul>

<p>Anmerkungen: Die Folien sind hier sehr gut! Insbesondere Folie 14-23
sollte man sich ansehen.</p>

<p>Es scheint folgende Beziehung zu gelten: HMMs, Kalman-Filter, Extended
Kalman-Filter, Partikel Filter sind Beispiele für Bayes-Filter. Bayes-Filter
sind Beispiele für dynamische Bayessche Netze.</p>

<p>Siehe auch:</p>

<ul>
<li>Udacity: <a href="https://www.youtube.com/watch?v=8O9GV4SUToA&amp;index=77&amp;list=PLAwxTw4SYaPkCSYXw6-a_aAoXVKLDwnHK">Artificial Intelligence for Robotics</a> - good content for Kalman Filters</li>
</ul>

<ul class="gallery mw-gallery-traditional" style="max-width: 326px; width: 326px;">
   <li class="gallerybox" style="width: 155px">
      <div style="width: 155px">
         <div class="thumb" style="width: 150px;">
            <div style="margin:21px auto;height: 113px;line-height: 150px;">
               <a href="../images/2016/01/tracking-robots.png" class="image">
                  <img src="//martin-thoma.com/captions/tracking-robots.png" alt="" style="max-width: 120px; max-height: 120px;" />
               </a>
            </div>
         </div>
         <div class="gallerytext">Tracking Robots</div>
      </div>
   </li>
   <li class="gallerybox" style="width: 155px">
      <div style="width: 155px">
         <div class="thumb" style="width: 150px;">
            <div style="margin:21px auto;height: 113px;line-height: 150px;">
               <a href="../images/2016/01/probabilisitc-graphical-models.png" class="image">
                  <img src="//martin-thoma.com/captions/probabilisitc-graphical-models.png" alt="" style="max-width: 120px; max-height: 120px;" />
               </a>
            </div>
         </div>
         <div class="gallerytext">Probabilistic Graphical Models</div>
      </div>
   </li>
</ul>

<h3 id="tocAnchor-1-1-7">Probablistisch Relationale Modelle</h3>

<p>Slides: <code>06_Probablistisch_Relationale_Modelle.pdf</code></p>

<p>TODO</p>

<dl>
  <dt><dfn>Objektorientierte Probablistisch Relationales Modelle</dfn> (<dfn>OPRM</dfn>)</dt>
  <dd>TODO</dd>
</dl>

<h3 id="tocAnchor-1-1-8">Gaussche Prozesse</h3>

<p>Slides: <code>07_Gaussche_Prozesse.pdf</code></p>

<p>I suggest reading the first two chapters of the online book
<a href="http://www.gaussianprocess.org/">gaussianprocess.org</a> before
starting to read the slides.</p>

<dl>
  <dt><a href="https://de.wikipedia.org/wiki/Lineare_Regression"><dfn>Lineare Regression</dfn></a></dt>
  <dd>Die lineare Regression ist ein Modell zur approximation von Datenpunkten
      \((x, y) \in \mathbb{R}^n \times \mathbb{R}\) durch eine
      lineare Funktion, d.h. einer Funktion der Form \(f(x) = x^T \cdot w\).
      Dabei ist \(w \in \mathbb{R}^n\).

      Wenn man als Optimierungskriterium den quadratischen Abstand
      \[E(f, data) = \sum_{(x,y) \in data} (f(x) - y)^2\]
      nimmt, dann ist eine optimale Lösung durch
      \[w = (X^T X)^{-1} X^T y\]
      gegeben.

      Siehe auch: <a href="http://math.stackexchange.com/q/691812/6876">Proof of when is \(A=X^T X\) invertible?</a>
      </dd>
  <dt><dfn>Affine Regression</dfn></dt>
  <dd>Die affine Regression ist ein Modell zur approximation von Datenpunkten
      \((x, y) \in \mathbb{R}^n \times \mathbb{R}\) durch eine
      affine Funktion, d.h. einer Funktion der Form \(f(x) = x^T \cdot w + b\).
      Dabei ist \(w \in \mathbb{R}^n, b \in \mathbb{R}\). Um das Problem auf
      ein lineares zu reduzieren kann man den Feature-Vektor \(x\) durch ein
      konstantes Feature \(x_0 = 1\) erweitern.
      </dd>
  <dt><dfn>Korrelationskoeffizient</dfn></dt>
  <dd>Der Korrelationskoeffizient \(\kappa(X, Y) \in [-1, 1]\) ist ein Maß für
      den linearen Zusammenhang zwischen zwei Zufallsvariablen \(X, Y\). Er
      ist definiert als
      \[\kappa(X, Y) := \frac{Cov(X, Y)}{\sigma(X) \cdot \sigma(Y)}\]</dd>
  <dt><a href="https://de.wikipedia.org/wiki/Gau%C3%9F-Prozess"><dfn>Gausscher Prozess</dfn></a> (<dfn>Kriging</dfn>)</dt>
  <dd>Gaussche Prozesse approximieren eine Funktion dadurch, dass sie an jedem
      Punkt eine Normalverteilung (Gauss-Verteilung) annehmen.

      Siehe <a href="https://en.wikipedia.org/wiki/Kriging">Gaussian process regression</a></dd>
</dl>

<h3 id="tocAnchor-1-1-9">Deep Learning</h3>

<p>Slides: <code>08_DeepLearning.pdf</code></p>

<p>Siehe auch: <a href="//martin-thoma.com/neuronale-netze-vorlesung/">Neuronale Netze Vorlesung</a></p>

<dl>
  <dt><dfn>Deep Belief Netz</dfn> (<dfn>DBN</dfn>)</dt>
  <dd>Ein Deep Belief Netz ist ein gerichtetes, azyklisches, probabilistisches
      graphisches Modell.</dd>
  <dt><dfn>Restricted Boltzmann Machine</dfn> (<dfn>RBM</dfn>)</dt>
  <dd>Eine <i>RBM</i> ist ein neuronales Netz mit nur einem Hidden Layer.
      Es werden keine Verbindungen zwischen den Hidden Units erlaubt.</dd>
  <dt><dfn>Contrastive Wake-Sleep Algorithm</dfn></dt>
  <dd>TODO (Folie 34) - see <a href="http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf">A Fast Learning Algorithm for Deep Belief Nets</a></dd>
</dl>

<h4 id="probleme-von-tiefen-netzen-und-wie-man-sie-lösen-kann:">Probleme von Tiefen Netzen und wie man sie lösen kann:</h4>

<ul>
<li><strong>Lange Trainingsdauer</strong>: GPUs / mehr Rechenpower / weniger Parameter durch
Parameter sharing, z.B. in <abbr title="Convolutional Neural Networks">CNNs</abbr>
/ <abbr title="Time Delay Neural Networks">TDNNs</abbr></li>
<li><strong>Extrem viele gelabelte Trainingsdaten werden benötigt</strong>: Internet
(z.B. Wikipedia, Soziale Netzwerke, Amazon Mechanical Turk) reduziert dieses
Problem; Nutzen ungelabelter Daten durch <abbr title="Semi-Supervised Learning">SSL</abbr>
in Auto-Encodern</li>
<li><strong>Lokale Minima</strong></li>
<li><strong>Overfitting</strong>: Regularis</li>
</ul>

<h4 id="siehe-auch">Siehe auch</h4>

<ul>
<li><a href="http://www.cs.toronto.edu/%7Ehinton/adi/index.htm">MNIST Demo</a> (Flash):
Neuronales Netz welches Ziffern generiert</li>
<li>Geoffry Hinton: <a href="https://www.youtube.com/watch?v=IcOMKXAw5VA">Deep Learning</a>
on YouTube, 2015. 43 minutes. (Topics: RBMs)</li>
</ul>

<h3 id="tocAnchor-1-1-10">Convolutional Neural Networks</h3>

<p>Slides: <code>09_ConvolutionalNeuralNetworks.pdf</code></p>

<p>Siehe auch: <a href="//martin-thoma.com/neuronale-netze-vorlesung/">Neuronale Netze Vorlesung</a></p>

<dl>
  <dt><a href="https://de.wikipedia.org/wiki/Convolutional_Neural_Network"><dfn>Convolutional Neural Networks</dfn></a> (<dfn>CNNs</dfn>)</dt>
  <dd><abbr title="Convolutional Neural Networks">CNNs</abbr> sind neuronale
      Netze welche weight sharing einsetzen. Sie setzen eine diskrete Faltung
      um. Ein CNN muss mindestens einen <i>Convolutional Layer</i> haben.
      Dieser hat folgende Parameter:
      <ul>
          <li>Padding: None, Zero, Copy</li>
          <li>Stride: \(s \in \mathbb{N}_{&gt; 0}\)</li>
          <li>Filter Size: \((x,y) \in \mathbb{N}^2\)</li>
          <li>Number of filters: How many filters should get learned?</li>
      </ul></dd>
  <dt><dfn>Feature Map</dfn></dt>
  <dd>Nach einem Convolutional Layer hat man die Ausgabe der Filter, welche
      auf die Eingabe angewandt wurden. Diese nennt man <i>Feature Map</i>.
      Für jeden Filter bekommt man eine Feature Map. Die Feature Maps sind
      wiederum Eingaben für die nächsten Schichten.</dd>
  <dt><dfn>Pooling Layer</dfn></dt>
  <dd>Ein <i>pooling layer</i> ist eine Schicht in einem CNN, welche
      Features zusammenfasst. Pooling Schichten haben folgende Parameter:

      <ul>
          <li>Größe: Typischerweise \(3 \times 3\)</li>
          <li>Stride \(s \in \mathbb{N}\): Typischerweise gleich der Größe des Pooling-Bereichs (also 3).</li>
          <li>Art: max, mean</li>
      </ul>

      Typischerweise reduziert sie die Anzahl der Features, da typischerweise
      ein \(s &gt; 1\) gewählt wird.</dd>
</dl>

<h3 id="tocAnchor-1-1-11">Spiking Neural Nets</h3>

<p>Slides: <code>10_SpikingNeuralNets.pdf</code></p>

<dl>
  <dt><a href="https://de.wikipedia.org/wiki/Gepulste_neuronale_Netze"><dfn>Spiking Neural Networks</dfn></a></dt>
  <dd>Gepulste neuronale Netze versuchen natürliche neuronen realistisch
      abzubilden. Das Hodgkin-Huxley Neuronenmodell wurde bereits 1952
      vorgestellt.</dd>
  <dt><dfn>Hodgkin-Huxley Neuronenmodell</dfn></dt>
  <dd>Das <i>Hodgkin-Huxley Neuronenmodell</i> modelliert die elektrochemischen
      Vorgänge innerhalb eines Neurons mit elektrischen Baugliedern. Dies
      resultiert in Differenzialgleichungen mit 4 Variablen (Kapazität
      der Membran, Widerstände der Ionenkanäle, Gleichgewichtspotentiale,
      Öffnung der Ionenkanäle).

      Das Modell ist realistisch, aber sehr komplex. </dd>
  <dt><dfn>LIF Neuronenmodell</dfn> (Leaky integrate and Fire)</dt>
  <dd>Das <i>LIF Neuronenmodell</i> modelliert ein Neuron durch eine
      gewöhnliche Differentialgleichung erster Ordnung.</dd>
  <dt><dfn>SRM Neuronenmodell</dfn> (Spike Response Model)</dt>
  <dd>Das <i>SRM Neuronenmodell</i> modelliert die Refraktionszeit. Das ist
      die Zeit, in der kein neues Aktionspotential aufgebaut werden kann.

      Das SRM ist ein rein phänomenologisches Modell, welches trotz der
      Einfachheit allgemeiner ist als das LIF-Modell.</dd>
</dl>

<h3 id="tocAnchor-1-1-12">Evaluation</h3>

<p>Slides: <code>11_Evaluation.pdf</code></p>

<h4 id="für-klassifikation:">Für Klassifikation:</h4>

<dl>
  <dt><a href="https://de.wikipedia.org/wiki/Konfusionsmatrix"><dfn>Konfusionsmatrix</dfn></a></dt>
  <dd>Eine Konfusionsmatrix ist eine Tabelle, in welcher die Spalten angeben,
      welche Hypothese gemacht wurde (Testentscheid) und die Zeilen den wahren
      Wert angeben. So kann für beliebig viele Klassen gezeigt werden, wie gut
      der Klassifikator ist und welche Art der Verwechslung er macht.</dd>
  <dt><dfn>Klassifikationsfehler</dfn></dt>
  <dd>\(\text{Klassifikationsfehler} = \frac{\text{Fehlerhafte Hypothesen}}{\text{Anzahl aller Beispiele}} \in [0, 1]\)</dd>
  <dt><dfn>Klassifikationsgüte</dfn></dt>
  <dd>Klassifikationsgüte = 1 - Klassifikationsfehler</dd>
  <dt><dfn>False Alarm Rate</dfn> (<dfn>FA</dfn>, <dfn>Falsch Positiv Rate</dfn>, <dfn>FPR</dfn>)</dt>
  <dd>Es sei FP die Anzahl der False Positive Testdaten, also der Testdaten
      für welche <i>Positive</i> vorhergesagt wurde, die aber negative sind. Weiter
      sei TN die Anzahl der True Negatives, also der Testdaten, für welche
      korrekterweise negative vorhergesagt wurde.

      Dann ist die <i>FPR</i> definiert als
      \[\text{FPR} := \frac{FP}{FP + TN} \in [0, 1]\]

      Die FPR gibt also den Anteil an, wie viele der tatsächlich negativen
      fälschlicherweise als positiv erkannt wurden.</dd>
  <dt><dfn>Miss-Rate</dfn> (<dfn>MR</dfn>, <dfn>Falsch Negativ Rate</dfn>, <dfn>FNR</dfn>)</dt>
  <dd>\[FNR := \frac{FN}{TP + FN} \in [0, 1]\]</dd>
  <dt><dfn>Recall</dfn> (<dfn>True Positive Rate</dfn>, <dfn>TPR</dfn>, <dfn>Sensitivität</dfn>)</dt>
  <dd>\[TPR = \frac{TP}{TP + FN} = 1 - FNR \in [0, 1]\]

      Der Recall gibt den Anteil der erkannten positiven aus allen positiven
      an.

      <i>Sensitivität</i> ist ein in der Medizin üblicher Begriff.</dd>
  <dt><dfn>Precision</dfn> (<dfn>Genauigkeit</dfn>)</dt>
  <dd>\[Precision = \frac{TP}{TP + FP} \in [0, 1]\]

      Die Precision gibt den Anteil der real positiven aus den als positiv
      erkannten an.</dd>
  <dt><dfn>ROC-Graph</dfn> (<dfn>Receiver-Operator Curve</dfn>)</dt>
  <dd>Der ROC-Graph gibt für einen Klassifikator, bei dem man einen Parameter
      einstellen kann, den Fehler an.

      Die \(x\)-Achse ist dabei die FPR, die \(y\)-Achse die TPR.</dd>
  <dt><dfn>Spezifität</dfn></dt>
  <dd>Der Begriff der <i>Spezifität</i> ist in der Medizin üblich und
      ist definiert durch
      \[Spezifität = \frac{TN}{TN + FP} = 1 - FPR\]

      Es ist eine Art recall für die negative Klasse. Im Beispiel eines
      medizinischen Tests wäre das der Anteil der Gesunden, bei denen
      tatsächlich auch die Diagnose "Gesund" gestellt wurde.</dd>
  <dt><dfn>PRC-Graph</dfn> (<dfn>Precision-Recall-Graph</dfn>)</dt>
  <dd>Die \(x\)-Achse ist Recall, die \(y\)-Achse ist Precision.</dd>
  <dt><dfn>Selektivität</dfn></dt>
  <dd>TODO</dd>
  <dt><dfn>F-Maß</dfn></dt>
  <dd>\[F_\alpha = \frac{precision \cdot recall}{\alpha^2 \cdot precision + recall}\]</dd>
</dl>

<p>Alternative:</p>

<ul>
<li>Aufstellen einer Kostenfunktion und optimieren nach Kosten.</li>
<li>Plotten der Anzahl der Trainingsdaten ((x)-Achse) und des Fehlers
((y)-Achse). Die Kurven sollten der Test-Fehler sowie der Trainingsfehler
sein. Damit lässt sich abschätzen, ob mehr Trainingsdaten ohne eine
Veränderung des Modells hilfreich sind.</li>
</ul>

<h4 id="für-regression">Für Regression</h4>

<dl>
    <dt><dfn>Mittlerer Quadratischer Fehler</dfn> (<dfn>MSE</dfn>, <dfn>Mean Squared Error</dfn>)</dt>
    <dd>\[E(f, data) = \frac{1}{|data|} \sum_{(x, y) \in data} (f(x) - y)^2\]</dd>
    <dt><dfn>Relativer Quadratischer Fehler</dfn></dt>
    <dd>\[E(f, data) = \frac{\sum_{(x, y) \in data} (f(x) - y)^2}{\sum_{(x,y) \in data} (y - \mu)^2}\]</dd>
    <dt><dfn>Mittlerer Absoluter Fehler</dfn></dt>
    <dd>\[E(f, data) = \frac{1}{|data|} \sum_{(x, y) \in data} |f(x) - y|\]</dd>
</dl>

<h4 id="siehe-auch">Siehe auch</h4>

<ul>
<li><a href="https://de.wikipedia.org/wiki/Beurteilung_eines_bin%C3%A4ren_Klassifikators">Beurteilung eines binären Klassifikators</a></li>
<li><a href="https://en.wikipedia.org/wiki/False_positives_and_false_negatives">False positives and false negatives</a></li>
<li>Matt Zeiler: <a href="https://www.youtube.com/watch?v=ghEmQSxT6tw">Visualizing and Understanding Deep Neural Networks</a> on YouTube, 2015. 48 minutes.</li>
</ul>

<h2 id="tocAnchor-1-14">Material und Links</h2>

<ul>
<li><a href="http://tks.anthropomatik.kit.edu/28_176.php">Vorlesungswebsite</a></li>
<li><a href="https://ilias.studium.kit.edu/goto_produktiv_crs_429082.html">Ilias</a>: Ist passwortgeschützt</li>
<li><a href="//martin-thoma.com/machine-learning-1-course/">Zusammenfassung der Vorlesung ML 1</a></li>
</ul>

<h2 id="tocAnchor-1-15">Übungsbetrieb</h2>

<p>Es gibt keine Übungen und keine Übungsblätter.</p>

<h2 id="tocAnchor-1-16">Kontakt</h2>

<ul>
<li>goettl@fzi.de: Sonja Göttl (Sekretariat, zum Anmelden zur Prüfung)</li>
</ul>

<h2 id="tocAnchor-1-17">Termine und Klausurablauf</h2>

<p><strong>Datum</strong>: nach Terminvereinbarung<br />
<strong>Ort</strong>: ?<br />
<strong>Übungsschein</strong>: gibt es nicht<br />
<strong>Bonuspunkte</strong>: gibt es nicht<br />
<strong>Erlaubte Hilfsmittel</strong>: keine</p>

                        </div>
                        <div class="postmeta">Posted in
                            
                                <a href="//martin-thoma.com/category/deutschland/">german posts</a><!--TODO: Displayed category name should be upper case! -->
                             | Tags:
                            
                                
                                    <a href="//martin-thoma.com/tag/klausur/">Klausur</a>
                                
                            
                                , <a href="//martin-thoma.com/tag/machine-learning/">Machine Learning</a>
                                
                             by <a rel="author" class="vcard author" href="//martin-thoma.com/author/martin-thoma/"><span class="fn">Martin Thoma</span></a> on <span class="updated"><span class="value-title" title="2015-05-11 11:00:00 +0200">
                                May
                                11th
                                  ,
                                2015</span></span></div>

                            <div class="navigation clearfix">
                                <div class="alignleft">
                                
                                    &laquo; <a href="//martin-thoma.com/neuronale-netze-vorlesung/" rel="prev">Neuronale Netze - Klausur</a>
                                
                                </div>
                                <div class="alignright">
                                
                                    <a href="//martin-thoma.com/python-markov-chain-packages/" rel="next">Python Markov Chain Packages</a> &raquo;
                                
                                </div>
                            </div>

                        </article>
                        <div id="respond">
                            <h3>Leave a Reply</h3>
                                <!-- comment discuss code -->
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'martinthoma'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    <!-- comment discuss code -->

                        </div>
                    </div>
                </div>
            <div class="span-8 last">
                <div id="subscriptions">
<a href="//martin-thoma.com/feed/"><img src="//martin-thoma.com/css/images/rss.png" alt="Subscribe to RSS Feed" title="Subscribe to RSS Feed" width="72" height="47" /></a>		
<a href="https://twitter.com/#!/themoosemind" title="Follow me on Twitter!"><img src="//martin-thoma.com/css/images/twitter.png" title="Follow me on Twitter!" alt="Follow me on Twitter!"  width="76" height="47" /></a>
</div>

                <div id="sidebar">
                <!-- type: searchbox.html - TODO-->
<ul>
    <li id="search">
        <div class="searchlayout">
            <form method="get" id="searchform" action="http://google.com/cse" role="search">
                <input type="hidden" name="cx" value="017345337424948206369:qrnnnentkkk" />
                <input type="search" value="" name="q" id="s" placeholder="Search with Google"/>
                <input type="image" src="//martin-thoma.com/css/images/search.gif" style="border:0; vertical-align: top;" alt="search"/> 
            </form>
        </div>
    </li>
</ul>

                <div class="addthis_toolbox">   
    <div class="custom_images">
            <a href="http://twitter.com/share?url=//martin-thoma.com/machine-learning-2-course&amp;hashtags=klausur,machine-learning,&amp;via=themoosemind" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/twitter.png" width="32" height="32" alt="Twitter" /></a>
            <a href="http://del.icio.us/post?url=//martin-thoma.com/machine-learning-2-course&amp;title=Machine%20Learning%202" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/delicious.png" width="32" height="32" alt="Delicious" /></a>
            <a href="http://www.facebook.com/sharer.php?u=//martin-thoma.com/machine-learning-2-course" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/facebook.png" width="32" height="32" alt="Facebook" /></a>
            <a href="http://digg.com/submit?phase=2&amp;url=//martin-thoma.com/machine-learning-2-course&amp;title=Machine%20Learning%202" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/digg.png" width="32" height="32" alt="Digg" /></a>
            <a href="http://www.stumbleupon.com/submit?url=//martin-thoma.com/machine-learning-2-course&amp;title=Machine%20Learning%202" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/stumbleupon.png" width="32" height="32" alt="Stumbleupon" /></a>
            <a href="https://plusone.google.com/_/+1/confirm?hl=en&amp;url=//martin-thoma.com/machine-learning-2-course" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/gplus.png" width="32" height="32" alt="Google Plus" /></a>
            <a href="http://reddit.com/submit?url=//martin-thoma.com/machine-learning-2-course&amp;title=Machine%20Learning%202" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/reddit.png" width="32" height="32" alt="Reddit" /></a>
    </div>
</div>

                <ul>
                    <li id="categories-3" class="widget widget_categories">
                        <!-- type: categories -->
<h2 class="widgettitle">Categories</h2>
    <ul>
        <li class="cat-item cat-item-11"><a href="//martin-thoma.com/category/code/" title="Tipps for coding in different languages like Python oder C++.">Code</a></li>
        <li class="cat-item cat-item-21"><a href="//martin-thoma.com/category/web/" title="New emerging websites and technologies.">The Web</a></li>
        <li class="cat-item cat-item-31"><a href="//martin-thoma.com/category/cyberculture/" title="Lolcats, planking, Trollfaces, ...">Cyberculture</a></li>
        <li class="cat-item cat-item-3404"><a href="//martin-thoma.com/category/maths/" title="View all posts filed under Mathematics">Mathematics</a></li>
        <li class="cat-item cat-item-881"><a href="//martin-thoma.com/category/bits-and-bytes/" title="Sometimes posts don&#039;t fit in any category.">My bits and bytes</a></li>
        <li class="cat-item cat-item-41"><a href="//martin-thoma.com/category/deutschland/" title="[All Posts here are written in German about German topics] - Die Bahn, unsere Politik und Europa.">German posts</a></li>
	</ul>

                    </li>
                    <li id="tag_cloud-3" class="widget widget_tag_cloud">
                        <h2 class="widgettitle">Tags</h2>
                        <div class="tagcloud"><a style='font-size: 130.77%' href='//martin-thoma.com/tag/ai/' title='9 topics'>AI</a>
<a style='font-size: 102.65%' href='//martin-thoma.com/tag/algebra/' title='6 topics'>Algebra</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/assembly-language/' title='5 topics'>Assembly language</a>
<a style='font-size: 102.65%' href='//martin-thoma.com/tag/bash/' title='6 topics'>Bash</a>
<a style='font-size: 192.76%' href='//martin-thoma.com/tag/c/' title='22 topics'>C</a>
<a style='font-size: 138.07%' href='//martin-thoma.com/tag/cpp/' title='10 topics'>CPP</a>
<a style='font-size: 113.34%' href='//martin-thoma.com/tag/challenge/' title='7 topics'>Challenge</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/chrome/' title='5 topics'>Chrome</a>
<a style='font-size: 138.07%' href='//martin-thoma.com/tag/clip/' title='10 topics'>Clip</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/command-line/' title='5 topics'>Command Line</a>
<a style='font-size: 102.65%' href='//martin-thoma.com/tag/computer-science/' title='6 topics'>Computer science</a>
<a style='font-size: 122.60%' href='//martin-thoma.com/tag/digitaltechnik/' title='8 topics'>Digitaltechnik</a>
<a style='font-size: 102.65%' href='//martin-thoma.com/tag/flashgames/' title='6 topics'>Flashgames</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/geometry/' title='5 topics'>Geometry</a>
<a style='font-size: 130.77%' href='//martin-thoma.com/tag/google/' title='9 topics'>Google</a>
<a style='font-size: 122.60%' href='//martin-thoma.com/tag/google-code-jam/' title='8 topics'>Google Code Jam</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/html5/' title='5 topics'>HTML5</a>
<a style='font-size: 130.77%' href='//martin-thoma.com/tag/it-security/' title='9 topics'>IT-Security</a>
<a style='font-size: 226.92%' href='//martin-thoma.com/tag/java/' title='36 topics'>Java</a>
<a style='font-size: 102.65%' href='//martin-thoma.com/tag/javascript/' title='6 topics'>JavaScript</a>
<a style='font-size: 178.84%' href='//martin-thoma.com/tag/kit/' title='18 topics'>KIT</a>
<a style='font-size: 211.92%' href='//martin-thoma.com/tag/klausur/' title='29 topics'>Klausur</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/kogsys/' title='5 topics'>KogSys</a>
<a style='font-size: 201.63%' href='//martin-thoma.com/tag/latex/' title='25 topics'>LaTeX</a>
<a style='font-size: 178.84%' href='//martin-thoma.com/tag/linear-algebra/' title='18 topics'>Linear algebra</a>
<a style='font-size: 170.67%' href='//martin-thoma.com/tag/linux/' title='16 topics'>Linux</a>
<a style='font-size: 138.07%' href='//martin-thoma.com/tag/machine-learning/' title='10 topics'>Machine Learning</a>
<a style='font-size: 122.60%' href='//martin-thoma.com/tag/matrix/' title='8 topics'>Matrix</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/os/' title='5 topics'>OS</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/operating-systems/' title='5 topics'>Operating Systems</a>
<a style='font-size: 130.77%' href='//martin-thoma.com/tag/php/' title='9 topics'>PHP</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/physics/' title='5 topics'>Physics</a>
<a style='font-size: 239.24%' href='//martin-thoma.com/tag/programming/' title='43 topics'>Programming</a>
<a style='font-size: 130.77%' href='//martin-thoma.com/tag/project-euler/' title='9 topics'>Project Euler</a>
<a style='font-size: 270.00%' href='//martin-thoma.com/tag/python/' title='67 topics'>Python</a>
<a style='font-size: 102.65%' href='//martin-thoma.com/tag/review/' title='6 topics'>Review</a>
<a style='font-size: 130.77%' href='//martin-thoma.com/tag/swt-i/' title='9 topics'>SWT I</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/science/' title='5 topics'>Science</a>
<a style='font-size: 130.77%' href='//martin-thoma.com/tag/theoretical-computer-science/' title='9 topics'>Theoretical computer science</a>
<a style='font-size: 102.65%' href='//martin-thoma.com/tag/tikz/' title='6 topics'>Tikz</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/ubuntu/' title='5 topics'>Ubuntu</a>
<a style='font-size: 156.27%' href='//martin-thoma.com/tag/video/' title='13 topics'>Video</a>
<a style='font-size: 130.77%' href='//martin-thoma.com/tag/vimeo/' title='9 topics'>Vimeo</a>
<a style='font-size: 138.07%' href='//martin-thoma.com/tag/web-development/' title='10 topics'>Web Development</a>
<a style='font-size: 102.65%' href='//martin-thoma.com/tag/wikipedia/' title='6 topics'>Wikipedia</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/windows-7/' title='5 topics'>Windows 7</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/wolfram-alpha/' title='5 topics'>Wolfram|Alpha</a>
<a style='font-size: 144.69%' href='//martin-thoma.com/tag/youtube/' title='11 topics'>YouTube</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/advertising/' title='5 topics'>advertising</a>
<a style='font-size: 122.60%' href='//martin-thoma.com/tag/algorithms/' title='8 topics'>algorithms</a>
<a style='font-size: 113.34%' href='//martin-thoma.com/tag/analysis/' title='7 topics'>analysis</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/cheat-sheet/' title='5 topics'>cheat sheet</a>
<a style='font-size: 166.20%' href='//martin-thoma.com/tag/funny/' title='15 topics'>funny</a>
<a style='font-size: 166.20%' href='//martin-thoma.com/tag/learning/' title='15 topics'>learning</a>
<a style='font-size: 138.07%' href='//martin-thoma.com/tag/lecture-notes/' title='10 topics'>lecture-notes</a>
<a style='font-size: 253.74%' href='//martin-thoma.com/tag/mathematics/' title='53 topics'>mathematics</a>
<a style='font-size: 189.53%' href='//martin-thoma.com/tag/puzzle/' title='21 topics'>puzzle</a>
</div>
                    </li>
                </ul>
                </div>
            </div>
        </div><!--/container-->
            <footer id="footer">
                <a href="//martin-thoma.com"><strong>Martin Thoma</strong></a> -  A blog about Code, the Web and Cyberculture. <br />
                <div class="footer-credits">
                    <a href="http://flexithemes.com/themes/modern-style/">Modern Style</a> theme by <a href="http://flexithemes.com/">FlexiThemes</a>
                </div>
            </footer><!--/footer-->

    </div><!--/wrapper-->
<!-- type: footer -->
<!-- MathJax -->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">
<!--
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [ ['$$', '$$'], ['\\[','\\]']],
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    processEscapes: true
  }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
// -->
</script>
<!-- TOC Plus -->
<script type='text/javascript'>
/* <![CDATA[ */
var tocplus = {"visibility_show":"show","visibility_hide":"hide","width":"275px"};
/* ]]> */
</script>
<script type='text/javascript' src="//martin-thoma.com/js/tocplus-front.js"></script>

</body>
</html>

