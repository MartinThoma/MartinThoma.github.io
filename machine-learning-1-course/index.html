<!DOCTYPE html>
<html lang="en">
  <!-- type: head.html -->
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    
    

    
        <meta name="thumbnail" content="//martin-thoma.com/images/logos/klausur.png" />
        <meta property="og:image" content="//martin-thoma.com/images/logos/klausur.png" />
    

    <meta property="og:type" content="blog"/>

    <title>Machine Learning 1</title>
    <link rel="stylesheet" href="//martin-thoma.com/css/screen.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="//martin-thoma.com/css/style.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="//martin-thoma.com/css/pygments.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="//martin-thoma.com/css/tocplus-screen.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="//martin-thoma.com/css/print.css" type="text/css" media="print" />
    <link rel="stylesheet" href="//martin-thoma.com/css/handheld.css" type="text/css" media="only screen and (max-device-width: 480px)" />

    <link rel="alternate" type="application/rss+xml" title="Martin Thoma RSS Feed" href="//martin-thoma.com/feed/" /><!--TODO-->
    <link rel="shortcut icon" href="//martin-thoma.com/favicon.ico" type="image/x-icon" />

    <link rel="canonical" href="//martin-thoma.com/machine-learning-1-course" />
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:site" content="@themoosemind"/>
<meta name="twitter:creator" content="@themoosemind"/>
<meta name="twitter:title" content="Machine Learning 1"/>

    <meta name="twitter:description" content="A blog about Code, the Web and Cyberculture" />


    <meta name="twitter:image" content="//martin-thoma.com/images/logos/klausur.png"/>



<meta name="twitter:url" content="//martin-thoma.com/machine-learning-1-course"/>
<meta name="twitter:domain" content="Martin Thoma.com"/>


    <script type='text/javascript' src="//martin-thoma.com/js/jquery.js"></script>
    <script type='text/javascript' src="//martin-thoma.com/js/jquery-migrate.min.js"></script>
    <style type="text/css">div#toc_container {width: 275px;}</style>
    <style type="text/css" id="syntaxhighlighteranchor"></style>

<!-- Latest compiled and minified CSS bootstrap -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">
</head>

<!-- type: post.html -->
<body>
    <div id="wrapper">
        <div id="container" class="container">
            <div class="span-16">
                <!-- type: header.html -->
<div id="header" role="banner">
    <h1><a href="//martin-thoma.com">Martin Thoma</a></h1>
    <h2 style="margin-top: 0;">A blog about Code, the Web and Cyberculture.</h2>
</div>
<nav class="navcontainer" role="navigation">
    <ul id="nav">
        <li class=""><a href="//martin-thoma.com">Home</a></li>
        <li class="page_item page-item-41 "><a href="//martin-thoma.com/author/martin-thoma/">About Me</a></li>
        <li class="page_item page-item-91 "><a href="//martin-thoma.com/imprint/">Imprint</a></li>
    </ul>
</nav>

                <div id="content">
                    <article class="post type-post format-standard hentry clearfix ">
                        <h2 class="title entry-title">Machine Learning 1</h2>
                        <div class="postdate entry-date">
                            <time datetime="2015-11-09T16:02:00+01:00">
                                November
                                9th,
                                  
                                2015
                            </time>
                        </div>

                        <div class="entry">
                            <div class="info">Dieser Artikel beschäftigt sich mit der Vorlesung „Machine Learning 1“ am KIT. Er dient als Prüfungsvorbereitung. Ich habe die Vorlesungen bei <a href="http://www.fzi.de/wir-ueber-uns/organisation/mitarbeiter/address/39/?no_cache=1">Herrn Prof. Dr. Zöllner</a> im Wintersemester 2014/2015 gehört.<br />Es gibt auch einen Artikel über <a href="//martin-thoma.com/machine-learning-2-course/">Machine Learning 2</a></div>

<div id="toc_container" class="toc_light_blue no_bullets">
   <p class="toc_title">Contents</p>
   <ul class="toc_list">
      <li class="toc_level-1 toc_section-1">
         <a href="#tocAnchor-1-1"><span class="tocnumber">1</span> <span class="toctext">Folien</span></a>
         <ul>
            <li class="toc_level-2 toc_section-2">
               <a href="#tocAnchor-1-1-1"><span class="tocnumber">1.1</span> <span class="toctext">Einordnungskriterien</span></a>
            </li>
            <li class="toc_level-2 toc_section-3">
               <a href="#tocAnchor-1-1-2"><span class="tocnumber">1.2</span> <span class="toctext">Einführung</span></a>
            </li>
            <li class="toc_level-2 toc_section-4">
               <a href="#tocAnchor-1-1-3"><span class="tocnumber">1.3</span> <span class="toctext">Induktives Lernen</span></a>
            </li>
            <li class="toc_level-2 toc_section-5">
               <a href="#tocAnchor-1-1-4"><span class="tocnumber">1.4</span> <span class="toctext">Reinforcement Learning</span></a>
            </li>
            <li class="toc_level-2 toc_section-6">
               <a href="#tocAnchor-1-1-5"><span class="tocnumber">1.5</span> <span class="toctext">Lerntheorie</span></a>
            </li>
            <li class="toc_level-2 toc_section-7">
               <a href="#tocAnchor-1-1-6"><span class="tocnumber">1.6</span> <span class="toctext">Neuronale Netze</span></a>
            </li>
            <li class="toc_level-2 toc_section-8">
               <a href="#tocAnchor-1-1-7"><span class="tocnumber">1.7</span> <span class="toctext">Instanzbasiertes Lernen</span></a>
            </li>
            <li class="toc_level-2 toc_section-9">
               <a href="#tocAnchor-1-1-8"><span class="tocnumber">1.8</span> <span class="toctext">SVM</span></a>
            </li>
            <li class="toc_level-2 toc_section-10">
               <a href="#tocAnchor-1-1-9"><span class="tocnumber">1.9</span> <span class="toctext">Entscheidungsbäume</span></a>
            </li>
            <li class="toc_level-2 toc_section-11">
               <a href="#tocAnchor-1-1-10"><span class="tocnumber">1.10</span> <span class="toctext">Bayes Lernen</span></a>
            </li>
            <li class="toc_level-2 toc_section-12">
               <a href="#tocAnchor-1-1-11"><span class="tocnumber">1.11</span> <span class="toctext">HMM</span></a>
            </li>
            <li class="toc_level-2 toc_section-13">
               <a href="#tocAnchor-1-1-12"><span class="tocnumber">1.12</span> <span class="toctext">Markov Logik Netze</span></a>
            </li>
         </ul>
      </li>
      <li class="toc_level-1 toc_section-14">
         <a href="#tocAnchor-1-14"><span class="tocnumber">2</span> <span class="toctext">Prüfungsfragen</span></a>
      </li>
      <li class="toc_level-1 toc_section-15">
         <a href="#tocAnchor-1-15"><span class="tocnumber">3</span> <span class="toctext">Material und Links</span></a>
      </li>
      <li class="toc_level-1 toc_section-16">
         <a href="#tocAnchor-1-16"><span class="tocnumber">4</span> <span class="toctext">Übungsbetrieb</span></a>
      </li>
      <li class="toc_level-1 toc_section-17">
         <a href="#tocAnchor-1-17"><span class="tocnumber">5</span> <span class="toctext">Termine und Klausurablauf</span></a>
      </li>
   </ul>
</div><h2 id="tocAnchor-1-1">Folien</h2>

<h3 id="tocAnchor-1-1-1">Einordnungskriterien</h3>

<p>Slide name: <code>ML-Einordnungskriterien.pdf</code></p>

<ul>
<li><strong>Inferenztyp</strong>: Induktiv (Version Space Algorithmus, <abbr title="k nearest neighbor">\(k\)-NN</abbr>, <abbr title="Case-Based">CBR</abbr>, ID3, ID5R, von Beispielen auf allgemeine Regel "raten") ↔ Deduktiv (Erklärungsbasierte Generalisierung; Von allgemeinen auf spezielles)</li>
<li><strong>Lernebene</strong>: symbolisch (Special-to-General Konzeptlernen, CBR, ID3, ID5R; Semantik in Daten von der der Algorithmus Gebrauch macht) ↔ subsymbolisch (Neuronale Netze, k-NN; Daten sind Signale)</li>
<li><strong>Lernvorgang</strong>: überwacht (k-NN, CBR, ID3, ID5R) ↔ unüberwacht (k-Means)</li>
<li><strong>Beispielgebung</strong>: inkrementell (Version Space Algorithmus, CBR, ID5R) ↔ nicht inkrementell (\(k\)-Means, \(k\)-NN, ID3)</li>
<li><strong>Beispielumfang</strong>: umfangreich (Neuronale Netze, k-NN, ID3, ID5R) ↔ gering (CBR)</li>
<li><strong>Hintergrundwissen</strong>: empirisch (SVMs, k-NN, CBR, ID3, ID5R) ↔ axiomatisch (Erklärungsbasierte Generalisierung)</li>
</ul>

<h3 id="tocAnchor-1-1-2">Einführung</h3>

<p>Slide name: <code>MLI_01_Einfuehrung_slides1.pdf</code></p>

<ul>
<li>Was ist Intelligenz? (Problemlösen, Erinnern, Sprache, Kreativität,
Bewusstsein, Überleben in komplexen Welten, )</li>
<li>Wissensrepräsentation:

<ul>
<li>Assoziierte Paare (Eingangs- und Ausgangsvariablen)</li>
<li>Entscheidungsbäume (Klassen diskriminieren)</li>
<li>Parameter in algebraischen ausdrücken</li>
<li>Formale Grammatiken</li>
<li>Logikbasierte Ausdrücke</li>
<li>Taxonomien</li>
<li>Semantische Netze</li>
<li>Markov-Ketten</li>
</ul></li>
</ul>

<dl>
  <dt><dfn>Machine Learning</dfn> von Tom Mitchell</dt>
  <dd>A computer program is said to learn from experience E with respect to
      some class of tasks T and performance measure P, if its performance at
      tasks in T, as measured by P, improves with experience E.</dd>
  <dt><dfn>Deduktion</dfn></dt>
  <dd>Die Deduktion ist eine Schlussfolgerung von gegebenen Prämissen auf die
      logisch zwingenden Konsequenzen. Deduktion ist schon bei Aristoteles als
      „Schluss vom Allgemeinen auf das Besondere“ verstanden worden.</dd>
  <dt><dfn>Modus ponens</dfn></dt>
  <dd>Der Modus ponens ist eine Art des logischen Schließens. Er besagt: Wenn
      die Prämissen \(A \rightarrow B\) und \(A\) gelten, dann gilt auch \(B\).</dd>
  <dt><dfn>Abduktion</dfn> by Peirce</dt>
  <dd>Deduction proves that something must be; Induction shows that something
      actually is operative; Abduction merely suggests that something may
      be.</dd>
</dl>

<h3 id="tocAnchor-1-1-3">Induktives Lernen</h3>

<p>Slide name: <code>MLI_02_InduktivesLernen_slides1.pdf</code></p>

<ul>
<li>Konzept: Beschreibt Untermenge von Objekten oder Ereignissen definiert auf
größerer Menge.</li>
<li>Konsistenz: Keine negativen Beispiele werden positiv klassifiziert.</li>
<li>Vollständigkeit: Alle positiven Beispiele werden als positiv klassifiziert.</li>
<li>Algorithmen: Bäume (Wälder?)

<ul>
<li>Suche vom Allgemeinen zum Speziellen: Negative Beispiele führen zur Spezialisierung</li>
<li>Suche vom Speziellen zum Allgemeinen: Positive Beispiele führen zur Verallgemeinerung</li>
<li><a href="https://de.wikipedia.org/wiki/Versionsraum">Version Space</a>: Beides gleichzeitig anwenden</li>
</ul></li>
<li>Präzendenzgraphen: In welcher Reihenfolge werden Aktionen ausgeführt?</li>
</ul>

<p>Version Space Algorithmus ist:</p>

<ul>
<li>Induktiver Inferenztyp</li>
<li>Symbolische Ebene des Lernens</li>
<li>Überwachtes Lernen</li>
<li>Inkrementelle Beispielgebung</li>
<li>Umfangreich (viele Beispiele)</li>
<li>Empirisches Hintergrundwissen</li>
<li>Voraussetzungen: Konsistente Beispiele, korrekte Hypothese im Hypothesenraum</li>
<li>Positive Aspekte:

<ul>
<li>Es ist feststellbar, welche Art von Beispielen noch nötig ist</li>
<li>Es ist feststellbar, wann das Lernen abgeschlossen ist</li>
</ul></li>
</ul>

<p>Weiteres</p>

<dl>
  <dt><dfn>Inductive bias</dfn></dt>
  <dd>Induktives Lernen benötigt Vorannahmen.</dd>
  <dt><dfn>Bias</dfn> ("Vorzugskriterium")</dt>
  <dd>Vorschrift, nach der Hypothese gebildet werden.</dd>
</dl>

<h3 id="tocAnchor-1-1-4">Reinforcement Learning</h3>

<p>Slide name: <code>MLI_03_ReinforcementLearning_slides1.pdf</code></p>

<dl>
  <dt><a href="https://en.wikipedia.org/wiki/Reinforcement_learning"><dfn>Reinforcement Learning</dfn></a> (<dfn>RL</dfn>, <dfn><a href="https://de.wikipedia.org/wiki/Best%C3%A4rkendes_Lernen">Bestärkendes Lernen</a></dfn>)</dt>
  <dd>Beim bestärkenden Lernen ist man in einem
      <a href="https://de.wikipedia.org/wiki/Markow-Entscheidungsproblem">Markow-Entscheidungsproblemen</a>.
      Es gibt also einen Agenten, der Aktionen ausführen kann. Diese können
      (nicht notwendigerweise sofort) bewertet werden.</dd>
</dl>

<ul>
<li>Beispiel für RL: Roboter muss zu einem Ziel navigieren</li>
</ul>

<p>Algorithmen:</p>

<ul>
<li>Policy Learning</li>
<li>Simple Value Iteration</li>
<li>Simple Temporal Difference Learning</li>
<li><a href="https://de.wikipedia.org/wiki/Temporal_Difference_Learning">TD-Learning</a> (Temporal Difference Learning): TODO - wo genau ist der Unterschied zum Q-Learning?</li>
</ul>

<h4 id="q-learning">Q-Learning</h4>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Q-learning">Q-learning</a>

<ul>
<li><a href="https://www.youtube.com/watch?v=yS5F_vm9Ahk">YouTube: Lecture 18: RL Part 1: Q-Learning</a>: 1:16:11. BrownCS141 Spring 2014.</li>
<li><a href="https://www.youtube.com/watch?v=3sLV0OJLdns">YouTube: PacMan</a></li>
</ul></li>
</ul>

<p>Pseudocode:</p>
<div class="highlight">
   <pre><code class="language-text" data-lang="text">initialize Q[num_states, num_actions]
start in state s
repeat:
    select and execute action a
    observe reward r and new state s'
    Q[s', a] &lt;- Q[s, a] + \alpha (r + \gamma \max_{a'} Q[s', a'] - Q[s, a])
    s &lt;- s'
</code></pre>
</div>
<p>where (\alpha \in (0, 1]) is a learning rate and (\gamma) is a discount
factor.</p>

<h4 id="siehe-auch">Siehe auch</h4>

<ul>
<li><a href="https://de.wikipedia.org/wiki/Optimalit%C3%A4tsprinzip_von_Bellman">Optimalitätsprinzip von Bellman</a></li>
<li>Guest Post (Part I): <a href="http://www.nervanasys.com/demystifying-deep-reinforcement-learning/">Demystifying Deep Reinforcement Learning</a></li>
</ul>

<h3 id="tocAnchor-1-1-5">Lerntheorie</h3>

<p>Slide name: <code>MLI_04_Lerntheorie_slides1.pdf</code></p>

<dl>
  <dt><dfn>Ockhams Rasiermesser</dfn> (Quelle: <a href="https://de.wikipedia.org/wiki/Ockhams_Rasiermesser">Wikipedia</a>)</dt>
  <dd>Von mehreren möglichen Erklärungen für ein und denselben Sachverhalt ist
  die einfachste Theorie allen anderen vorzuziehen. Eine Theorie ist einfach,
  wenn sie möglichst wenige Variablen und Hypothesen enthält, und wenn diese in
  klaren logischen Beziehungen zueinander stehen, aus denen der zu erklärende
  Sachverhalt logisch folgt.</dd>
  <dt><a href="https://en.wikipedia.org/wiki/Structural_risk_minimization"><dfn>Structural Risc Minimization</dfn></a> (<dfn>SRM</dfn>)</dt>
  <dd>Unter <i>Structural risk minimization</i> versteht man die Abwägung
      zwischen einem einfachen Modell und einem komplexen Modell, welches
      auf den Trainingsdaten besser funktioniert aber eventuell mehr unter
      Overfitting leidet.</dd>
  <dt><dfn>Vapnik-Chervonenkis Dimension</dfn> (<dfn>VC-Dimension</dfn>)</dt>
  <dd>Die <abbr title="Vapnik-Chervonenkis">VC</abbr>-Dimension \(VC(H^\alpha)\) eines Hypothesenraumes \(H^\alpha\)
      ist gleich der maximalen Anzahl an Datenpunkten, die von \(H^\alpha\)
      beliebig separiert werden können.</dd>
</dl>

<ul>
<li>Lernmaschine wird definiert durch Hypothesenraum \({h_\alpha: \alpha \in A}\)
und Lernverfahren. Das Lernverfahren ist die Methode um \(\alpha_{\text{opt}}\)
mit Hilfe von Lernbeispielen zu finden.</li>
<li>Probleme beim Lernen:

<ul>
<li>Größe des Hypothesenraums im Vergleich zur Anzahl der Trainingsdaten.</li>
<li>Das Verfahren könnte nur suboptimale Lösungen finden.</li>
<li>Das Verfahren könnte die passende Hypothese nicht beinhalten.</li>
</ul></li>
<li>Lernproblemtypen: Sei die Menge der Lernbeispiele in \(X \times Y\), mit \(X \times Y =\)...

<ul>
<li>\(\{Attribut_1, Attribut_2, ...\} \times \{True, False\}\): Konzeptlernen</li>
<li>\(\mathbb{R}^n \times \{Klasse_1, ..., Klasse_n\}\): Klassifikation</li>
<li>\(\mathbb{R}^n \times \mathbb{R}\): Regression</li>
</ul></li>
<li>Gradientenabstieg, Overfitting</li>
<li>Kreuzvalidierung</li>
<li><a href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning">Probably approximately correct learning</a>

<ul>
<li>Folie 35: Was ist eine Instanz der Länge \(n\)? (TODO)</li>
</ul></li>
<li>VC-Dimension: Siehe <a href="https://youtu.be/puDzy2XmR5c">YouTube</a>

<ul>
<li>Folie 44: Was ist \(\eta\)? (TODO)</li>
</ul></li>
<li>TODO: Wie hängen PAC und VC-Dimension zusammen?</li>
</ul>

<h4 id="boosting">Boosting</h4>

<dl>
  <dt><a href="https://de.wikipedia.org/wiki/Boosting"><dfn>Boosting</dfn></a></dt>
  <dd>Kombiniere mehrere schwache Modelle um ein gutes zu bekommen, indem
      Trainingsbeispiele unterschiedlich gewichtet werden.</dd>
  <dt><a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating"><dfn>Bagging</dfn></a> (<dfn>Bootstrap aggregating</dfn>)</dt>
  <dd>Kombiniere mehrere schwache Modelle um ein gutes zu bekommen. Dabei
      bekommt jedes schwache Modell nur eine Teilmenge aller Trainingsdaten.</dd>
  <dt><dfn>AdaBoost</dfn> (<dfn>Adaptive Boosting</dfn>)</dt>
  <dd>Learn a classifier for data. Get examples where the classifier got it
      wrong. Train new classifier on the wrong ones.</dd>
</dl>

<ul>
<li>Folie 22:

<ul>
<li>Wofür steht \(i\) und welchen Wertebereich hat \(i\)? (TODO)</li>
<li>Stellt \(W_k(i)\) die Wahrscheinlichkeit dar, dass Beispiel \(i\) im \(k\)-ten
Durchlauf für das Training verwendet wird? (TODO)</li>
</ul></li>
</ul>

<p>Siehe auch:</p>

<ul>
<li>Alexander Ihler: <a href="https://www.youtube.com/watch?v=ix6IvwbVpw0">AdaBoost</a>.</li>
</ul>

<div style="width: 510px" class="wp-caption aligncenter">
   <a href="../images/2015/12/ml-ensemble-learning.png">
      <img src="//martin-thoma.com/captions/ml-ensemble-learning.png" alt="Ensemble Learning Techniques: Boosting, Bagging, Random Subspaces, Pasting, Random Patches" width="500" height="478" class="" />
   </a>
   <p class="wp-caption-text">Ensemble Learning Techniques: Boosting, Bagging, Random Subspaces, Pasting, Random Patches</p>
</div>

<h4 id="vc-dimension">VC-Dimension</h4>

<dl>
  <dt>VC-Dimension</dt>
  <dd>Sei \(H^\alpha = \{h_\alpha : \alpha \in A\}\) der Hypothesenraum. Die
      VC-Dimension \(VC(h_\alpha)\) von \(H^\alpha\) ist gleich der maximalen
      Anzahl von beliebig platzierten Datenpunkten, die von \(H^\alpha\) separiert
      werden können.</dd>
</dl>

<ul>
<li>TODO - Folie 39: Was ist \(A\)? Warum ist \(h_\alpha\) wichtig? Sollte es
nicht eher \(VC(H^\alpha)\) sein?</li>
</ul>

<h3 id="tocAnchor-1-1-6">Neuronale Netze</h3>

<p>Slide name: <code>MLI_05_Neuronale_Netze_slides1.pdf</code></p>

<ul>
<li>Einsatzfelder:

<ul>
<li>Klassifiktion: Spracherkennung, Schrifterkennung</li>
<li>Funktionsapproximation</li>
<li>Mustervervollständigung: Kodierung, Bilderkennung (TODO: Warum zählt das nicht zu Klassifikation?)</li>
</ul></li>
<li>Perzeptron von Rosenblatt (1960)

<ul>
<li>Auswertung: Input-Vektor und Bias mit Gewichten multiplizieren, addieren und Aktivierungsfunktion anwenden.</li>
<li>Training: Zufällige Initialisierung des Gewichtsvektors, addieren von fehlklassifizierten Vektoren auf Gewichtsvektor.</li>
</ul></li>
<li>Gradientenabstieg</li>
<li>Kernel-Methoden (TODO)</li>
<li>Radial-Basis Funktion Netz (TODO)</li>
</ul>

<dl>
    <dt><dfn>Cascade Correlation</dfn></dt>
    <dd>Cascade Correlation ist ein konstruktiver Algorithmus zum erzeugen
        von Feed-Forward Neuronalen Netzen. Diese haben eine andere Architektur
        als typische multilayer Perceptrons. Bei Netzen, welche durch
        Cascade Correlation aufgebaut werden, ist jede Hidden Unit mit
        den Input-Neuronen verbunden, mit den Output-Neuronen und mit allen
        Hidden Units in der Schicht zuvor.</dd>
    <dt><abbr title="Resilient Propagation"><dfn>RPROP</dfn></abbr></dt>
    <dd>TODO</dd>
    <dt><a href="https://en.wikipedia.org/wiki/Delta_rule"><dfn>Delta-Regel</dfn></a>, siehe <a href="http://www.neuronalesnetz.de/delta.html">neuronalesnetz.de</a></dt>
    <dd>Die Delta-Regel ist ein Lernalgorithmus für neuronale Netze mit nur
        einer Schicht. Sie ist ein Spezialfall des algemeineren
        Backpropagation-Algorithmus und lautet wie folgt:
        \[\Delta w_{ji} = \alpha (t_j - y_j) \varphi'(h_j) x_i\]
        wobei

        <ul>
        <li>\(\Delta w_{ji} \in \mathbb{R}\) die Änderung des Gewichts von Input \(i\)
        zum Neuron \(j\),</li>
        <li>\(\alpha \in [0, 1]\) die Lernrate (typischerweise \(\alpha \approx 0.1\)),</li>
        <li>\(t_j \in \mathbb{R}\) der Zielwert des Neurons \(j\),</li>
        <li>\(y_j \in \mathbb{R}\) die tatsächliche Ausgabe,</li>
        <li>\(\varphi'\) die Ableitung der Aktivierungsfunktion des Neurons,</li>
        <li>\(h_j \in \mathbb{R}\) die gewichtete Summe der Eingaben des Neurons und</li>
        <li>\(x_i \in \mathbb{R}\) der \(i\)-te Input</li>
        </ul>

        ist.
    </dd>
    <dt><dfn>Gradient-Descent Algorithmus</dfn></dt>
    <dd>Der Gradient-Descent Algorithmus ist ein Optimierungsalgorithmus für
        differenzierbare Funktionen. Er startet an einer zufälligen Stelle \(x_0\).
        Dann wird folgender Schritt mehrfach ausgeführt:
        \[x_0 \gets x_0 - \alpha \cdot \text(grad) f (x_0)\]
        wobei \(\alpha \in (0, 1]\) die Lernrate ist und \(f\) die zu
        optimierende Funktion. Dabei könnte \(\alpha\) mit der Zeit auch
        kleiner gemacht werden.
    </dd>
    <dt><dfn>Backpropagation</dfn> (siehe <a href="http://neuralnetworksanddeeplearning.com/chap2.html">neuralnetworksanddeeplearning.com</a>)</dt>
    <dd>Der Backpropagation-Algorithmus ist eine Variante des Gradient-Descent
        Algorithmus, welche für <abbr title="multilayer Perceptrons">MLPs</abbr>
        angepasst wurde. Sie besteht aus drei Schritten:

        <ul>
            <li><b>Forward-Pass</b>: Lege die Input-Features an das Netz an und erhalte den Output</li>
            <li><b>Fehlerberechnung</b>: Mache das für alle Daten</li>
            <li><b>Backward-Pass</b>: Passe die Gewichte </li>
        </ul>

        Im Grunde ist Backpropagation nur eine Geschwindigkeitsoptimierte
        Variante des Gradient-Descent Algorithmus, da die Gradienten im
        Backpropagation-Algorithmus auf geschickte Weise berechnet werden.</dd>
</dl>

<h4 id="siehe-auch">Siehe auch</h4>

<ul>
<li><a href="//martin-thoma.com/neuronale-netze-vorlesung/">Neuronale Netze - Vorlesung</a></li>
<li><a href="http://datascience.stackexchange.com/q/9672/8820">How exactly does adding a new unit work in Cascade Correlation?</a></li>
</ul>

<h3 id="tocAnchor-1-1-7">Instanzbasiertes Lernen</h3>

<p>Slide name: <code>MLI_06_InstanzbasiertesLernen_slides1.pdf</code></p>

<dl>
  <dt><dfn>Instanzenbasiertes Lernen</dfn> bzw. <dfn>Lazy Learning</dfn></dt>
  <dd><i>Instanzenbasiertes Lernen</i> ist ein Lernverfahren, welches einfach nur
      die Beispiele abspeichert, also faul (engl. lazy) ist. Soll der Lerner
      neue Daten klassifizieren, so wird die Klasse des ähnlichsten
      Datensatzes gewählt.</dd>
  <dt><dfn>Case-based Reasoning</dfn> bzw. kurz <dfn>CBR</dfn></dt>
  <dd><i>CBR</i> ist ein allgemeines, abstraktes Framework und kein direkt anwendbarer
      Algorithmus. Die Idee ist, dass nach ähnlichen, bekannten Fällen gesucht
      wird, auf die der aktuelle Fall übertragen werden kann.</dd>
  <dt><dfn>Fall</dfn> im Kontext des CBR</dt>
  <dd>Ein Fall ist eine Abstraktion eines Ereignisses, die in Zeit und Raum
      begrenzt ist. Ein Fall enthält eine Problembeschreibung, eine Lösung und
      ein Ergebnis. Zusätzlich kann ein Fall eine Erklärung enthalten warum
      das Ergebnis auftrat, Informationen über die Lösungsmethode, Verweise
      auf andere Fälle oder Güteinformationen enthalten.</dd>
</dl>

<ul>
<li><p>Beispiel für Lazy Learning: <abbr title="k Nearest Neighbors">(k)-NN</abbr>,
<abbr title="Case-based Reasoning">CBR</abbr></p></li>
<li><p>TODO: Folie 3: „Fleißige“ Lernalgorithmen mit dem gleichen Hypothesenraum sind
eingeschränkter - was ist damit gemeint?</p></li>
</ul>

<h3 id="tocAnchor-1-1-8">SVM</h3>

<p>Slide name: <code>MLI_07_SVM_slides1.pdf</code></p>

<p>Eine Erklärung von <abbr title="Support Vector Machines">SVMs</abbr>
findet sich im Artikel <a href="//martin-thoma.com/svm-with-sklearn/">Using SVMs with sklearn</a>.</p>

<ul>
<li>SVMs sind laut Vapnik die Lernmaschine mit der kleinsten möglichen VC-
Dimension, falls die Klassen linear trennbar sind.</li>
<li>Primäres Optimierungsproblem: Finde einen Sattelpunkt der Funktion<br />
\(L_P = L(\vec{w}, b, \vec{\alpha}) = \frac{1}{2}|\vec{w}|^2 - \sum_{i=1}^N \alpha_i (y_i(\vec{w}\vec{x_i}+b)-1)\)
wobei \(\alpha_1, \dots, \alpha_N \geq 0\) Lagrange-Multiplikatoren sind</li>
<li>Soft Margin Hyperebene</li>
<li>Der Parameter $C$ dient der Regularisierung. Ist $C$ groß gibt es wenige
Missklassifikationen in der Trainingsdatenmenge. Ist $C$ klein, werden die
Margins größer.</li>
<li>Nichtlineare Kernelmethoden</li>
<li>Kernel-Trick</li>
</ul>

<div class="alert alert-info"><h4>Abschätzung des realen Fehlers</h4>
Der reale Fehler kann durch den empirischen Fehler und die VC-Dimension wie
folgt abgeschätzt werden:

Mit Wahrscheinlichkeit \(P(1-\eta)\) gilt:
\[E(h_\alpha) \leq E_{emp}(h_\alpha) + \sqrt{\dots \frac{VC(h_\alpha)}{N} \dots}\]

wobei gilt:

<ul>
    <li>\(E(h_\alpha)\) ist der reale Fehler der mit der Hypothese \(h_\alpha\)
        gemacht wird</li>
    <li>\(E_{emp}(h_\alpha)\) ist der empirische Fehler der mit der Hypothese \(h_\alpha\)
        gemacht wird</li>
    <li>\(VC(h_\alpha)\) ist die VC-Dimension der Lernmaschine</li>
    <li>\(N\) ist die Anzahl der Lernbeispiele</li>
</ul>

Dieser Term wird in der <i>Structural Risc Minimization</i> minimiert.
</div>

<div class="alert alert-info"><h4>Die fünf Prinzipien von SVMs</h4>
<ol>
    <li>Lineare Trennung mit maximalen Abstand der Trennebenen zu den
        nächstgelegenen Stichproben (Support Vektoren)</li>
    <li>Duale Formulierung des linearen Klassifikators.
        (vgl. <a href="https://de.wikipedia.org/wiki/Support_Vector_Machine#Duales_Problem">Wiki</a>, \(k(m) = w^T m + b = \langle w, m \rangle + b = \sum_{j=1}^N \alpha_j z_j \langle m_j, m \rangle + b\))</li>
    <li>Nichtlineare Abbildung der primären Merkmale in einen hochdimensionalen
        Merkmalsraum \(\Phi\)</li>
    <li>Implizite Nutzung des unter Umständen \(\infty\)-dimensionalen
        Eigenfunktionsraumes einer sog. Kernfunktion \(K\) als transformierten
        Merkmalsraum \(\Phi\). Dabei müssen die transformierten Merkmale nicht
        explizit berechnet werden und der Klassifikator hat trotz der hohen
        Dimension von \(\Phi\) nur eine niedrige Zahl von freien Parametern
        (Kernel-Trick).</li>
    <li>Relaxation der Forderung nach linearer Trennbarkeit durch Einführung
        von Schlupfvariablen (slack variables).</li>
</ol>
</div>

<h3 id="tocAnchor-1-1-9">Entscheidungsbäume</h3>

<p>Slide name: <code>MLI_08_Entscheidungsbaeume_slides1.pdf</code></p>

<dl>
  <dt><dfn>Entscheidungsbaum</dfn></dt>
  <dd>Ein Entscheidungsbaum ist ein Klassifikator in Baumstruktur. Die
      inneren Knoten des Entscheidungsbaumes sind Attributtests, die Blätter
      sind Klassen.</dd>
  <dt><a href="https://de.wikipedia.org/wiki/ID3"><dfn>ID3</dfn></a> (siehe <a href="(https://github.com/MartinThoma/LaTeX-examples/tree/master/source-code/Pseudocode/ID3">pseudocode</a>)</dt>
  <dd>ID3 ist ein Top-Bottom Verfahren zum Aufbau eines Entscheidungsbaumes.</dd>
  <dt><a href="https://de.wikipedia.org/wiki/C4.5"><dfn>C4.5</dfn></a> (siehe <a href="(https://github.com/MartinThoma/LaTeX-examples/tree/master/source-code/Pseudocode/ID3">pseudocode</a>)</dt>
  <dd>ID3 ist ein Top-Bottom Verfahren zum Aufbau eines Entscheidungsbaumes, welches auf ID3 basiert.</dd>
  <dt><dfn>Random Forest</dfn>, Quelle: <a href="https://de.wikipedia.org/wiki/Random_Forest">Wikipedia</a></dt>
  <dd>Ein Random Forest ist ein Klassifikationsverfahren, welches aus mehreren
  verschiedenen, unkorrelierten Entscheidungsbäumen besteht. Alle
  Entscheidungsbäume sind unter einer bestimmten Art von Randomisierung während
  des Lernprozesses gewachsen. Für eine Klassifikation darf jeder Baum in
  diesem Wald eine Entscheidung treffen und die Klasse mit den meisten Stimmen
  entscheidet die endgültige Klassifikation.</dd>
</dl>

<ul>
<li>Der Algorithmus ID5R dienen dem Aufbau eines Entscheidungsbaumes.</li>
<li>C4.5 unterstützt - im Gegensatz zu ID3 - kontinuierliche Attributwerte.
Außerdem kann C4.5 mit fehlenden Attributwerten umgehen.</li>
<li>Mögliches Qualtitätsmaß ist Entropie:<br />
\(Entropie(S) = - p_\oplus \log_2 p_\oplus - p_\ominus \log_2 p_\ominus\)
wobei $\oplus$ die positiven Beispiele und $\ominus$ die negativen Beispiele
bezeichnet.</li>
<li>TODO, Folie 41: Wo ist der Vorteil von ID5R im Vergleich zu ID3, wenn das
Ergebnis äquivalent ist?</li>
<li>Random Forest: Erstelle mehrere Entscheidungsbäume mit einer zufälligen
Wahl an Attributen. Jeder Baum stimmt für eine Klasse und die Klasse, für die
die meisten Stimmen, wird gewählt.</li>
</ul>

<h3 id="tocAnchor-1-1-10">Bayes Lernen</h3>

<p>Slide name: <code>MLI_09_BayesLernen_slides1.pdf</code></p>

<dl>
  <dt><dfn>Satz von Bayes</dfn></dt>
  <dd>Seien \(A, B\) Ereignisse, \(P(B) &gt; 0\). Dann gilt:
      \(P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}\)<br />
      Dabei wird \(P(A)\) a priori Wahrscheinlichkeit, \(P(B|A)\) likelihood,
      und \(P(A|B)\) a posteriori Wahrscheinlichkeit genannt.</dd>
  <dt><dfn>Naiver Bayes-Klassifikator</dfn></dt>
  <dd>Ein Klassifizierer heißt naiver Bayes-Klassifikator, wenn er den
      Satz von Bayes unter der naiven Annahme der Unabhängigkeit der Features
      benutzt.</dd>
  <dt><dfn>Produktregel</dfn></dt>
  <dd>\(P(A \land B) = P(A|B) \cdot P(B) = P(B|A) \cdot P(A)\)</dd>
  <dt><dfn>Summenregel</dfn></dt>
  <dd>\(P(A \lor B) = P(A) + P(B) - P(A \land P)\)</dd>
  <dt><dfn>Theorem der totalen Wahrscheinlichkeit</dfn></dt>
  <dd>Es seien \(A_1, \dots, A_n\) Ereignisse mit \(i \neq j \Rightarrow A_i \cap A_j = \emptyset \;\;\;\forall i, j \in 1, \dots, n\) und \(\sum_{i=1}^n A_i = 1\). Dann gilt:<br />
      \(P(B) = \sum_{i=1}^n P(B|A_i) P(A_i)\)</dd>
  <dt><dfn>Maximum A Posteriori Hypothese</dfn> (MAP-Hypothese)</dt>
  <dd>Sei \(H\) der Raum aller Hypothesen und \(D\) die Menge der beobachteten
      Daten. Dann heißt<br />
      \(h_{MAP} = \text{arg max}_{h \in H} P(h|D) \cdot P(h)\)<br />
      die Menge der Maximum A Posteriori Hypothesen.</dd>
  <dt><dfn>Maximum Likelihood Hypothese</dfn> (ML-Hypothese)</dt>
  <dd>Sei \(H\) der Raum aller Hypothesen und \(D\) die Menge der beobachteten
      Daten. Dann heißt<br />
      \(h_{ML} = \text{arg max}_{h \in H} P(h|D)\)<br />
      die Menge der Maximum Likelihood Hypothesen.</dd>
  <dt><dfn>Normalverteilung</dfn></dt>
  <dd>Eine stetige Zufallsvariable \(X\) mit der Wahrscheinlichkeitsdichte
      \(f\colon\mathbb{R}\to\mathbb{R}\), gegeben durch<br />
      \(f(x) = \frac {1}{\sigma\sqrt{2\pi}} e^{-\frac {1}{2} \left(\frac{x-\mu}{\sigma}\right)^2}\)<br />
      heißt \(\mathcal N\left(\mu, \sigma^2\right)\)-verteilt, normalverteilt
      mit den Erwartungswert \(\mu\) und Varianz \(\sigma^2\).</dd>
  <dt><a href="https://de.wikipedia.org/wiki/Minimum_Description_Length"><dfn>Prinzip der minimalen Beschreibungslänge</dfn></a></dt>
  <dd>Das Prinzip der minimalen Beschreibungslänge ist eine formale
      Beschreibung von Ockhams Rasiermesser. Nach diesem Prinzip werden
      Hypothesen bevorzugt, die zur besten Kompression gegebener Daten führen.
  </dd>
  <dt><a href="https://de.wikipedia.org/wiki/Gibbs-Sampling"><dfn>Gibbs-Algorithmus</dfn></a> (<a href="http://stats.stackexchange.com/a/10216/25741">stats.stackexchange</a>)</dt>
  <dd>Der Algorithmus von Gibbs ist eine Methode um Stichproben von bedingten
      Verteilungen zu erzeugen. TODO: Ist das richtig? Wann wird es verwendet?
  </dd>
  <dt><a href="https://de.wikipedia.org/wiki/Bedingte_Unabh%C3%A4ngigkeit"><dfn>Bedingte Unabhängigkeit</dfn></a></dt>
  <dd>Seien \(X, Y, Z\) Zufallsvariablen. Dann heißt \(X\) bedingt unabhängig
      von \(Y\) gegeben \(Z\), wenn \[P(X|Y,Z) = P(X|Z)\] gilt.
  </dd>
  <dt><a href="https://en.wikipedia.org/wiki/Additive_smoothing"><dfn>Add \(k\) smoothing</dfn></a></dt>
  <dd>Unter Add-\(k\)-smoothing versteht man eine Technik, durch die
      sichergestellt wird, dass die geschätzte Wahrscheinlichkeit für kein
      Ereignis gleich null ist. Wenn man \(d \in \mathbb{N}\) mögliche
      Ergebnisse eines Experiments hat, \(N \in \mathbb{N}\) experimente
      durchgeführt werden, dann schätzt man die Wahrscheinlichkeit von dem
      Ergebnis \(i\) mit
      \[\hat{\theta_i} = \frac{x_i + k}{N+ kd}, \]
      wobei \(x_i\) die Anzahl der Beobachtungen von \(i\) ist und \(k \geq 0\)
      der Glättungsparameter ist.
  </dd>
  <dt><a href="https://de.wikipedia.org/wiki/Bayessches_Netz"><dfn>Bayessches Netz</dfn></a></dt>
  <dd>Ein bayessches Netz ist ein gerichteter azyklischer Graph in dem die
      Knoten Zufallsvariablen und die Kanten bedingte Abhängigkeiten
      beschreiben.
  </dd>
</dl>

<p>Fragen:</p>

<ul>
<li>Folie 23: Warum ist \(h_{MAP(x)}\) nicht die wahrscheinlichste
Klassifikation?</li>
<li>Folie 24: Was ist \(V\)?</li>
</ul>

<h3 id="tocAnchor-1-1-11">HMM</h3>

<p>Slide name: <code>MLI_10_HMM_slides1.pdf</code></p>

<dl>
  <dt><dfn>Markov-Bedingung</dfn> (Beschränkter Horizont)</dt>
  <dd>\(P(q_{t+1}=S_{t+1}|q_t = S_t, q_{t-1} = S_{t-1}, \dots) = P(q_{t+1}=S_{t+1}|q_t = S_t)\)</dd>
  <dt><dfn>Hidden Markov Modell</dfn> (<dfn>HMM</dfn>)</dt>
  <dd>Eine HMM ist ein Tupel \(\lambda = (S, V, A, B, \Pi)\):
      <ul>
          <li>\(S = \{S_1, \dots, S_n\}\): Menge der Zustände</li>
          <li>\(q_t\): Zustand zum Zeitpunkt \(t\)</li>
          <li>\(V = \{v_1, \dots, v_m\}\): Menge der Ausgabezeichen</li>
          <li>\(A \in [0,1]^{n \times n}\) = (a_{ij}): Übergangsmatrix, die die Wahrscheinlichkeit von Zustand \(i\) in Zustand \(j\) zu kommen beinhaltet</li>
          <li>\(B = (b_{ik})\) die Emissionswahrscheinlichkeit \(v_k\) im Zustand \(S_i\) zu beobachten</li>
          <li>\(\Pi = (\pi_i) = P(q_1 = i)\): Die Startverteilung</li>
      </ul></dd>
  <dt><a href="https://de.wikipedia.org/wiki/Forward-Algorithmus"><dfn>Vorwärts-Algorithmus</dfn></a></dt>
  <dd>Der Vorwärts-Algorithmus löst das Evaluierungsproblem. Er benutzt dazu
      dynamische Programmierung: Die Variablen \(\alpha_t(i) = P(o_1 o_2 \dots o_t; q_t = s_i)\) gibt die Wahrscheinlichkeit
      an zum Zeitpunkt \(t \in 1 \leq t \leq T\) im Zustand \(s_i \in S\) zu
      sein und die Sequenz \(o_1 o_2 \dots o_t\) beobachtet zu haben. Diese
      werden rekursiv berechnet.</dd>
  <dt><a href="https://de.wikipedia.org/wiki/Backward-Algorithmus"><dfn>Rückwärts-Algorithmus</dfn></a></dt>
  <dd>Der Rückwärts-Algorithmus löst das Dekodierungsproblem. Er benutzt dazu
      dynamische Programmierung: Die Variablen \(\beta_t(i) = P(o_{t+1} o_{t+2} \dots o_{T}|q_t = s_i, \lambda)\) geben
      die Wahrscheinlichkeit an, dass die Sequenz \(o_{t+1} o_{t+2} \dots o_{T}\)
      beobachtet werden wird, gegeben das HMM \(\lambda\) und den
      Startzustand \(s_i\).</dd>
  <dt><a href="https://de.wikipedia.org/wiki/Viterbi-Algorithmus"><dfn>Viterbi-Algorithmus</dfn></a></dt>
  <dd>Löst P2: Siehe <a href="//martin-thoma.com/apply-viterbi-aglrithm/">How to apply the Viterbi algorithm</a></dd>
  <dt><a href="https://de.wikipedia.org/wiki/Baum-Welch-Algorithmus"><dfn>Baum-Welch-Algorithmus</dfn></a></dt>
  <dd>Löst P3:

      Gegeben sei eine Trainingssequenz \(O_{\text{train}}\) und ein Modell
      \[\lambda = \{S, V, A, B, \Pi\}\]


      Gesucht ist ein Modell

      \[\bar \lambda = \text{arg max}_{\bar \lambda = \{S, V, \bar A, \bar B, \bar Pi\}} P(O_{\text{train}}|\lamba)\]

      Der Baum-Welch-Algorithmus geht wie folgt vor:

      <ol>
          <li>Bestimme \(P(O_{\text{train}} | \lambda)\)</li>
          <li>Schätze ein besseres Modell \(\bar \lambda\): TODO - Genauer! (Folie 31 - 36)</li>
      </ol>

      Iteriere diese Schritte so lange, bis ein lokales Maximum gefunden wurde.
      </dd>
  <dt><dfn>Ergodisches Modell</dfn></dt>
  <dd>Unter dem <i>ergodischen Modell</i> versteht man im Kontext von
      <abbr title="Hidden Markov Models">HMMs</abbr> die vollverbundene
      Topologie inclusive Schleifen.</dd>
  <dt><dfn>Bakis-Modell</dfn> (<dfn>Links-nach-Rechts-Modell</dfn>)</dt>
  <dd>Unter dem <i>Bakis-Modell</i> versteht man im Kontext von
      <abbr title="Hidden Markov Models">HMMs</abbr> eine Links-nach-Rechts
      Topologie, bei der maximal ein Zustand übersprungen werden kann.
      Das bedeutet, es gibt eine Ordnung über den Zuständen. Von einem
      Zustand \(i\) kommt man in die Zustände \(i, i+1, i+2\).</dd>
</dl>

<p>Die drei Probleme von HMMs sind</p>

<ul>
<li><strong>P1 - Evaluierungsproblem</strong>: Wie wahrscheinlich ist eine Sequenz
\(\bf{o} = o_1 o_2 \dots o_T\)
gegeben ein HMM \(\lambda\), also \(P(\bf{o}|\lambda)\).</li>
<li><strong>P2 - Dekodierungsproblem</strong>: Finden der wahrscheinlichsten Zustandssequenz,
gegeben eine Sequenz von Beobachtungen.</li>
<li><strong>P3 - Lernproblem</strong>: Optimieren der Modellparameter</li>
</ul>

<p>Anwendungen:</p>

<ul>
<li>Gestenerkennung</li>
<li>Phonem-Erkennung</li>
</ul>

<h3 id="tocAnchor-1-1-12">Markov Logik Netze</h3>

<p>Slides: <code>MLI_11-MLN_slides1</code></p>

<dl>
  <dt><a href="https://de.wikipedia.org/wiki/Markov_Logik_Netze"><dfn>Markov Logik Netze</dfn></a> (<dfn>MLN</dfn>)</dt>
  <dd>Ein Markov Logik Netz \(L\) ist ein Menge aus Tupeln \((F_i, w_i)\), wobei \(F_i\) eine Formel der Prädikatenlogik erster Ordnung ist und \(w_i \in \mathbb{R}\) ein Gewicht ist.
      Ein MLN ist eine Schablone für ein MRF.</dd>
  <dt><dfn>Markov Random Field</dfn> (<dfn>Markov Netzwerk</dfn>, <dfn>MRF</dfn>)</dt>
  <dd></dd>
</dl>

<p>Siehe auch:</p>

<ul>
<li>Matthew Richardson, Pedro Domingos: <a href="http://link.springer.com/article/10.1007/s10994-006-5833-1">Markov logic networks</a></li>
<li>Coursera: <a href="https://www.coursera.org/course/pgm">Probabilistic Graphical Models</a></li>
<li>Pedro Domingos: <a href="https://www.youtube.com/watch?v=bW5DzNZgGxY">Unifying Logical and Statistical AI</a>, September 2009.</li>
<li>Software: <a href="https://alchemy.cs.washington.edu/">Alchemy</a></li>
</ul>

<h2 id="tocAnchor-1-14">Prüfungsfragen</h2>

<ul>
    <li>Was ist Induktives Lernen?
        → Eine große Menge an Beispielen wird gegeben. Der Lerner muss selbst
           das Konzept herausfinden.</li>
    <li>Was ist Deduktives Lernen?
        → Fakten werden gegeben. Der lernende bekommt das allgemeine Konzept
           gesagt und muss nur logische Schlussfolgerungen machen.</li>
    <li>Wie lautet die Bellman-Gleichung?
        → \(Q(s, a) = r + \gamma \max_{a'} Q(s', a')\) wobei \(\gamma\) ein
        Diskontierungsfaktor ist, \(s'\) der Zustand in den man kommt, wenn
        man \(a\) ausführt und \(r\) der Reward nach ausführen von \(a\) in
        \(s\) ist.</li>
    <li>Wie lautet die Fehlerabschätzung von Vapnik?
        → Siehe abschätzung des realen Fehlers durch den empirischen Fehler
           und die VC-Dimension in "Abschätzung des Testfehlers"</li>
    <li>Wie funktioniert Q-Learning?
        → TODO</li>
    <li>Was versteht man unter Cascade Correlation?
        → <a href="https://www.youtube.com/watch?v=1E3XZr-bzZ4">YouTube</a> (4:05 min)</li>
    <li>Welche übwerwachten Lernverfahren gibt es?
        → Neuronale Netze, SVMs</li>
</ul>

<h2 id="tocAnchor-1-15">Material und Links</h2>

<ul>
<li><a href="http://cg.ivd.kit.edu/lehre/ws2015/cg/index.php">Vorlesungswebsite</a></li>
<li><a href="http://cg.ivd.kit.edu/lehre/ws2015/cg/uebung.php">Übungswebsite</a></li>
<li>StackExchange

<ul>
<li><a href="http://datascience.stackexchange.com/q/8642/8820">What is the difference between concept learning and classification?</a></li>
</ul></li>
<li><a href="//martin-thoma.com/machine-learning-2-course/">Zusammenfassung der Vorlesung ML 2</a></li>
</ul>

<h2 id="tocAnchor-1-16">Übungsbetrieb</h2>

<p>Es gibt keine Übungsblätter, keine Übungen, keine Tutorien und keine
Bonuspunkte.</p>

<h2 id="tocAnchor-1-17">Termine und Klausurablauf</h2>

<p><strong>Datum</strong>: Mündliche Prüfung (in Zukunft schriftlich)<br />
<strong>Ort</strong>: nach Absprache<br />
<strong>Zeit</strong>: ? min<br />
<strong>Übungsschein</strong>: gibt es nicht<br />
<strong>Bonuspunkte</strong>: gibt es nicht<br />
<strong>Ergebnisse</strong>: werden ca. 5 - 10 min. nach der Prüfung gesagt<br />
<strong>Erlaubte Hilfsmittel</strong>: keine</p>

                        </div>
                        <div class="postmeta">Posted in
                            
                                <a href="//martin-thoma.com/category/deutschland/">german posts</a><!--TODO: Displayed category name should be upper case! -->
                             | Tags:
                            
                                
                                    <a href="//martin-thoma.com/tag/klausur/">Klausur</a>
                                
                             by <a rel="author" class="vcard author" href="//martin-thoma.com/author/martin-thoma/"><span class="fn">Martin Thoma</span></a> on <span class="updated"><span class="value-title" title="2015-11-09 16:02:00 +0100">
                                November
                                9th
                                  ,
                                2015</span></span></div>

                            <div class="navigation clearfix">
                                <div class="alignleft">
                                
                                    &laquo; <a href="//martin-thoma.com/wo-ist-hs9/" rel="prev">Wo ist Hörsaal 9 am KIT?</a>
                                
                                </div>
                                <div class="alignright">
                                
                                    <a href="//martin-thoma.com/tensor-flow-quick/" rel="next">Tensor Flow - A quick impression</a> &raquo;
                                
                                </div>
                            </div>

                        </article>
                        <div id="respond">
                            <h3>Leave a Reply</h3>
                                <!-- comment discuss code -->
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'martinthoma'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    <!-- comment discuss code -->

                        </div>
                    </div>
                </div>
            <div class="span-8 last">
                <div id="subscriptions">
<a href="//martin-thoma.com/feed/"><img src="//martin-thoma.com/css/images/rss.png" alt="Subscribe to RSS Feed" title="Subscribe to RSS Feed" width="72" height="47" /></a>		
<a href="https://twitter.com/#!/themoosemind" title="Follow me on Twitter!"><img src="//martin-thoma.com/css/images/twitter.png" title="Follow me on Twitter!" alt="Follow me on Twitter!"  width="76" height="47" /></a>
</div>

                <div id="sidebar">
                <!-- type: searchbox.html - TODO-->
<ul>
    <li id="search">
        <div class="searchlayout">
            <form method="get" id="searchform" action="http://google.com/cse" role="search">
                <input type="hidden" name="cx" value="017345337424948206369:qrnnnentkkk" />
                <input type="search" value="" name="q" id="s" placeholder="Search with Google"/>
                <input type="image" src="//martin-thoma.com/css/images/search.gif" style="border:0; vertical-align: top;" alt="search"/> 
            </form>
        </div>
    </li>
</ul>

                <div class="addthis_toolbox">   
    <div class="custom_images">
            <a href="http://twitter.com/share?url=//martin-thoma.com/machine-learning-1-course&amp;hashtags=klausur,&amp;via=themoosemind" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/twitter.png" width="32" height="32" alt="Twitter" /></a>
            <a href="http://del.icio.us/post?url=//martin-thoma.com/machine-learning-1-course&amp;title=Machine%20Learning%201" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/delicious.png" width="32" height="32" alt="Delicious" /></a>
            <a href="http://www.facebook.com/sharer.php?u=//martin-thoma.com/machine-learning-1-course" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/facebook.png" width="32" height="32" alt="Facebook" /></a>
            <a href="http://digg.com/submit?phase=2&amp;url=//martin-thoma.com/machine-learning-1-course&amp;title=Machine%20Learning%201" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/digg.png" width="32" height="32" alt="Digg" /></a>
            <a href="http://www.stumbleupon.com/submit?url=//martin-thoma.com/machine-learning-1-course&amp;title=Machine%20Learning%201" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/stumbleupon.png" width="32" height="32" alt="Stumbleupon" /></a>
            <a href="https://plusone.google.com/_/+1/confirm?hl=en&amp;url=//martin-thoma.com/machine-learning-1-course" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/gplus.png" width="32" height="32" alt="Google Plus" /></a>
            <a href="http://reddit.com/submit?url=//martin-thoma.com/machine-learning-1-course&amp;title=Machine%20Learning%201" target="_blank"><img src="//martin-thoma.com/css/images/socialicons/reddit.png" width="32" height="32" alt="Reddit" /></a>
    </div>
</div>

                <ul>
                    <li id="categories-3" class="widget widget_categories">
                        <!-- type: categories -->
<h2 class="widgettitle">Categories</h2>
    <ul>
        <li class="cat-item cat-item-11"><a href="//martin-thoma.com/category/code/" title="Tipps for coding in different languages like Python oder C++.">Code</a></li>
        <li class="cat-item cat-item-21"><a href="//martin-thoma.com/category/web/" title="New emerging websites and technologies.">The Web</a></li>
        <li class="cat-item cat-item-31"><a href="//martin-thoma.com/category/cyberculture/" title="Lolcats, planking, Trollfaces, ...">Cyberculture</a></li>
        <li class="cat-item cat-item-3404"><a href="//martin-thoma.com/category/maths/" title="View all posts filed under Mathematics">Mathematics</a></li>
        <li class="cat-item cat-item-881"><a href="//martin-thoma.com/category/bits-and-bytes/" title="Sometimes posts don&#039;t fit in any category.">My bits and bytes</a></li>
        <li class="cat-item cat-item-41"><a href="//martin-thoma.com/category/deutschland/" title="[All Posts here are written in German about German topics] - Die Bahn, unsere Politik und Europa.">German posts</a></li>
	</ul>

                    </li>
                    <li id="tag_cloud-3" class="widget widget_tag_cloud">
                        <h2 class="widgettitle">Tags</h2>
                        <div class="tagcloud"><a style='font-size: 130.54%' href='//martin-thoma.com/tag/ai/' title='9 topics'>AI</a>
<a style='font-size: 102.57%' href='//martin-thoma.com/tag/algebra/' title='6 topics'>Algebra</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/assembly-language/' title='5 topics'>Assembly language</a>
<a style='font-size: 102.57%' href='//martin-thoma.com/tag/bash/' title='6 topics'>Bash</a>
<a style='font-size: 192.18%' href='//martin-thoma.com/tag/c/' title='22 topics'>C</a>
<a style='font-size: 137.80%' href='//martin-thoma.com/tag/cpp/' title='10 topics'>CPP</a>
<a style='font-size: 113.20%' href='//martin-thoma.com/tag/challenge/' title='7 topics'>Challenge</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/chrome/' title='5 topics'>Chrome</a>
<a style='font-size: 137.80%' href='//martin-thoma.com/tag/clip/' title='10 topics'>Clip</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/command-line/' title='5 topics'>Command Line</a>
<a style='font-size: 102.57%' href='//martin-thoma.com/tag/computer-science/' title='6 topics'>Computer science</a>
<a style='font-size: 122.41%' href='//martin-thoma.com/tag/digitaltechnik/' title='8 topics'>Digitaltechnik</a>
<a style='font-size: 102.57%' href='//martin-thoma.com/tag/flashgames/' title='6 topics'>Flashgames</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/geometry/' title='5 topics'>Geometry</a>
<a style='font-size: 130.54%' href='//martin-thoma.com/tag/google/' title='9 topics'>Google</a>
<a style='font-size: 122.41%' href='//martin-thoma.com/tag/google-code-jam/' title='8 topics'>Google Code Jam</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/html5/' title='5 topics'>HTML5</a>
<a style='font-size: 130.54%' href='//martin-thoma.com/tag/it-security/' title='9 topics'>IT-Security</a>
<a style='font-size: 226.14%' href='//martin-thoma.com/tag/java/' title='36 topics'>Java</a>
<a style='font-size: 102.57%' href='//martin-thoma.com/tag/javascript/' title='6 topics'>JavaScript</a>
<a style='font-size: 178.34%' href='//martin-thoma.com/tag/kit/' title='18 topics'>KIT</a>
<a style='font-size: 211.23%' href='//martin-thoma.com/tag/klausur/' title='29 topics'>Klausur</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/kogsys/' title='5 topics'>KogSys</a>
<a style='font-size: 200.99%' href='//martin-thoma.com/tag/latex/' title='25 topics'>LaTeX</a>
<a style='font-size: 178.34%' href='//martin-thoma.com/tag/linear-algebra/' title='18 topics'>Linear algebra</a>
<a style='font-size: 170.22%' href='//martin-thoma.com/tag/linux/' title='16 topics'>Linux</a>
<a style='font-size: 144.37%' href='//martin-thoma.com/tag/machine-learning/' title='11 topics'>Machine Learning</a>
<a style='font-size: 122.41%' href='//martin-thoma.com/tag/matrix/' title='8 topics'>Matrix</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/os/' title='5 topics'>OS</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/operating-systems/' title='5 topics'>Operating Systems</a>
<a style='font-size: 130.54%' href='//martin-thoma.com/tag/php/' title='9 topics'>PHP</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/physics/' title='5 topics'>Physics</a>
<a style='font-size: 238.39%' href='//martin-thoma.com/tag/programming/' title='43 topics'>Programming</a>
<a style='font-size: 130.54%' href='//martin-thoma.com/tag/project-euler/' title='9 topics'>Project Euler</a>
<a style='font-size: 270.00%' href='//martin-thoma.com/tag/python/' title='68 topics'>Python</a>
<a style='font-size: 102.57%' href='//martin-thoma.com/tag/review/' title='6 topics'>Review</a>
<a style='font-size: 130.54%' href='//martin-thoma.com/tag/swt-i/' title='9 topics'>SWT I</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/science/' title='5 topics'>Science</a>
<a style='font-size: 130.54%' href='//martin-thoma.com/tag/theoretical-computer-science/' title='9 topics'>Theoretical computer science</a>
<a style='font-size: 102.57%' href='//martin-thoma.com/tag/tikz/' title='6 topics'>Tikz</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/ubuntu/' title='5 topics'>Ubuntu</a>
<a style='font-size: 155.90%' href='//martin-thoma.com/tag/video/' title='13 topics'>Video</a>
<a style='font-size: 130.54%' href='//martin-thoma.com/tag/vimeo/' title='9 topics'>Vimeo</a>
<a style='font-size: 137.80%' href='//martin-thoma.com/tag/web-development/' title='10 topics'>Web Development</a>
<a style='font-size: 102.57%' href='//martin-thoma.com/tag/wikipedia/' title='6 topics'>Wikipedia</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/windows-7/' title='5 topics'>Windows 7</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/wolfram-alpha/' title='5 topics'>Wolfram|Alpha</a>
<a style='font-size: 144.37%' href='//martin-thoma.com/tag/youtube/' title='11 topics'>YouTube</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/advertising/' title='5 topics'>advertising</a>
<a style='font-size: 122.41%' href='//martin-thoma.com/tag/algorithms/' title='8 topics'>algorithms</a>
<a style='font-size: 113.20%' href='//martin-thoma.com/tag/analysis/' title='7 topics'>analysis</a>
<a style='font-size: 90.00%' href='//martin-thoma.com/tag/cheat-sheet/' title='5 topics'>cheat sheet</a>
<a style='font-size: 165.76%' href='//martin-thoma.com/tag/funny/' title='15 topics'>funny</a>
<a style='font-size: 165.76%' href='//martin-thoma.com/tag/learning/' title='15 topics'>learning</a>
<a style='font-size: 137.80%' href='//martin-thoma.com/tag/lecture-notes/' title='10 topics'>lecture-notes</a>
<a style='font-size: 252.81%' href='//martin-thoma.com/tag/mathematics/' title='53 topics'>mathematics</a>
<a style='font-size: 188.97%' href='//martin-thoma.com/tag/puzzle/' title='21 topics'>puzzle</a>
</div>
                    </li>
                </ul>
                </div>
            </div>
        </div><!--/container-->
            <footer id="footer">
                <a href="//martin-thoma.com"><strong>Martin Thoma</strong></a> -  A blog about Code, the Web and Cyberculture. <br />
                <div class="footer-credits">
                    <a href="http://flexithemes.com/themes/modern-style/">Modern Style</a> theme by <a href="http://flexithemes.com/">FlexiThemes</a>
                </div>
            </footer><!--/footer-->

    </div><!--/wrapper-->
<!-- type: footer -->
<!-- MathJax -->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">
<!--
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [ ['$$', '$$'], ['\\[','\\]']],
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    processEscapes: true
  }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
// -->
</script>
<!-- TOC Plus -->
<script type='text/javascript'>
/* <![CDATA[ */
var tocplus = {"visibility_show":"show","visibility_hide":"hide","width":"275px"};
/* ]]> */
</script>
<script type='text/javascript' src="//martin-thoma.com/js/tocplus-front.js"></script>

</body>
</html>

