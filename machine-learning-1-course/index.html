<!DOCTYPE html>
<html lang="en">
  <!-- type: head.html -->
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    
    

    
        <meta name="thumbnail" content="http://martin-thoma.com/images/logos/klausur.png" />
        <meta property="og:image" content="http://martin-thoma.com/images/logos/klausur.png" />
    

    <meta property="og:type" content="blog"/>

    <title>Machine Learning 1</title>
    <link rel="stylesheet" href="http://martin-thoma.com/css/screen.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="http://martin-thoma.com/css/style.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="http://martin-thoma.com/css/pygments.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="http://martin-thoma.com/css/tocplus-screen.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="http://martin-thoma.com/css/print.css" type="text/css" media="print" />
    <link rel="stylesheet" href="http://martin-thoma.com/css/handheld.css" type="text/css" media="only screen and (max-device-width: 480px)" />

    <link rel="alternate" type="application/rss+xml" title="Martin Thoma RSS Feed" href="http://martin-thoma.com/feed/" /><!--TODO-->
    <link rel="shortcut icon" href="http://martin-thoma.com/favicon.ico" type="image/x-icon" />

    <link rel="canonical" href="http://martin-thoma.com/machine-learning-1-course" />
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:site" content="@themoosemind"/>
<meta name="twitter:creator" content="@themoosemind"/>
<meta name="twitter:title" content="Machine Learning 1"/>

    <meta name="twitter:description" content="A blog about Code, the Web and Cyberculture" />


    <meta name="twitter:image" content="http://martin-thoma.com/images/logos/klausur.png"/>



<meta name="twitter:url" content="http://martin-thoma.com/machine-learning-1-course"/>
<meta name="twitter:domain" content="Martin Thoma.com"/>


    <script type='text/javascript' src="http://martin-thoma.com/js/jquery.js"></script>
    <script type='text/javascript' src="http://martin-thoma.com/js/jquery-migrate.min.js"></script>
    <style type="text/css">div#toc_container {width: 275px;}</style>
    <style type="text/css" id="syntaxhighlighteranchor"></style>
</head>

<!-- type: post.html -->
<body>
    <div id="wrapper">
        <div id="container" class="container">
            <div class="span-16">
                <!-- type: header.html -->
<div id="header" role="banner">
    <h1><a href="http://martin-thoma.com">Martin Thoma</a></h1>
    <h2>A blog about Code, the Web and Cyberculture.</h2>
</div>
<nav class="navcontainer" role="navigation">
    <ul id="nav">
        <li class=""><a href="http://martin-thoma.com">Home</a></li>
        <li class="page_item page-item-41 "><a href="http://martin-thoma.com/author/martin-thoma/">About Me</a></li>
        <li class="page_item page-item-91 "><a href="http://martin-thoma.com/imprint/">Imprint</a></li>
    </ul>
</nav>

                <div id="content">
                    <article class="post type-post format-standard hentry clearfix ">
                        <h2 class="title entry-title">Machine Learning 1</h2>
                        <div class="postdate entry-date">
                            <time datetime="2015-11-09T16:02:00+01:00">
                                November
                                9th,
                                  
                                2015
                            </time>
                        </div>

                        <div class="entry">
                            <div class="info">Dieser Artikel beschäftigt sich mit der Vorlesung „Machine Learning 1“ am KIT. Er dient als Prüfungsvorbereitung. Ich habe die Vorlesungen bei <a href="http://www.fzi.de/wir-ueber-uns/organisation/mitarbeiter/address/39/?no_cache=1">Herrn Prof. Dr. Zöllner</a> im Wintersemester 2014/2015 gehört.</div>

<div id="toc_container" class="toc_light_blue no_bullets">
   <p class="toc_title">Contents</p>
   <ul class="toc_list">
      <li class="toc_level-1 toc_section-1">
         <a href="#tocAnchor-1-1"><span class="tocnumber">1</span> <span class="toctext">Folien</span></a>
         <ul>
            <li class="toc_level-2 toc_section-2">
               <a href="#tocAnchor-1-1-1"><span class="tocnumber">1.1</span> <span class="toctext">Einordnungskriterien</span></a>
            </li>
            <li class="toc_level-2 toc_section-3">
               <a href="#tocAnchor-1-1-2"><span class="tocnumber">1.2</span> <span class="toctext">Einführung</span></a>
            </li>
            <li class="toc_level-2 toc_section-4">
               <a href="#tocAnchor-1-1-3"><span class="tocnumber">1.3</span> <span class="toctext">Induktives Lernen</span></a>
            </li>
            <li class="toc_level-2 toc_section-5">
               <a href="#tocAnchor-1-1-4"><span class="tocnumber">1.4</span> <span class="toctext">Reinforcement Learning</span></a>
            </li>
            <li class="toc_level-2 toc_section-6">
               <a href="#tocAnchor-1-1-5"><span class="tocnumber">1.5</span> <span class="toctext">Lerntheorie</span></a>
            </li>
            <li class="toc_level-2 toc_section-7">
               <a href="#tocAnchor-1-1-6"><span class="tocnumber">1.6</span> <span class="toctext">Neuronale Netze</span></a>
            </li>
            <li class="toc_level-2 toc_section-8">
               <a href="#tocAnchor-1-1-7"><span class="tocnumber">1.7</span> <span class="toctext">Instanzbasiertes Lernen</span></a>
            </li>
            <li class="toc_level-2 toc_section-9">
               <a href="#tocAnchor-1-1-8"><span class="tocnumber">1.8</span> <span class="toctext">SVM</span></a>
            </li>
            <li class="toc_level-2 toc_section-10">
               <a href="#tocAnchor-1-1-9"><span class="tocnumber">1.9</span> <span class="toctext">Entscheidungsbäume</span></a>
            </li>
            <li class="toc_level-2 toc_section-11">
               <a href="#tocAnchor-1-1-10"><span class="tocnumber">1.10</span> <span class="toctext">Bayes Lernen</span></a>
            </li>
            <li class="toc_level-2 toc_section-12">
               <a href="#tocAnchor-1-1-11"><span class="tocnumber">1.11</span> <span class="toctext">HMM</span></a>
            </li>
            <li class="toc_level-2 toc_section-13">
               <a href="#tocAnchor-1-1-12"><span class="tocnumber">1.12</span> <span class="toctext">Markov Logik Netze</span></a>
            </li>
         </ul>
      </li>
      <li class="toc_level-1 toc_section-14">
         <a href="#tocAnchor-1-14"><span class="tocnumber">2</span> <span class="toctext">Prüfungsfragen</span></a>
      </li>
      <li class="toc_level-1 toc_section-15">
         <a href="#tocAnchor-1-15"><span class="tocnumber">3</span> <span class="toctext">Material und Links</span></a>
      </li>
      <li class="toc_level-1 toc_section-16">
         <a href="#tocAnchor-1-16"><span class="tocnumber">4</span> <span class="toctext">Übungsbetrieb</span></a>
      </li>
      <li class="toc_level-1 toc_section-17">
         <a href="#tocAnchor-1-17"><span class="tocnumber">5</span> <span class="toctext">Termine und Klausurablauf</span></a>
      </li>
   </ul>
</div><h2 id="tocAnchor-1-1">Folien</h2>

<h3 id="tocAnchor-1-1-1">Einordnungskriterien</h3>

<p>Slide name: ML-Einordnungskriterien.pdf</p>

<ul>
<li><strong>Inferenztyp</strong>: Induktiv (version-space Algorithmus, k-NN, CBR, ID3, ID5R, von Beispielen auf allgemeine Regel "raten") ↔ Deduktiv (Erklärungsbasierte Generalisierung; Von allgemeinen auf spezielles)</li>
<li><strong>Lernebene</strong>: symbolisch (Special-to-General Konzeptlernen, CBR, ID3, ID5R; Semantik in Daten von der der Algorithmus Gebrauch macht) ↔ subsymbolisch (Neuronale Netze, k-NN; Daten sind Signale)</li>
<li><strong>Lernvorgang</strong>: überwacht (k-NN, CBR, ID3, ID5R) ↔ unüberwacht (k-Means)</li>
<li><strong>Beispielgebung</strong>: inkrementell (Version Space Algorithmus, CBR, ID5R) ↔ nicht inkrementell (k-Means, k-NN, ID3)</li>
<li><strong>Beispielumfang</strong>: umfangreich (Neuronale Netze, k-NN, ID3, ID5R) ↔ gering (CBR)</li>
<li><strong>Hintergrundwissen</strong>: empirisch (SVMs, k-NN, CBR, ID3, ID5R) ↔ axiomatisch (Erklärungsbasierte Generalisierung)</li>
</ul>

<h3 id="tocAnchor-1-1-2">Einführung</h3>

<p>Slide name: MLI_01_Einfuehrung_slides1.pdf</p>

<ul>
<li>Was ist Intelligenz? (Problemlösen, Erinnern, Sprache, Kreativität,
Bewusstsein, Überleben in komplexen Welten, )</li>
<li>Wissensrepräsentation:

<ul>
<li>Assoziierte Paare (Eingangs- und Ausgangsvariablen)</li>
<li>Entscheidungsbäume (Klassen diskriminieren)</li>
<li>Parameter in algebraischen ausdrücken</li>
<li>Formale Grammatiken</li>
<li>Logikbasierte Ausdrücke</li>
<li>Taxonomien</li>
<li>Semantische Netze</li>
<li>Markov-Ketten</li>
</ul></li>
</ul>

<dl>
  <dt><dfn>Machine Learning</dfn> by Tom Mitchell</dt>
  <dd>A computer program is said to learn from experience E with respect to
      some class of tasks T and performance measure P, if its performance at
      tasks in T, as measured by P, improves with experience E.</dd>
  <dt><dfn>Deduktion</dfn></dt>
  <dd>Die Deduktion ist eine Schlussfolgerung von gegebenen Prämissen auf die
      logisch zwingenden Konsequenzen. Deduktion ist schon bei Aristoteles als
      „Schluss vom Allgemeinen auf das Besondere“ verstanden worden.</dd>
  <dt><dfn>Modus ponens</dfn></dt>
  <dd>Der Modus ponens ist eine Art des logischen Schließens. Er besagt: Wenn
      die Prämissen \(A \rightarrow B\) und \(A\) gelten, dann gilt auch \(B\).</dd>
  <dt><dfn>Abduktion</dfn> by Peirce</dt>
  <dd>Deduction proves that something must be; Induction shows that something
      actually is operative; Abduction merely suggests that something may
      be.</dd>
</dl>

<h3 id="tocAnchor-1-1-3">Induktives Lernen</h3>

<p>Slide name: MLI_02_InduktivesLernen_slides1.pdf</p>

<ul>
<li>Konzept: Beschreibt Untermenge von Objekten oder Ereignissen definiert auf
größerer Menge.</li>
<li>Konsistenz: Keine negativen Beispiele werden positiv klassifiziert.</li>
<li>Vollständigkeit: Alle positiven Beispiele werden als positiv klassifiziert.</li>
<li>Algorithmen: Bäume (Wälder?)

<ul>
<li>Suche vom Allgemeinen zum Speziellen: Negative Beispiele führen zur Spezialisierung</li>
<li>Suche vom Speziellen zum Allgemeinen: Positive Beispiele führen zur Verallgemeinerung</li>
<li><a href="https://de.wikipedia.org/wiki/Versionsraum">Version Space</a>: Beides gleichzeitig anwenden</li>
</ul></li>
<li>Präzendenzgraphen: In welcher Reihenfolge werden Aktionen ausgeführt?</li>
</ul>

<p>Version Space Algorithmus ist:</p>

<ul>
<li>Induktiver Inferenztyp</li>
<li>Symbolische Ebene des Lernens</li>
<li>Überwachtes Lernen</li>
<li>Inkrementelle Beispielgebung</li>
<li>Umfangreich (viele Beispiele)</li>
<li>Empirisches Hintergrundwissen</li>
<li>Voraussetzungen: Konsistente Beispiele, korrekte Hypothese im Hypothesenraum</li>
<li>Positive Aspekte:

<ul>
<li>Es ist feststellbar, welche Art von Beispielen noch nötig ist</li>
<li>Es ist feststellbar, wann das Lernen abgeschlossen ist</li>
</ul></li>
</ul>

<p>Weiteres</p>

<dl>
  <dt><dfn>Inductive bias</dfn></dt>
  <dd>Induktives Lernen benötigt Vorannahmen</dd>
  <dt><dfn>Bias</dfn> ("Vorzugskriterium")</dt>
  <dd>Vorschrift, nach der Hypothese gebildet werden</dd>
</dl>

<h3 id="tocAnchor-1-1-4">Reinforcement Learning</h3>

<p>Slide name: MLI_03_ReinforcementLearning_slides1.pdf</p>

<p>Beim <a href="https://de.wikipedia.org/wiki/Best%C3%A4rkendes_Lernen">reinforcement learning</a> gehen wir hier von <a href="https://de.wikipedia.org/wiki/Markow-Entscheidungsproblem">Markow-Entscheidungsproblemen</a> aus.</p>

<ul>
<li>Beispiel: Roboter muss zu einem Ziel navigieren</li>
</ul>

<p>Algorithmen:</p>

<ul>
<li>Policy Learning</li>
<li>Simple Value Iteration</li>
<li>Simple Temporal Difference Learning</li>
<li><a href="https://en.wikipedia.org/wiki/Q-learning">Q-learning</a>

<ul>
<li><a href="https://www.youtube.com/watch?v=yS5F_vm9Ahk">YouTube: Lecture 18: RL Part 1: Q-Learning</a>: 1:16:11. BrownCS141 Spring 2014.</li>
<li><a href="https://www.youtube.com/watch?v=3sLV0OJLdns">YouTube: PacMan</a></li>
</ul></li>
<li><a href="https://de.wikipedia.org/wiki/Temporal_Difference_Learning">TD-Learning</a> (Temporal Difference Learning): TODO - wo genau ist der Unterschied zum Q-Learning?</li>
</ul>

<p>Siehe auch:</p>

<ul>
<li><a href="https://de.wikipedia.org/wiki/Optimalit%C3%A4tsprinzip_von_Bellman">Optimalitätsprinzip von Bellman</a></li>
</ul>

<h3 id="tocAnchor-1-1-5">Lerntheorie</h3>

<p>Slide name: MLI_04_Lerntheorie_slides1.pdf</p>

<dl>
  <dt><dfn>Ockhams Rasiermesser</dfn> (Quelle: <a href="https://de.wikipedia.org/wiki/Ockhams_Rasiermesser">Wikipedia</a>)</dt>
  <dd>Von mehreren möglichen Erklärungen für ein und denselben Sachverhalt ist
  die einfachste Theorie allen anderen vorzuziehen. Eine Theorie ist einfach,
  wenn sie möglichst wenige Variablen und Hypothesen enthält, und wenn diese in
  klaren logischen Beziehungen zueinander stehen, aus denen der zu erklärende
  Sachverhalt logisch folgt.</dd>
</dl>

<ul>
<li>Lernmaschine wird definiert durch Hypothesenraum \({h_\alpha: \alpha \in A}\)
und Lernverfahren. Das Lernverfahren ist die Methode um \(\alpha_{\text{opt}}\)
mit Hilfe von Lernbeispielen zu finden.</li>
<li>Probleme beim Lernen:

<ul>
<li>Größe des Hypothesenraums im Vergleich zur Anzahl der Trainingsdaten.</li>
<li>Das Verfahren könnte nur suboptimale Lösungen finden.</li>
<li>Das Verfahren könnte die passende Hypothese nicht beinhalten.</li>
</ul></li>
<li>Lernproblemtypen: Sei die Menge der Lernbeispiele in \(X \times Y\), mit \(X \times Y =\)...

<ul>
<li>\(\{Attribut_1, Attribut_2, ...\} \times \{True, False\}\): Konzeptlernen</li>
<li>\(\mathbb{R}^n \times \{Klasse_1, ..., Klasse_n\}\): Klassifikation</li>
<li>\(\mathbb{R}^n \times \mathbb{R}\): Regression</li>
</ul></li>
<li>Gradientenabstieg, Overfitting</li>
<li>Kreuzvalidierung</li>
<li>Bootstrap: TODO (Folie 18 - 20)</li>
<li><a href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning">Probably approximately correct learning</a>

<ul>
<li>Folie 35: Was ist eine Instanz der Länge \(n\)? (TODO)</li>
</ul></li>
<li>VC-Dimension

<ul>
<li>Folie 44: Was ist \(\eta\)?</li>
</ul></li>
<li>TODO: Wie hängen PAC und VC-Dimension zusammen?</li>
<li>Structural Risc Minimization: TODO - Was ist das?</li>
</ul>

<h4 id="boosting">Boosting</h4>

<dl>
  <dt><a href="https://de.wikipedia.org/wiki/Boosting"><dfn>Boosting</dfn></a></dt>
  <dd>Kombiniere mehrere schwache Modelle um ein gutes zu bekommen, indem
      Trainingsbeispiele unterschiedlich gewichtet werden.</dd>
  <dt><a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating"><dfn>Bagging</dfn></a> (<dfn>Bootstrap aggregating</dfn>)</dt>
  <dd>Kombiniere mehrere schwache Modelle um ein gutes zu bekommen. Dabei
      bekommt jedes schwache Modell nur eine Teilmenge aller Trainingsdaten.</dd>
</dl>

<ul>
<li>Fragen zu Folie 20:

<ul>
<li>Sind die Datensätze disjunkt?</li>
<li>TODO: Verstehe Algorithmus nicht.</li>
</ul></li>
<li>Folie 21: TODO - Verstehe AdaBoost nicht</li>
<li>Folie 22:

<ul>
<li>Wofür steht \(i\) und welchen Wertebereich hat \(i\)?</li>
<li>Stellt \(W_k(i)\) die Wahrscheinlichkeit dar, dass Beispiel \(i\) im \(k\)-ten
Durchlauf für das Training verwendet wird?</li>
</ul></li>
</ul>

<div style="width: 510px" class="wp-caption aligncenter">
   <a href="../images/2015/12/ml-ensemble-learning.png">
      <img src="http://martin-thoma.com/captions/ml-ensemble-learning.png" alt="Ensemble Learning Techniques: Boosting, Bagging, Random Subspaces, Pasting, Random Patches" width="500" height="478" class="" />
   </a>
   <p class="wp-caption-text">Ensemble Learning Techniques: Boosting, Bagging, Random Subspaces, Pasting, Random Patches</p>
</div>

<h4 id="vc-dimension">VC-Dimension</h4>

<dl>
  <dt>VC-Dimension</dt>
  <dd>Sei \(H^\alpha = \{h_\alpha : \alpha \in A\}\) der Hypothesenraum. Die
      VC-Dimension \(VC(h_\alpha)\) von \(H^\alpha\) ist gleich der maximalen
      Anzahl von beliebig platzierten Datenpunkten, die von \(H^\alpha\) separiert
      werden können.</dd>
</dl>

<ul>
<li>TODO - Folie 39: Was ist \(A\)? Warum ist \(h_\alpha\) wichtig? Sollte es
nicht eher \(VC(H^\alpha)\) sein?</li>
</ul>

<h3 id="tocAnchor-1-1-6">Neuronale Netze</h3>

<p>Slide name: MLI_05_Neuronale_Netze_slides1.pdf</p>

<ul>
<li>Einsatzfelder:

<ul>
<li>Klassifiktion: Spracherkennung, Schrifterkennung</li>
<li>Funktionsapproximation</li>
<li>Mustervervollständigung: Kodierung, Bilderkennung (TODO: Warum zählt das nicht zu Klassifikation?)</li>
</ul></li>
<li>Perzeptron von Rosenblatt (1960)

<ul>
<li>Auswertung: Input-Vektor und Bias mit Gewichten multiplizieren, addieren und Aktivierungsfunktion anwenden.</li>
<li>Training: Zufällige Initialisierung des Gewichtsvektors, addieren von fehlklassifizierten Vektoren auf Gewichtsvektor.</li>
</ul></li>
<li>Gradientenabstieg</li>
<li>Delta-Regel</li>
<li>Kernel-Methoden (TODO)</li>
<li>Radial-Basis Funktion Netz (TODO)</li>
<li><abbr title="Resilient Propagation">RPROP</abbr>: TODO</li>
</ul>

<h3 id="tocAnchor-1-1-7">Instanzbasiertes Lernen</h3>

<p>Slide name: MLI_06_InstanzbasiertesLernen_slides1.pdf</p>

<dl>
  <dt><dfn>Instanzenbasiertes Lernen</dfn> bzw. <dfn>Lazy Learning</dfn></dt>
  <dd>Instanzenbasiertes Lernen ist ein Lernverfahren, welches einfach nur
      die Beispiele abspeichert, also faul (engl. lazy) ist. Soll der Lerner
      neue Daten klassifizieren, so wird die Klasse des ähnlichsten
      Datensatzes gewählt.</dd>
  <dt><dfn>Case-based Reasoning</dfn> bzw. kurz <dfn>CBR</dfn></dt>
  <dd>CBR ist ein allgemeines, abstraktes Framework und kein direkt anwendbarer
      Algorithmus. Die Idee ist, dass nach ähnlichen, bekannten Fällen gesucht
      wird, auf die der aktuelle Fall übertragen werden kann.</dd>
  <dt><dfn>Fall</dfn> im Kontext des CBR</dt>
  <dd>Ein Fall ist eine Abstraktion eines Ereignisses, die in Zeit und Raum
      begrenzt ist. Ein Fall enthält eine Problembeschreibung, eine Lösung und
      ein Ergebnis. Zusätzlich kann ein Fall eine Erklärung enthalten warum
      das Ergebnis auftrat, Informationen über die Lösungsmethode, Verweise
      auf andere Fälle oder Güteinformationen enthalten.</dd>
</dl>

<ul>
<li><p>Beispiel für Lazy Learning: <abbr title="k Nearest Neighbors">(k)-NN</abbr>,
<abbr title="Case-based Reasoning">CBR</abbr></p></li>
<li><p>TODO: Folie 3: „Fleißige“ Lernalgorithmen mit dem gleichen Hypothesenraum sind
eingeschränkter - was ist damit gemeint?</p></li>
<li><p>TODO: Folie 6 - Was ist CBR?</p></li>
</ul>

<h3 id="tocAnchor-1-1-8">SVM</h3>

<p>Slide name: MLI_07_SVM_slides1.pdf</p>

<p>TODO: Allgemeines Verständnis, mal auf konkrete Fälle anwenden</p>

<ul>
<li>SVMs sind laut Vapnik die Lernmaschine mit der kleinsten möglichen VC-
Dimension, falls die Klassen linear trennbar sind.</li>
<li>Primäres Optimierungsproblem: Finde einen Sattelpunkt der Funktion<br />
\(L_P = L(\vec{w}, b, \vec{\alpha}) = \frac{1}{2}|\vec{w}|^2 - \sum_{i=1}^N \alpha_i (y_i(\vec{w}\vec{x_i}+b)-1)\)
wobei \(\alpha_1, \dots, \alpha_N \geq 0\) Lagrange-Multiplikatoren sind</li>
<li>Soft Margin Hyperebene</li>
<li>Der Parameter $C$ dient der Regularisierung. Ist $C$ groß gibt es wenige
Missklassifikationen in der Trainingsdatenmenge. Ist $C$ klein, werden die
Margins größer.</li>
<li>Nichtlineare Kernelmethoden</li>
<li>Kernel-Trick</li>
</ul>

<div class="alert alert-info"><h4>Abschätzung des Testfehlers</h4>
Mit Wahrscheinlichkeit \(P(1-\eta)\) gilt:
\[E(h_\alpha) \leq E_{emp}(h_\alpha) + \sqrt{\dots \frac{VC(h_\alpha)}{N} \dots}\]

wobei gilt:

<ul>
    <li>\(E(h_\alpha)\) ist der reale Fehler der mit der Hypothese \(h_alpha\)
        gemacht wird</li>
    <li>\(E_{emp}(h_\alpha)\) ist der empirische Fehler der mit der Hypothese \(h_alpha\)
        gemacht wird</li>
    <li>\(VC(h_\alpha)\) ist die VC-Dimension der Lernmaschine</li>
    <li>\(N\) ist die Anzahl der Lernbeispiele</li>
</ul>

Dieser Term wird in der <i>Structural Risc Minimization</i> minimiert.
</div>

<div class="alert alert-info"><h4>Die fünf Prinzipien von SVMs</h4>
<ol>
    <li>Lineare Trennung mit maximalen Abstand der Trennebenen zu den
        nächstgelegenen Stichproben (Support Vektoren)</li>
    <li>Duale Formulierung des linearen Klassifikators.
        (vgl. [Wiki](https://de.wikipedia.org/wiki/Support_Vector_Machine#Duales_Problem), \(k(m) = w^T m + b = \langle w, m \rangle + b = \sum_{j=1}^N \alpha_j z_j \langle m_j, m \rangle + b\))</li>
    <li>Nichtlineare Abbildung der primären Merkmale in einen hochdimensionalen
        Merkmalsraum \(\Phi\)</li>
    <li>Implizite Nutzung des unter Umständen \(\infty\)-dimensionalen
        Eigenfunktionsraumes einer sog. Kernfunktion \(K\) als transformierten
        Merkmalsraum \(\Phi\). Dabei müssen die transformierten Merkmale nicht
        explizit berechnet werden und der Klassifikator hat trotz der hohen
        Dimension von \(\Phi\) nur eine niedrige Zahl von freien Parametern
        (Kernel-Trick).</li>
    <li>Relaxation der Forderung nach linearer Trennbarkeit durch Einführung
        von Schlupfvariablen (slack variables).</li>
</ol>
</div>

<h3 id="tocAnchor-1-1-9">Entscheidungsbäume</h3>

<p>Slide name: MLI_08_Entscheidungsbaeume_slides1.pdf</p>

<dl>
  <dt><dfn>Entscheidungsbaum</dfn></dt>
  <dd>Ein Entscheidungsbaum ist ein Klassifikator in Baumstruktur. Die
      inneren Knoten des Entscheidungsbaumes sind Attributtests, die Blätter
      sind Klassen.</dd>
  <dt><a href="https://de.wikipedia.org/wiki/ID3"><dfn>ID3</dfn></a> (siehe <a href="(https://github.com/MartinThoma/LaTeX-examples/tree/master/source-code/Pseudocode/ID3">pseudocode</a>)</dt>
  <dd>ID3 ist ein Top-Bottom Verfahren zum Aufbau eines Entscheidungsbaumes.</dd>
  <dt><a href="https://de.wikipedia.org/wiki/C4.5"><dfn>C4.5</dfn></a> (siehe <a href="(https://github.com/MartinThoma/LaTeX-examples/tree/master/source-code/Pseudocode/ID3">pseudocode</a>)</dt>
  <dd>ID3 ist ein Top-Bottom Verfahren zum Aufbau eines Entscheidungsbaumes, welches auf ID3 basiert.</dd>
  <dt><dfn>Random Forest</dfn>, Quelle: <a href="https://de.wikipedia.org/wiki/Random_Forest">Wikipedia</a></dt>
  <dd>Ein Random Forest ist ein Klassifikationsverfahren, welches aus mehreren
  verschiedenen, unkorrelierten Entscheidungsbäumen besteht. Alle
  Entscheidungsbäume sind unter einer bestimmten Art von Randomisierung während
  des Lernprozesses gewachsen. Für eine Klassifikation darf jeder Baum in
  diesem Wald eine Entscheidung treffen und die Klasse mit den meisten Stimmen
  entscheidet die endgültige Klassifikation.</dd>
</dl>

<ul>
<li>Der Algorithmus ID5R dienen dem Aufbau eines Entscheidungsbaumes.</li>
<li>C4.5 unterstützt - im Gegensatz zu ID3 - kontinuierliche Attributwerte.
Außerdem kann C4.5 mit fehlenden Attributwerten umgehen.</li>
<li>Mögliches Qualtitätsmaß ist Entropie:<br />
\(Entropie(S) = - p_\oplus \log_2 p_\oplus - p_\ominus \log_2 p_\ominus\)
wobei $\oplus$ die positiven Beispiele und $\ominus$ die negativen Beispiele
bezeichnet.</li>
<li>TODO, Folie 41: Wo ist der Vorteil von ID5R im Vergleich zu ID3, wenn das
Ergebnis äquivalent ist?</li>
<li>Random Forest: Erstelle mehrere Entscheidungsbäume mit einer zufälligen
Wahl an Attributen. Jeder Baum stimmt für eine Klasse und die Klasse, für die
die meisten Stimmen, wird gewählt.</li>
</ul>

<h3 id="tocAnchor-1-1-10">Bayes Lernen</h3>

<p>Slide name: MLI_09_BayesLernen_slides1.pdf</p>

<dl>
  <dt><dfn>Satz von Bayes</dfn></dt>
  <dd>Seien \(A, B\) Ereignisse, \(P(B) &gt; 0\). Dann gilt:
      \(P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}\)<br />
      Dabei wird \(P(A)\) a priori Wahrscheinlichkeit, \(P(B|A)\) likelihood,
      und \(P(A|B)\) a posteriori Wahrscheinlichkeit genannt.</dd>
  <dt><dfn>Naiver Bayes-Klassifikator</dfn></dt>
  <dd>Ein Klassifizierer heißt naiver Bayes-Klassifikator, wenn er den
      Satz von Bayes unter der naiven Annahme der Unabhängigkeit der Features
      benutzt.</dd>
  <dt><dfn>Produktregel</dfn></dt>
  <dd>\(P(A \land B) = P(A|B) \cdot P(B) = P(B|A) \cdot P(A)\)</dd>
  <dt><dfn>Summenregel</dfn></dt>
  <dd>\(P(A \lor B) = P(A) + P(B) - P(A \land P)\)</dd>
  <dt><dfn>Theorem der totalen Wahrscheinlichkeit</dfn></dt>
  <dd>Es seien \(A_1, \dots, A_n\) Ereignisse mit \(i \neq j \Rightarrow A_i \cap A_j = \emptyset \;\;\;\forall i, j \in 1, \dots, n\) und \(\sum_{i=1}^n A_i = 1\). Dann gilt:<br />
      \(P(B) = \sum_{i=1}^n P(B|A_i) P(A_i)\)</dd>
  <dt><dfn>Maximum A Posteriori Hypothese</dfn> (MAP-Hypothese)</dt>
  <dd>Sei \(H\) der Raum aller Hypothesen und \(D\) die Menge der beobachteten
      Daten. Dann heißt<br />
      \(h_{MAP} = \text{arg max}_{h \in H} P(h|D) \cdot P(h)\)<br />
      die Menge der Maximum A Posteriori Hypothesen.</dd>
  <dt><dfn>Maximum Likelihood Hypothese</dfn> (ML-Hypothese)</dt>
  <dd>Sei \(H\) der Raum aller Hypothesen und \(D\) die Menge der beobachteten
      Daten. Dann heißt<br />
      \(h_{ML} = \text{arg max}_{h \in H} P(h|D)\)<br />
      die Menge der Maximum Likelihood Hypothesen.</dd>
  <dt><dfn>Normalverteilung</dfn></dt>
  <dd>Eine stetige Zufallsvariable \(X\) mit der Wahrscheinlichkeitsdichte
      \(f\colon\mathbb{R}\to\mathbb{R}\), gegeben durch<br />
      \(f(x) = \frac {1}{\sigma\sqrt{2\pi}} e^{-\frac {1}{2} \left(\frac{x-\mu}{\sigma}\right)^2}\)<br />
      heißt \(\mathcal N\left(\mu, \sigma^2\right)\)-verteilt, normalverteilt
      mit den Erwartungswert \(\mu\) und Varianz \(\sigma^2\).</dd>
  <dt><dfn>Prinzip der minimalen Beschreibungslänge</dfn> (<a href="https://de.wikipedia.org/wiki/Minimum_Description_Length">Wikipedia</a>)</dt>
  <dd>Das Prinzip der minimalen Beschreibungslänge ist eine formale
      Beschreibung von Ockhams Rasiermesser. Nach diesem Prinzip werden
      Hypothesen bevorzugt, die zur besten Kompression gegebener Daten führen.
  </dd>
  <dt><dfn>Gibbs-Algorithmus</dfn> (<a href="https://de.wikipedia.org/wiki/Gibbs-Sampling">Wikipedia</a> und <a href="http://stats.stackexchange.com/a/10216/25741">stats.stackexchange</a>)</dt>
  <dd>Der Algorithmus von Gibbs ist eine Methode um Stichproben von bedingten
      Verteilungen zu erzeugen. TODO: Ist das richtig? Wann wird es verwendet?
  </dd>
  <dt><dfn>Bedingte Unabhängigkeit</dfn> (<a href="https://de.wikipedia.org/wiki/Bedingte_Unabh%C3%A4ngigkeit">Wikipedia</a>)</dt>
  <dd>Seien \(X, Y, Z\) Zufallsvariablen. Dann heißt \(X\) bedingt unabhängig
      von \(Y\) gegeben \(Z\), wenn \[P(X|Y,Z) = P(X|Z)\] gilt.
  </dd>
  <dt><dfn>Add \(k\) smoothing</dfn> (<a href="https://en.wikipedia.org/wiki/Additive_smoothing">Wikipedia</a>)</dt>
  <dd>Unter Add-\(k\)-smoothing versteht man eine Technik, durch die
      sichergestellt wird, dass die geschätzte Wahrscheinlichkeit für kein
      Ereignis gleich null ist. Wenn man \(d \in \mathbb{N}\) mögliche
      Ergebnisse eines Experiments hat, \(N \in \mathbb{N}\) experimente
      durchgeführt werden, dann schätzt man die Wahrscheinlichkeit von dem
      Ergebnis \(i\) mit \[\hat{\theta_i} = \frac{x_i + k}{N+ kd}\], wobei
      \(x_i\) die Anzahl der Beobachtungen von \(i\) ist und \(k \geq 0\) der
      Glättungsparameter ist.
  </dd>
  <dt><dfn>Bayessches Netz</dfn> (<a href="https://de.wikipedia.org/wiki/Bayessches_Netz">Wikipedia</a>)</dt>
  <dd>Ein bayessches Netz ist ein gerichteter azyklischer Graph in dem die
      Knoten Zufallsvariablen und die Kanten bedingte Abhängigkeiten
      beschreiben.
  </dd>
</dl>

<p>Fragen:</p>

<ul>
<li>Folie 23: Warum ist \(h_{MAP(x)}\) nicht die wahrscheinlichste
Klassifikation?</li>
<li>Folie 24: Was ist (V)?</li>
</ul>

<h3 id="tocAnchor-1-1-11">HMM</h3>

<p>Slides: MLI_10_HMM_slides1.pdf</p>

<dl>
  <dt><dfn>Markov-Bedingung</dfn> (Beschränkter Horizont)</dt>
  <dd>P(q_{t+1}=S_{t+1}|q_t = S_t, q_{t-1} = S_{t-1}, \dots) = P(q_{t+1}=S_{t+1}|q_t = S_t)</dd>
  <dt><dfn>Hidden Markov Modell</dfn> (<dfn>HMM</dfn>)</dt>
  <dd>Eine HMM ist ein Tupel \(\lambda = (S, V, A, B, \Pi)\):
      <ul>
          <li>\(S = \{S_1, \dots, S_n\}\): Menge der Zustände</li>
          <li>\(q_t\): Zustand zum Zeitpunkt \(t\)</li>
          <li>\(V = \{v_1, \dots, v_m\}\): Menge der Ausgabezeichen</li>
          <li>\(A \in [0,1]^{n \times n}\) = (a_{ij}): Übergangsmatrix, die die Wahrscheinlichkeit von Zustand \(i\) in Zustand \(j\) zu kommen beinhaltet</li>
          <li>\(B = (b_{ik})\) die Emissionswahrscheinlichkeit \(v_k\) im Zustand \(S_i\) zu beobachten</li>
          <li>\(\Pi = (\pi_i) = P(q_1 = i)\): Die Startverteilung</li>
      </ul></dd>
  <dt><dfn>Vorwärts-Algorithmus</dfn></dt>
  <dd>Löst P1: TODO</dd>
  <dt><dfn>Rückwärts-Algorithmus</dfn></dt>
  <dd>TODO</dd>
  <dt><dfn>Viterbi-Algorithmus</dfn></dt>
  <dd>Löst P2: TODO</dd>
  <dt><dfn>Baum-Welch-Algorithmus</dfn></dt>
  <dd>Löst P3: TODO</dd>
  <dt><dfn>Ergodisches Modell</dfn></dt>
  <dd>TODO</dd>
  <dt><dfn>Bakis-Modell</dfn> (<dfn>Links-nach-Rechts-Modell</dfn>)</dt>
  <dd>TODO</dd>
</dl>

<p>Die drei Probleme von HMMs sind</p>

<ul>
<li>P1 - Evaluierungsproblem: Wie gut erklärt ein Modell eine beobachtete Sequenz?</li>
<li>P2 - Dekodierungsproblem: Finden der wahrscheinlichsten Zustandssequenz, gegeben
eine Sequenz von Beobachtungen</li>
<li>P3 - Lernproblem: Optimieren der Modellparameter</li>
</ul>

<p>Anwendungen:</p>

<ul>
<li>Gestenerkennung</li>
<li>Phonem-Erkennung</li>
</ul>

<h3 id="tocAnchor-1-1-12">Markov Logik Netze</h3>

<p>Slides: MLI_11-MLN_slides1</p>

<dl>
  <dt><dfn>Markov Logik Netze</dfn> (<dfn>MLN</dfn>)</dt>
  <dd>TODO</dd>
</dl>

<h2 id="tocAnchor-1-14">Prüfungsfragen</h2>

<p>Kommt noch</p>

<h2 id="tocAnchor-1-15">Material und Links</h2>

<ul>
<li><a href="http://cg.ivd.kit.edu/lehre/ws2015/cg/index.php">Vorlesungswebsite</a></li>
<li><a href="http://cg.ivd.kit.edu/lehre/ws2015/cg/uebung.php">Übungswebsite</a></li>
<li>StackExchange

<ul>
<li><a href="http://datascience.stackexchange.com/q/8642/8820">What is the difference between concept learning and classification?</a></li>
</ul></li>
</ul>

<h2 id="tocAnchor-1-16">Übungsbetrieb</h2>

<p>Es gibt keine Übungsblätter, keine Übungen, keine Tutorien und keine
Bonuspunkte.</p>

<h2 id="tocAnchor-1-17">Termine und Klausurablauf</h2>

<p><strong>Datum</strong>: Mündliche Prüfung<br />
<strong>Ort</strong>: nach Absprache<br />
<strong>Zeit</strong>: ? min<br />
<strong>Übungsschein</strong>: gibt es nicht<br />
<strong>Bonuspunkte</strong>: gibt es nicht<br />
<strong>Ergebnisse</strong>: werden ca. 5 - 10 min. nach der Prüfung gesagt<br />
<strong>Erlaubte Hilfsmittel</strong>: keine</p>

                        </div>
                        <div class="postmeta">Posted in
                            
                                <a href="http://martin-thoma.com/category/deutschland/">german posts</a><!--TODO: Displayed category name should be upper case! -->
                             | Tags:
                            
                                
                                    <a href="http://martin-thoma.com/tag/klausur/">Klausur</a>
                                
                             by <a rel="author" class="vcard author" href="http://martin-thoma.com/author/martin-thoma/"><span class="fn">Martin Thoma</span></a> on <span class="updated"><span class="value-title" title="2015-11-09 16:02:00 +0100">
                                November
                                9th
                                  ,
                                2015</span></span></div>

                            <div class="navigation clearfix">
                                <div class="alignleft">
                                
                                    &laquo; <a href="http://martin-thoma.com/wo-ist-hs9/" rel="prev">Wo ist Hörsaal 9 am KIT?</a>
                                
                                </div>
                                <div class="alignright">
                                
                                    <a href="http://martin-thoma.com/tensor-flow-quick/" rel="next">Tensor Flow - A quick impression</a> &raquo;
                                
                                </div>
                            </div>

                        </article>
                        <div id="respond">
                            <h3>Leave a Reply</h3>
                                <!-- comment discuss code -->
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'martinthoma'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    <!-- comment discuss code -->

                        </div>
                    </div>
                </div>
            <div class="span-8 last">
                <div id="subscriptions">
<a href="http://martin-thoma.com/feed/"><img src="http://martin-thoma.com/css/images/rss.png" alt="Subscribe to RSS Feed" title="Subscribe to RSS Feed" width="72" height="47" /></a>		
<a href="https://twitter.com/#!/themoosemind" title="Follow me on Twitter!"><img src="http://martin-thoma.com/css/images/twitter.png" title="Follow me on Twitter!" alt="Follow me on Twitter!"  width="76" height="47" /></a>
</div>

                <div id="sidebar">
                <!-- type: searchbox.html - TODO-->
<ul>
    <li id="search">
        <div class="searchlayout">
            <form method="get" id="searchform" action="http://google.com/cse" role="search">
                <input type="hidden" name="cx" value="017345337424948206369:qrnnnentkkk" />
                <input type="search" value="" name="q" id="s" placeholder="Search with Google"/>
                <input type="image" src="http://martin-thoma.com/css/images/search.gif" style="border:0; vertical-align: top;" alt="search"/> 
            </form>
        </div>
    </li>
</ul>

                <div class="addthis_toolbox">   
    <div class="custom_images">
            <a href="http://twitter.com/share?url=http://martin-thoma.com/machine-learning-1-course&amp;hashtags=klausur,&amp;via=themoosemind" target="_blank"><img src="http://martin-thoma.com/css/images/socialicons/twitter.png" width="32" height="32" alt="Twitter" /></a>
            <a href="http://del.icio.us/post?url=http://martin-thoma.com/machine-learning-1-course&amp;title=Machine%20Learning%201" target="_blank"><img src="http://martin-thoma.com/css/images/socialicons/delicious.png" width="32" height="32" alt="Delicious" /></a>
            <a href="http://www.facebook.com/sharer.php?u=http://martin-thoma.com/machine-learning-1-course" target="_blank"><img src="http://martin-thoma.com/css/images/socialicons/facebook.png" width="32" height="32" alt="Facebook" /></a>
            <a href="http://digg.com/submit?phase=2&amp;url=http://martin-thoma.com/machine-learning-1-course&amp;title=Machine%20Learning%201" target="_blank"><img src="http://martin-thoma.com/css/images/socialicons/digg.png" width="32" height="32" alt="Digg" /></a>
            <a href="http://www.stumbleupon.com/submit?url=http://martin-thoma.com/machine-learning-1-course&amp;title=Machine%20Learning%201" target="_blank"><img src="http://martin-thoma.com/css/images/socialicons/stumbleupon.png" width="32" height="32" alt="Stumbleupon" /></a>
            <a href="https://plusone.google.com/_/+1/confirm?hl=en&amp;url=http://martin-thoma.com/machine-learning-1-course" target="_blank"><img src="http://martin-thoma.com/css/images/socialicons/gplus.png" width="32" height="32" alt="Google Plus" /></a>
            <a href="http://reddit.com/submit?url=http://martin-thoma.com/machine-learning-1-course&amp;title=Machine%20Learning%201" target="_blank"><img src="http://martin-thoma.com/css/images/socialicons/reddit.png" width="32" height="32" alt="Reddit" /></a>
    </div>
</div>

                <ul>
                    <li id="categories-3" class="widget widget_categories">
                        <!-- type: categories -->
<h2 class="widgettitle">Categories</h2>
    <ul>
        <li class="cat-item cat-item-11"><a href="http://martin-thoma.com/category/code/" title="Tipps for coding in different languages like Python oder C++.">Code</a></li>
        <li class="cat-item cat-item-21"><a href="http://martin-thoma.com/category/web/" title="New emerging websites and technologies.">The Web</a></li>
        <li class="cat-item cat-item-31"><a href="http://martin-thoma.com/category/cyberculture/" title="Lolcats, planking, Trollfaces, ...">Cyberculture</a></li>
        <li class="cat-item cat-item-3404"><a href="http://martin-thoma.com/category/maths/" title="View all posts filed under Mathematics">Mathematics</a></li>
        <li class="cat-item cat-item-881"><a href="http://martin-thoma.com/category/bits-and-bytes/" title="Sometimes posts don&#039;t fit in any category.">My bits and bytes</a></li>
        <li class="cat-item cat-item-41"><a href="http://martin-thoma.com/category/deutschland/" title="[All Posts here are written in German about German topics] - Die Bahn, unsere Politik und Europa.">German posts</a></li>
	</ul>

                    </li>
                    <li id="tag_cloud-3" class="widget widget_tag_cloud">
                        <h2 class="widgettitle">Tags</h2>
                        <div class="tagcloud"><a style='font-size: 131.00%' href='http://martin-thoma.com/tag/ai/' title='9 topics'>AI</a>
<a style='font-size: 102.72%' href='http://martin-thoma.com/tag/algebra/' title='6 topics'>Algebra</a>
<a style='font-size: 90.00%' href='http://martin-thoma.com/tag/assembly-language/' title='5 topics'>Assembly language</a>
<a style='font-size: 102.72%' href='http://martin-thoma.com/tag/bash/' title='6 topics'>Bash</a>
<a style='font-size: 193.36%' href='http://martin-thoma.com/tag/c/' title='22 topics'>C</a>
<a style='font-size: 138.36%' href='http://martin-thoma.com/tag/cpp/' title='10 topics'>CPP</a>
<a style='font-size: 113.47%' href='http://martin-thoma.com/tag/challenge/' title='7 topics'>Challenge</a>
<a style='font-size: 90.00%' href='http://martin-thoma.com/tag/chrome/' title='5 topics'>Chrome</a>
<a style='font-size: 138.36%' href='http://martin-thoma.com/tag/clip/' title='10 topics'>Clip</a>
<a style='font-size: 90.00%' href='http://martin-thoma.com/tag/command-line/' title='5 topics'>Command Line</a>
<a style='font-size: 102.72%' href='http://martin-thoma.com/tag/computer-science/' title='6 topics'>Computer science</a>
<a style='font-size: 122.79%' href='http://martin-thoma.com/tag/digitaltechnik/' title='8 topics'>Digitaltechnik</a>
<a style='font-size: 102.72%' href='http://martin-thoma.com/tag/flashgames/' title='6 topics'>Flashgames</a>
<a style='font-size: 90.00%' href='http://martin-thoma.com/tag/geometry/' title='5 topics'>Geometry</a>
<a style='font-size: 131.00%' href='http://martin-thoma.com/tag/google/' title='9 topics'>Google</a>
<a style='font-size: 122.79%' href='http://martin-thoma.com/tag/google-code-jam/' title='8 topics'>Google Code Jam</a>
<a style='font-size: 90.00%' href='http://martin-thoma.com/tag/html5/' title='5 topics'>HTML5</a>
<a style='font-size: 131.00%' href='http://martin-thoma.com/tag/it-security/' title='9 topics'>IT-Security</a>
<a style='font-size: 227.72%' href='http://martin-thoma.com/tag/java/' title='36 topics'>Java</a>
<a style='font-size: 102.72%' href='http://martin-thoma.com/tag/javascript/' title='6 topics'>JavaScript</a>
<a style='font-size: 179.36%' href='http://martin-thoma.com/tag/kit/' title='18 topics'>KIT</a>
<a style='font-size: 212.63%' href='http://martin-thoma.com/tag/klausur/' title='29 topics'>Klausur</a>
<a style='font-size: 90.00%' href='http://martin-thoma.com/tag/kogsys/' title='5 topics'>KogSys</a>
<a style='font-size: 202.28%' href='http://martin-thoma.com/tag/latex/' title='25 topics'>LaTeX</a>
<a style='font-size: 179.36%' href='http://martin-thoma.com/tag/linear-algebra/' title='18 topics'>Linear algebra</a>
<a style='font-size: 171.14%' href='http://martin-thoma.com/tag/linux/' title='16 topics'>Linux</a>
<a style='font-size: 138.36%' href='http://martin-thoma.com/tag/machine-learning/' title='10 topics'>Machine Learning</a>
<a style='font-size: 122.79%' href='http://martin-thoma.com/tag/matrix/' title='8 topics'>Matrix</a>
<a style='font-size: 90.00%' href='http://martin-thoma.com/tag/os/' title='5 topics'>OS</a>
<a style='font-size: 90.00%' href='http://martin-thoma.com/tag/operating-systems/' title='5 topics'>Operating Systems</a>
<a style='font-size: 131.00%' href='http://martin-thoma.com/tag/php/' title='9 topics'>PHP</a>
<a style='font-size: 90.00%' href='http://martin-thoma.com/tag/physics/' title='5 topics'>Physics</a>
<a style='font-size: 240.11%' href='http://martin-thoma.com/tag/programming/' title='43 topics'>Programming</a>
<a style='font-size: 131.00%' href='http://martin-thoma.com/tag/project-euler/' title='9 topics'>Project Euler</a>
<a style='font-size: 270.00%' href='http://martin-thoma.com/tag/python/' title='66 topics'>Python</a>
<a style='font-size: 102.72%' href='http://martin-thoma.com/tag/review/' title='6 topics'>Review</a>
<a style='font-size: 131.00%' href='http://martin-thoma.com/tag/swt-i/' title='9 topics'>SWT I</a>
<a style='font-size: 90.00%' href='http://martin-thoma.com/tag/science/' title='5 topics'>Science</a>
<a style='font-size: 131.00%' href='http://martin-thoma.com/tag/theoretical-computer-science/' title='9 topics'>Theoretical computer science</a>
<a style='font-size: 102.72%' href='http://martin-thoma.com/tag/tikz/' title='6 topics'>Tikz</a>
<a style='font-size: 90.00%' href='http://martin-thoma.com/tag/ubuntu/' title='5 topics'>Ubuntu</a>
<a style='font-size: 156.66%' href='http://martin-thoma.com/tag/video/' title='13 topics'>Video</a>
<a style='font-size: 131.00%' href='http://martin-thoma.com/tag/vimeo/' title='9 topics'>Vimeo</a>
<a style='font-size: 138.36%' href='http://martin-thoma.com/tag/web-development/' title='10 topics'>Web Development</a>
<a style='font-size: 102.72%' href='http://martin-thoma.com/tag/wikipedia/' title='6 topics'>Wikipedia</a>
<a style='font-size: 90.00%' href='http://martin-thoma.com/tag/windows-7/' title='5 topics'>Windows 7</a>
<a style='font-size: 90.00%' href='http://martin-thoma.com/tag/wolfram-alpha/' title='5 topics'>Wolfram|Alpha</a>
<a style='font-size: 145.00%' href='http://martin-thoma.com/tag/youtube/' title='11 topics'>YouTube</a>
<a style='font-size: 90.00%' href='http://martin-thoma.com/tag/advertising/' title='5 topics'>advertising</a>
<a style='font-size: 122.79%' href='http://martin-thoma.com/tag/algorithms/' title='8 topics'>algorithms</a>
<a style='font-size: 113.47%' href='http://martin-thoma.com/tag/analysis/' title='7 topics'>analysis</a>
<a style='font-size: 90.00%' href='http://martin-thoma.com/tag/cheat-sheet/' title='5 topics'>cheat sheet</a>
<a style='font-size: 166.64%' href='http://martin-thoma.com/tag/funny/' title='15 topics'>funny</a>
<a style='font-size: 166.64%' href='http://martin-thoma.com/tag/learning/' title='15 topics'>learning</a>
<a style='font-size: 138.36%' href='http://martin-thoma.com/tag/lecture-notes/' title='10 topics'>lecture-notes</a>
<a style='font-size: 254.70%' href='http://martin-thoma.com/tag/mathematics/' title='53 topics'>mathematics</a>
<a style='font-size: 190.11%' href='http://martin-thoma.com/tag/puzzle/' title='21 topics'>puzzle</a>
</div>
                    </li>
                </ul>
                </div>
            </div>
        </div><!--/container-->
            <footer id="footer">
                <a href="http://martin-thoma.com"><strong>Martin Thoma</strong></a> -  A blog about Code, the Web and Cyberculture. <br />
                <div class="footer-credits">
                    <a href="http://flexithemes.com/themes/modern-style/">Modern Style</a> theme by <a href="http://flexithemes.com/">FlexiThemes</a>
                </div>
            </footer><!--/footer-->

    </div><!--/wrapper-->
<!-- type: footer -->
<!-- MathJax -->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">
<!--
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [ ['$$', '$$'], ['\\[','\\]']],
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    processEscapes: true
  }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
// -->
</script>
<!-- TOC Plus -->
<script type='text/javascript'>
/* <![CDATA[ */
var tocplus = {"visibility_show":"show","visibility_hide":"hide","width":"275px"};
/* ]]> */
</script>
<script type='text/javascript' src="http://martin-thoma.com/js/tocplus-front.js"></script>

</body>
</html>

