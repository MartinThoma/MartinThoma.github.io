<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Martin Thoma" />
        <meta name="copyright" content="Martin Thoma" />
        <link title = "Martin Thoma"
              type  = "application/opensearchdescription+xml"
              rel   = "search"
              href  = "../opensearch.xml">

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="Machine Learning, Machine Learning, " />

<meta property="og:title" content="Machine Learning Glossary "/>
<meta property="og:url" content="../ml-glossary/" />
<meta property="og:description" content="The following is a list of short explanations of different terms in machine learning. The aim is to keep things simple and brief, not to explain the terms in full detail. Active Learning The algorithm gives a pattern and asks for a label. Backpropagation A clever implementation of gradient descent …" />
<meta property="og:site_name" content="Martin Thoma" />
<meta property="og:article:author" content="Martin Thoma" />
<meta property="og:article:published_time" content="2016-10-24T20:00:00+02:00" />
<meta name="twitter:title" content="Machine Learning Glossary ">
<meta name="twitter:description" content="The following is a list of short explanations of different terms in machine learning. The aim is to keep things simple and brief, not to explain the terms in full detail. Active Learning The algorithm gives a pattern and asks for a label. Backpropagation A clever implementation of gradient descent …">
<meta property="og:image" content="logos/ml.png" />
<meta name="twitter:image" content="logos/ml.png" >

        <title>Machine Learning Glossary  · Martin Thoma
</title>
        <!-- Latest compiled and minified CSS -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="../theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../static/custom.css" media="screen">

        <!-- MathJax -->
<script type="text/x-mathjax-config">
<!--
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [ ['$$', '$$'], ['\\[','\\]']],
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    processEscapes: true
  }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
// -->
</script>
<script type="text/javascript" async
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

        <link href="https://martin-thoma.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Martin Thoma - Full Atom Feed" />
        <link href="https://martin-thoma.com/feeds/index.xml" type="application/rss+xml" rel="alternate" title="Martin Thoma - Full RSS Feed" />
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top navbar-default">
            <div class="container">
                <div class="container-fluid">
                    <div class="collapse navbar-collapse">
                        <ul class="nav pull-left top-menu navbar-nav">
                            <li><a href=".." style="font-family: 'Monaco', 'Inconsolata', 'Andale Mono', 'Lucida Console', 'Bitstream Vera Sans Mono', 'Courier New', Courier, Monospace;
                        font-size: 20px;" class="navbar-brand">Martin Thoma</a>
                            </li>
                        </ul>
                        <ul class="nav pull-right top-menu navbar-nav">
                            <li ><a href="..">Home</a></li>
                            <li ><a href="../categories.html">Categories</a></li>
                            <li ><a href="../tags.html">Tags</a></li>
                            <li ><a href="../archives.html">Archives</a></li>
                            <li><a href="../support-me/">Support me</a></li>
                            <li><form class="navbar-form" action="../search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="search" class="search-query form-control" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row">
                <div class="col-sm-1 col-md-1"></div>
                <div class="col-sm-10 col-md-10">
<article>
<div class="row">
    <header class="page-header col-sm-10 col-md-10 col-md-offset-2">
    <h1><a href="../ml-glossary/"> Machine Learning Glossary  </a></h1>
    </header>
</div>

<div class="row">
    <div class="col-sm-2 col-md-2 table-of-content">
        <nav>
        <h4>Contents</h4>
        <div id="toc"><ul><li><a class="toc-href" href="#see-also" title="See also">See also</a></li></ul></div>
        </nav>
    </div>
    <div class="col-sm-8 col-md-8 article-content" id="contentAfterTitle">

            
            <p>The following is a list of short explanations of different terms in machine
learning. The aim is to keep things simple and brief, not to explain the terms
in full detail.</p>
<dl>
<dt><dfn id="active-learning">Active Learning</dfn></dt>
<dd>The algorithm gives a pattern and asks for a label.</dd>
<dt><dfn id="backpropagation">Backpropagation</dfn></dt>
<dd>A clever implementation of gradient descent for neural networks.</dd>
<dt><dfn id="bias">Bias</dfn></dt>
<dd>Bias is a concept which describes a systematic error. A classifier
        with a high bias tends to give one answer more often, no matter what
        the input is. This concept is relatied to <i>variance</i> and well
        described with the images <a href="https://sebastianraschka.com/blog/2016/model-evaluation-selection-part2.html">here</a>.</dd>
<dt><dfn id="blstm">BLSTM</dfn>, <dfn id="bilstm">BiLSTM</dfn></dt>
<dd>Bidirectional long short-term memory (see <a href="http://www.di.ufpe.br/~fnj/RNA/bibliografia/BRNN.pdf">paper</a> and <a href="https://www.cs.toronto.edu/~graves/asru_2013_poster.pdf">poster</a>).</dd>
<dt><dfn id="co-training">Co-Training</dfn></dt>
<dd>A form of semi-supervised learning. Two independant classifiers are
        trained on different labeled datasets. The classifiers are applied to
        the unlabeled data. Data with high confidence will be added to the
        other classifiers data.</dd>
<dt><dfn id="collaborative-filtering">Collaborative Filtering</dfn></dt>
<dd>You have users and items which are rated. No user rated everything.
        You want to fill the gaps (see <a href="https://martin-thoma.com/collaborative-filtering/">article</a>).</dd>
<dt><dfn id="computer-vision">Computer Vision</dfn></dt>
<dd>The academic discipline which deals with how to gain high-level understanding from digital images or videos. Common tasks include image classifiction, semantic segmentation, detection and localization.</dd>
<dt><dfn id="curriculum-learning">Curriculum learning</dfn></dt>
<dd>A method for pretraining. First optimize a smoothed objective and gradually consider less smoothing. So a curriculum is a sequence of training criteria. One might show gradually more difficult training examples. See <a href="http://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf">Curriculum Learning</a> by Benigo, Louradour, Collobert and Weston for details.</dd>
<dt><dfn id="curse-of-dimensionality">Curse of dimensionality</dfn></dt>
<dd>Various problems of high-dimensional spaces that do not occur in low-dimensional spaces.
        High-dimensional often means several 100 dimensions. See also: <a href="https://martin-thoma.com/average-distance-of-points">Average Distance of Points</a></dd>
<dt><dfn id="dcgan">DCGAN</dfn> (<dfn>Deep Convolutional Generative Adverserial Networks</dfn>)</dt>
<dd>TODO</dd>
<dt><dfn id="dcign">DCIGN</dfn> (<dfn>Deep Convolutional Inverse Graphic Network</dfn>)</dt>
<dd>TODO</dd>
<dt><dfn id="dcnn">DCNN</dfn> (<dfn>Doubly Convolutional Neural Network</dfn>)</dt>
<dd>Introduced in <a href="https://arxiv.org/pdf/1610.09716v1.pdf">this paper</a> (<a href="http://www.shortscience.org/paper?bibtexKey=conf/nips/ZhaiCZL16#martinthoma">summary</a>).

    <b>Note</b> Some people also call <i>Deep Convolutional Neural Networks</i> DCNNs.</dd>
<dt><dfn id="dnn">DNN</dfn></dt>
<dd>Deep Neural Network. The meaning of "deep" differs. Sometimes it means at
        least one hidden layer, sometimes it means at least 12 hidden layers.</dd>
<dt><dfn id="domain-adaptation">Domain adaptation</dfn></dt>
<dd>A model is trained on dataset $A$. How does it have to be changed to work on dataset $B$?</dd>
<dt><dfn id="object-detection">Detection in Computer Vision</dfn> (<dfn>Object detection</dfn>)</dt>
<dd>Object detection in an image is a computer vision task. The input
        is an image and the output is a list with rectangles which contain
        objects of the given type. Face detection is one well-studied example.
        A photo could contain no face or hundrets of them. The rectangles
        can overlap.</dd>
<dt><dfn id="deep-learing">Deep Learning</dfn></dt>
<dd>Buzzword. The meaning depends on who you ask / in which year you asked.
        Sometimes it means multi-layer perceptrons with more than $N$ layers
        (some say $N=2$ is already deep learning, others want N&gt;20 or nowadays
        $N&gt;100$).</dd>
<dt><dfn id="discriminative-model">Discriminative Model</dfn></dt>
<dd>The model gives a conditional probability of the classes $k$, given the
        feature vector $x$: $P(k | x)$.
        This kind of model is often used for prediction.</dd>
<dt><dfn id="fc7-features">FC7-Features</dfn></dt>
<dd>Features of an image which was run through a trained neural network.
        AlexNet called the last fully connected layer FC7. However, FC7
        features are not necessarily created by AlexNet.</dd>
<dt><dfn id="fmllr">FMLLR</dfn></dt>
<dd>Feature-Space Maximum Likelihood Linear Regression</dd>
<dt><dfn id="feature-map">Feature Map</dfn></dt>
<dd>A feature map is the result of a single filter of a convolutional layer
        being applied. So it is the activation of that filter over the given
        input.</dd>
<dt><dfn id="fine-tuning">Fine-tuning</dfn></dt>
<dd>See <a href="#pre-training">pre-training</a></dd>
<dt><a href="https://en.wikipedia.org/wiki/Mixture_model"><dfn id="gmm">GMM</dfn></a></dt>
<dd>Gaussian Mixture Model</dd>
<dt><dfn id="gemm">GEMM</dfn> (<dfn>GEneral Matrix to Matrix Multiplication</dfn>)</dt>
<dd>General Matrix to Matrix Multiplication is the problem of
        calculating the result of $C = A \cdot B$ with $A \in \mathbb{R}^{n \times m}, B \in \mathbb{R}^{m \times k}, C \in \mathbb{R}^{n \times k}$.</dd>
<dt><dfn id="generative-model">Generative model</dfn></dt>
<dd>The model gives the relationship of variables: $P(x, y)$.
        This kind of model can be used for prediction, too.</dd>
<dt><dfn id="gradient-descent">Gradient Descent</dfn></dt>
<dd>An iterative optimization algorithm for differentiable functions.</dd>
<dt><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model"><dfn id="hmm">HMM</dfn></a></dt>
<dd>Hidden Markov Model</dd>
<dt><dfn id="i-vector">i-vector</dfn></dt>
<dd>speaker identity vector. See <a href="http://ieeexplore.ieee.org/abstract/document/5545402/">Front-End Factor Analysis for Speaker Verification</a>.</dd>
<dt><dfn id="mann">MANN</dfn></dt>
<dd>Memory-Augmented Neural Networks (see <a href="http://rylanschaeffer.github.io/content/research/one_shot_learning_with_memory_augmented_nn/main.html">Blog post</a>)</dd>
<dt><dfn id="machine-vision">Machine Vision</dfn></dt>
<dd>Computer vision applied for industrial applications.</dd>
<dt><a href="https://en.wikipedia.org/wiki/Matrix_completion"><dfn id="matrix-completion">Matrix Completion</dfn></a></dt>
<dd>See <a href="#collaborative-filtering">collaborative filtering</a>.</dd>
<dt><dfn id="mllr">MLLR</dfn></dt>
<dd>Maximum Likelihood Linear Regression</dd>
<dt><dfn id="mmd">MMD</dfn> (<dfn id="maximum-mean-descripancy">Maximum Mean Descrepancy</dfn>)</dt>
<dd>MMD is a measure of the difference between a distribution $P$ and a distribution $Q$:

        $$MMD(F, p, q) = sup_{f \in F} (\mathbb{E}_{x \sim p} [f(x)] - \mathbb{E}_{y \sim q} [f(y)])$$

    </dd>
<dt><dfn id="multi-task-learning">Multi-Task learning</dfn></dt>
<dd>Train a model which does multiple tasks at the same time, e.g.
        segmentation and detection (see <a href="https://arxiv.org/abs/1612.07695">MultiNet</a>).</dd>
<dt><a href="https://en.wikipedia.org/wiki/Neuroevolution_of_augmenting_topologies"><dfn id="neat">NEAT</dfn></a></dt>
<dd>Neuroevolution of Augmenting Topologies (see <a href="http://blog.otoro.net/2016/05/07/backprop-neat/">Blogpost</a>).</dd>
<dt><dfn id="object-recognition">Object recognition</dfn></dt>
<dd>Classification on images. The task is to decide in which class a given
        image falls, judging by the content. This can be cat, dog, plane or
        similar.</dd>
<dt><dfn id="one-shot-learning"><a href="https://en.wikipedia.org/wiki/One-shot_learning">One-Shot learning</a></dfn></dt>
<dd>Learn only with one or very few examples per class. See <a href="http://vision.stanford.edu/documents/Fei-FeiFergusPerona2006.pdf">One-Shot Learning of Object Categories</a>.</dd>
<dt><a href="https://en.wikipedia.org/wiki/Optical_flow"><dfn id="optical-flow">Optical Flow</dfn></a></dt>
<dd>Optical flow is defined for two images. It describes how the points in
        one image moved when switching to the second image.</dd>
<dt><a href="https://en.wikipedia.org/wiki/Principal_component_analysis"><dfn id="pca">PCA</dfn></a></dt>
<dd>Principal component analysis (short: PCA) is a linear transformation
        which projects $n$ points $\mathbf{x} \in \mathbb{R}^{n \times s}$ with
        $s$ features each on a hyperplane in such a way
        that the projection error is minimal. Hence it is an unsupervised
        method for feature reduction. It simply works by finding a matrix
        $P \in \mathbb{R}^{s \times m}$, where $m \leq s$ can be chosen as small
        as desired.
        </dd>
<dt><dfn id="pre-training">Pre-training</dfn></dt>
<dd>
<ol>
<li>You have machine learning model $m$.</li>
<li><i>Pre-training</i>: You have a dataset $A$ on which you train $m$.</li>
<li>You have a dataset $B$. Before you start training the model,
                you initialize some of the parameters of $m$ with the model
                which is trained on $A$.</li>
<li><i>Fine-tuning</i>: You train $m$ on $B$.</li>
</ol>
</dd>
<dt><dfn id="regularization">Regularization</dfn></dt>
<dd>Regularization are techniques to make the fitted function smoother. This
        helps to prevent overfitting.<br/>
        Examples: L1, L2, Dropout, Weight Decay in Neural Networks. Parameter $C$ in SVMs.</dd>
<dt><dfn id="reinforcement-learning">Reinforcement Learning</dfn></dt>
<dd>Reinforcment learning is a sub-field of machine learning, which focuses
        on the question how to find actions which lead to higher rewards. See
        <a href="../probabilistische-planung/#reinforcement-learning">German lecture notes</a>.</dd>
<dt><dfn id="self-learning">Self-Learning</dfn></dt>
<dd>One form of semi-supervised learning, where you train an initial system
        on the labeled data, then label the unlabeled data where the classifier
        is 'sure enough'. After that, you train a new system on all data and
        re-label the unlabeled data. This is iterated.</dd>
<dt><dfn id="semi-supervised-learning">Semi-supervised learning</dfn></dt>
<dd>Some training data has labels, but most has no labels.</dd>
<dt><dfn id="supervised-learning">Supervised learning</dfn></dt>
<dd>All training data has labels.</dd>
<dt><dfn id="spatial-pyramid-pooling">Spatial Pyramid Pooling</dfn> (<dfn>SPP</dfn>)</dt>
<dd>SPP is the idea of dividing the image into a grid with a fixed number
        of cells and a variable size, depending on the input. Each cell computes
        one feature and hence leads to a fixed-size representation of a variable-sized
        input.<br/>
        See <a href="https://arxiv.org/abs/1406.4729v4">paper</a> and <a href="http://www.shortscience.org/paper?bibtexKey=journals/corr/1406.4729">summary</a></dd>
<dt><a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf"><dfn id="tf-idf">TF-IDF</dfn></a></dt>
<dd>TF-IDF (short for Term frequency&ndash;inverse document frequency)
        is a measure that reflects how important a word is to a document in a
        collection or corpus.</dd>
<dt><dfn id="transductive-learning">Transductive learning</dfn></dt>
<dd>label unlabeled data (the aim here is NOT to find a hypothesis)</dd>
<dt><dfn id="unsupervised-learning">Unsupervised learning</dfn></dt>
<dd>No training data has labels.</dd>
<dt><dfn id="vc-dimension">VC-Dimension</dfn></dt>
<dd>A theoretical natural number assigned to any classifier. The higher
        the VC dimension of a classifier, the more situations it is able
        to capture (see <a href="http://datascience.stackexchange.com/a/16144/8820">longer explanation</a>, <a href="https://martin-thoma.com/machine-learning-1-course/#vc-dimension">german explanation</a>).</dd>
<dt><dfn id="vlad">VLAD</dfn></dt>
<dd>Vector of Locally Aligned Descriptors</dd>
<dt><dfn id="vtln">VTLN</dfn></dt>
<dd>vocal tract length normalization</dd>
<dt><dfn id="wrn">WRN</dfn></dt>
<dd>Wide residual network</dd>
<dt><dfn id="zero-shot-learning">Zero-Shot learning</dfn></dt>
<dd>Learning to predict classes, of which no example has been seen during
        training. For example, Flicker gets several new tags each day and they
        want to predict tags for new images. One idea is to use WordNet and
        ImageNet to generate a common embedding. This way, new words of WordNet
        could already have an embedding and thus new images categories could also automatically be classified the right way. See <a href="http://www.cs.cmu.edu/afs/cs/project/theo-73/www/papers/zero-shot-learning.pdf">Zero-Shot Learning with Semantic Output Codes</a> as well as <a href="https://www.youtube.com/watch?v=Pmv5JHKX2y4">this YouTube video</a>.</dd>
</dl>
<h2 id="see-also">See also</h2>
<ul>
<li>Lectures:<ul>
<li><a href="https://martin-thoma.com/analysetechniken-grosser-datenbestaende/">Analysetechniken gro&szlig;er Datenbest&auml;nde</a></li>
<li><a href="https://martin-thoma.com/informationsfusion/">Informationsfusion</a></li>
<li><a href="https://martin-thoma.com/machine-learning-1-course/">Machine Learning 1</a></li>
<li><a href="https://martin-thoma.com/machine-learning-2-course/">Machine Learning 2</a></li>
<li><a href="https://martin-thoma.com/mustererkennung-klausur/">Mustererkennung</a></li>
<li><a href="https://martin-thoma.com/neuronale-netze-vorlesung/">Neuronale Netze</a></li>
<li><a href="https://martin-thoma.com/lma/">Lokalisierung Mobiler Agenten</a></li>
<li><a href="https://martin-thoma.com/probabilistische-planung/">Probabilistische Planung</a></li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Main_Page">Wikipedia</a></li>
<li><a href="http://www.scholarpedia.org/">scholarpedia</a></li>
<li>Other<ul>
<li><a href="http://alumni.media.mit.edu/~tpminka/statlearn/glossary/">alumni.media.mit.edu</a></li>
<li><a href="http://robotics.stanford.edu/~ronnyk/glossary.html">robotics.stanford.edu</a></li>
<li><a href="http://www.ee.columbia.edu/~vittorio/Glossary.pdf">ee.columbia.edu</a></li>
<li><a href="http://www.cse.unsw.edu.au/~billw/mldict.html">The Machine Learning Dictionary</a></li>
<li><a href="http://37steps.com/glossary/">37steps.com</a></li>
<li><a href="http://www.asimovinstitute.org/neural-network-zoo/">asimovinstitute.org</a>: The Neural Network Zoo</li>
</ul>
</li>
</ul>
            
            <div id="disqus_thread"></div>
<script>
    /*
    var disqus_config = function () {
        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW
        var d = document, s = d.createElement('script');

        s.src = '//martinthoma.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

            <hr/>
        </div>
        <section>
        <div class="col-sm-2 col-md-2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time pubdate="pubdate" datetime="2016-10-24T20:00:00+02:00">Okt 24, 2016</time>
            <br/>
            by <a rel="author" class="vcard author post-author" itemprop="author" href="../author/martin-thoma/"><span class="fn" itemscope="" itemtype="https://schema.org/Person"><span itemprop="name">Martin Thoma</span></span></a>
            <h4>Category</h4>
            <a class="category-link" href="../categories.html#machine-learning-ref">Machine Learning</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="../tags.html#machine-learning-ref">Machine Learning
                    <span>57</span>
</a></li>
            </ul>
<h4>Contact</h4>
    <a href="https://twitter.com/themoosemind" title="My Twitter Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-twitter sidebar-social-links"></i></a>
    <a href="mailto:info@martin-thoma.de" title="My Email Address" class="sidebar-social-links" target="_blank">
    <i class="fa fa-envelope sidebar-social-links"></i></a>
    <a href="https://github.com/MartinThoma" title="My Github Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-github sidebar-social-links"></i></a>
    <a href="http://stackoverflow.com/users/562769/martin-thoma" title="My Stackoverflow Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-stackoverflow sidebar-social-links"></i></a>
        </div>
        </section>
</div>
</article>
                </div>
                <div class="col-sm-1 col-md-1"></div>
            </div>
        </div>
        <div id="push"></div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-subtitle"><span class="site-name">Martin Thoma</span> - A blog about Code, the Web and Cyberculture</li>
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>        <script src="//code.jquery.com/jquery.min.js"></script>
        <!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>
        <script>
(function(){
    'use strict';

    /*
    Create intra-page links
    Requires that your headings already have an `id` attribute set (because that's what jekyll does)
    For every heading in your page, this adds a little anchor link `#` that you can click to get a permalink to the heading.
    Ignores `h1`, because you should only have one per page.
    The text content of the tag is used to generate the link, so it will fail "gracefully-ish" if you have duplicate heading text.

    Credit: https://gist.github.com/SimplGy/a229d25cdb19d7f21231
     */

    var headingNodes = [], results, link,
        tags = ['h2', 'h3', 'h4', 'h5', 'h6'];

    tags.forEach(function(tag){
        var contentTag = document.getElementById('contentAfterTitle');
      results = contentTag.getElementsByTagName(tag);
      Array.prototype.push.apply(headingNodes, results);
    });

    headingNodes.forEach(function(node){
      link = document.createElement('a');
      link.className = 'deepLink';
      link.textContent = ' ¶';
      link.href = '#' + node.getAttribute('id');
      node.appendChild(link);
    });

  })();
</script>
    </body>
    <!-- Theme: Elegant built for Pelican
    License : http://oncrashreboot.com/pelican-elegant -->
</html>