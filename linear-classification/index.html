<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Martin Thoma" />
        <meta name="copyright" content="Martin Thoma" />
        <link title = "Martin Thoma"
              type  = "application/opensearchdescription+xml"
              rel   = "search"
              href  = "../opensearch.xml">

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="Algorithms, Machine Learning, optimization, Python, Machine Learning, " />

<meta property="og:title" content="Linear Classification "/>
<meta property="og:url" content="linear-classification/" />
<meta property="og:description" content="In classification problems you have data points \(x \in \mathbb{R}^m\) which you want to classify into one of \(k \in \mathbb{N}_{\geq 2}\) classes. This is a supervised task. This means you have \(n\) data points for training in a matrix \(X \in \mathbb{R}^{n …" />
<meta property="og:site_name" content="Martin Thoma" />
<meta property="og:article:author" content="Martin Thoma" />
<meta property="og:article:published_time" content="2016-06-22T20:00:00+02:00" />
<meta name="twitter:title" content="Linear Classification ">
<meta name="twitter:description" content="In classification problems you have data points \(x \in \mathbb{R}^m\) which you want to classify into one of \(k \in \mathbb{N}_{\geq 2}\) classes. This is a supervised task. This means you have \(n\) data points for training in a matrix \(X \in \mathbb{R}^{n …">
<meta property="og:image" content="logos/ml.png" />
<meta name="twitter:image" content="logos/ml.png" >

        <title>Linear Classification  · Martin Thoma
</title>
        <!-- Latest compiled and minified CSS -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="../theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../static/print.css" media="print">
        <link rel="stylesheet" type="text/css" href="../static/custom.css" media="screen">

        <!-- MathJax -->
<script type="text/x-mathjax-config">
<!--
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [ ['$$', '$$'], ['\\[','\\]']],
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    processEscapes: true
  }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
// -->
</script>
<script type="text/javascript" async
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

        <link href="https://martin-thoma.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Martin Thoma - Full Atom Feed" />
        <link href="https://martin-thoma.com/feeds/index.xml" type="application/rss+xml" rel="alternate" title="Martin Thoma - Full RSS Feed" />
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top navbar-default">
            <div class="container">
                <div class="container-fluid">
                    <div class="collapse navbar-collapse">
                        <ul class="nav pull-left top-menu navbar-nav">
                            <li><a href=".." style="font-family: 'Monaco', 'Inconsolata', 'Andale Mono', 'Lucida Console', 'Bitstream Vera Sans Mono', 'Courier New', Courier, Monospace;
                        font-size: 20px;" class="navbar-brand" tabindex="-1">Martin Thoma</a>
                            </li>
                        </ul>
                        <ul class="nav pull-right top-menu navbar-nav">
                            <li ><a href="..">Home</a></li>
                            <li ><a href="../categories.html">Categories</a></li>
                            <li ><a href="../tags.html">Tags</a></li>
                            <li ><a href="../archives.html">Archives</a></li>
                            <li><a href="../support-me/">Support me</a></li>
                            <li><form class="navbar-form" action="../search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="search" class="search-query form-control" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row">
                <div class="col-sm-1 col-md-1"></div>
                <div class="col-sm-10 col-md-10">
<article>
<div class="row">
    <header class="page-header col-sm-10 col-md-10 col-md-offset-2">
    <h1><a href="linear-classification/"> Linear Classification  </a></h1>
    </header>
</div>

<div class="row">
    <div class="col-sm-2 col-md-2 table-of-content">
        <nav>
        <h4>Contents</h4>
        <div id="toc"><ul><li><a class="toc-href" href="#one-hot-encoding" title="One-hot encoding">One-hot encoding</a><ul><li><a class="toc-href" href="#normalizing-the-output" title="Normalizing the output">Normalizing the output</a></li><li><a class="toc-href" href="#decision-boundary" title="Decision boundary">Decision boundary</a></li></ul></li><li><a class="toc-href" href="#-11-encoding_1" title="-1/+1 encoding">-1/+1 encoding</a></li><li><a class="toc-href" href="#differences-from-target-encoding" title="Differences from target encoding">Differences from target encoding</a></li><li><a class="toc-href" href="#implementation" title="Implementation">Implementation</a></li><li><a class="toc-href" href="#footnotes" title="Footnotes">Footnotes</a></li></ul></div>
        </nav>
    </div>
    <div class="col-sm-8 col-md-8 article-content" id="contentAfterTitle">

            
            <p>In classification problems you have data points <span class="math">\(x \in \mathbb{R}^m\)</span> which you want to classify into one of <span class="math">\(k \in \mathbb{N}_{\geq 2}\)</span> classes. This is a supervised task. This means you have <span class="math">\(n\)</span> data points for training in a matrix <span class="math">\(X \in \mathbb{R}^{n \times m}\)</span> with their labels.<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup></p>
<p>Initially, the label might be something like "cat" or "dog". But the machine learning algorithms can't deal with those directly, so you need to encode the labels. The simplest way of encoding them is to use integers <span class="math">\(0, 1, \dots, k - 1\)</span>. However, in many cases it is handy to use a <em>one-hot encoding</em>. This means you make a <span class="math">\(k\)</span>-dimensional vector for each label. <span class="math">\((1, 0)\)</span> might encode "dog" and <span class="math">\((0, 1)\)</span> might encode "cat". <code>sklearn</code> supports this encoding in a convenient way
(<a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html">docs</a>). Another common encoding is <span class="math">\(-1\)</span> and <span class="math">\(1\)</span> for binary classification problems.<sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup></p>
<p>A linear model is one which applies only linear operations to the features. So basically the model may only be a matrix. Typically the elements of the matrix are called <em>weights</em>, because they weight the importance of each feature.</p>
<p>The objective of such a classifier<sup id="fnref:4"><a class="footnote-ref" href="#fn:4">4</a></sup> is to find a matrix <span class="math">\(W^*\)</span> such that the MSE is as small as possible on the test set <span class="math">\(D\)</span>:</p>
<div class="math">$$W^* = \arg \min_{W} E_{MSE} (f_W, D)$$</div>
<h2 id="one-hot-encoding">One-hot encoding</h2>
<p>If one-hot encoded labels are used,<sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup> a linear classifier
</p>
<div class="math">$$f: \text{feature space} \rightarrow \text{class space}$$</div>
<p>
usually works like this:
</p>
<div class="math">$$f(x) = {\arg \max}_{i \in 1, \dots, k} (W \cdot x)^{(i)}$$</div>
<p>
with <span class="math">\(W \in \mathbb{R}^{k \times m}\)</span> and <span class="math">\(^{(i)}\)</span> denoting the <span class="math">\(i\)</span>-th element of the vector. Given a test set </p>
<div class="math">$$D = \{(x_i, y_i) \text{ with } i \in \{1, \dots, n_t\}, x_i \in \mathbb{R}^{m}, y_i \in \mathbb{R}_+^{k}\}$$</div>
<p>
you can calculate the <em>mean squared error</em> (MSE) of the classifier <span class="math">\(f\)</span>:</p>
<div class="math">$$E_{MSE}(f, D) = \frac{1}{n_t}\sum_{i=1}^{n_t} (t_i - W \cdot x_i)^T (t_i - W \cdot x_i)$$</div>
<p>Please note that the MSE is always non-negative for every single data point.</p>
<h3 id="normalizing-the-output">Normalizing the output</h3>
<p>The nice thing about the MSE is that it is simple. It can easily be calculated and is used in regression problems very often. What is not so nice is the fact that it punishes several good solutions, too. For example, say we have a data point <span class="math">\(x_1\)</span> which has the target <span class="math">\((1, 0)^T\)</span>. The classification output</p>
<div class="math">$$c_1 = \begin{pmatrix}101\\0\end{pmatrix}\quad E_{MSE}(f, x_1) = 100^2\tag{1.1}$$</div>
<p>while the classification output of another point <span class="math">\(x_2\)</span> is</p>
<div class="math">$$c_2 = \begin{pmatrix}0\\0.1\end{pmatrix}\quad E_{MSE}(f, x_2) = 1 + 0.1^2\tag{1.2}$$</div>
<p>This is problematic, as the actual classification in <span class="math">\((1.1)\)</span> is correct whereas the classification in <span class="math">\((1.2)\)</span> is wrong. However, this can easily be fixed by normalizing the result.</p>
<h4 id="simple-normalization">Simple Normalization</h4>
<p>The simplest way to normalize the result of the classifier would be to divide each entry by the sum of all entries, e.g. for <span class="math">\((1.1)\)</span> we get</p>
<div class="math">$$
\begin{align}
c_1' &amp;= \begin{pmatrix}1\\0\end{pmatrix}\quad &amp;E_{MSE}(f'', x_1) &amp;= 0\tag{2.1}\\
c_2' &amp;= \begin{pmatrix}0\\1\end{pmatrix}\quad &amp;E_{MSE}(f', x_2) &amp;= 1\tag{2.2}
\end{align}
$$</div>
<h4 id="standardization">Standardization</h4>
<p>You might want to interpret the output of your classifier as a probability of the data point belonging to the different classes. Then you may not have negative values and you also want to avoid a probability of 0. A common normalization then is the <em>softmax function</em> <span class="math">\(\sigma\)</span>. It first exponentiates the single values and then normalizes:</p>
<div class="math">$$\begin{align}
c_1'' &amp;= \begin{pmatrix}\frac{e^{101}}{e^{101} + e^0}\\\frac{e^{0}}{e^{101} + e^0}\end{pmatrix} \approx \begin{pmatrix}1\\0\end{pmatrix}\quad &amp;E_{MSE}(f'', x_1) &amp;\approx 0\tag{3.1}\\
c_2'' &amp;= \begin{pmatrix}\frac{e^{0}}{e^{0} + e^{0.1}}\\\frac{e^{0.1}}{e^{0} + e^{0.1}}\end{pmatrix} \approx \begin{pmatrix}0.475\\0.525\end{pmatrix}\quad &amp;E_{MSE}(f'', x_2) &amp;\approx 0.551\tag{3.2}
\end{align}$$</div>
<p>Note that this is the same as a neural network with only an input layer and a softmax output layer.</p>
<p>Now the optimization problem is:</p>
<div class="math">$$
\begin{align}
W^* &amp;= \arg \min_{W \in \mathbb{R}^{k \times m}} E(f'', D)\\
  &amp;= \arg \min_{W \in \mathbb{R}^{k \times m}} \frac{1}{n} \sum_{i=1}^n (t_i - \sigma(W x_i))(t_i - \sigma(W x_i))^T \\
  &amp;= \arg \min_{W \in \mathbb{R}^{k \times m}} \sum_{i=1}^n (t_i - \sigma(W x_i))(t_i - \sigma(W x_i))^T \\
\end{align}
$$</div>
<p>This is a differentiable function. This means to optimize we can calculate the gradient and apply gradient descent:</p>
<div class="math">$$
\begin{align}
&amp;\frac{\partial}{\partial W} \sum_{i=1}^n (t_i - \sigma(W x_i))(t_i - \sigma(W x_i))^T\\
=&amp; \sum_{i=1}^n \frac{\partial}{\partial W} (t_i - \sigma(W x_i))(t_i - \sigma(W x_i))^T\\
=&amp; \sum_{i=1}^n \left (\frac{\partial}{\partial W} (t_i - \sigma(W x_i)) \right ) \left(t_i - \sigma(W x_i) \right)^T + (t_i - \sigma(W x_i)) \left (\frac{\partial}{\partial W} (t_i - \sigma(W x_i))\right )\\
=&amp; \sum_{i=1}^n \left (\frac{\partial}{\partial W} \sigma(W x_i) \right ) \left(t_i - \sigma(W x_i) \right)^T + (t_i - \sigma(W x_i)) \left (\frac{\partial}{\partial W} \sigma(W x_i)\right )^T\\
\end{align}$$</div>
<p>as you can see it gets quite ugly. I don't want to continue this calculation here.  But I hope you can see that this is possible. <a href="http://stats.stackexchange.com/q/79454/25741">stats.stackexchange.com</a> gives some hints on how to continue.</p>
<h3 id="decision-boundary">Decision boundary</h3>
<p>The decision boundary for a two-class problem with 2-dimensional data vectors (and one bias) can be calculated as</p>
<div class="math">$$
\begin{align}
&amp;W^{(0, 0)} \cdot 1 + W^{(0, 1)} \cdot x_1 + W^{(0, 2)} \cdot x_2 = W^{(1, 0)} \cdot 1 + W^{(1, 1)} \cdot x_1 + W^{(1, 2)} \cdot x_2\tag{DB}\\
\Leftrightarrow x_2&amp;= \frac{W^{(0, 0)} \cdot 1 + W^{(0, 1)} \cdot x_1 - (W^{(1, 0)} \cdot 1 + W^{(1, 1)} \cdot x_1)}{W^{(1, 2)} - W^{(0, 2)}}\\
\Leftrightarrow x_2 &amp;= \frac{(W^{(0, 1)} - W^{(1, 1)})}{W^{(1, 2)} - W^{(0, 2)}} \cdot x_1 + \frac{W^{(0, 0)} - W^{(1, 0)}}{W^{(1, 2)} - W^{(0, 2)}}
\end{align}
$$</div>
<p>As you can see, the decision boundary of the non-normalized form is a line:</p>
<div class="math">$$y = a \cdot x + b \qquad a, b \in \mathbb{R}$$</div>
<p>Please also note that <span class="math">\(W^{(1, 2)} = W^{(0, 2)}\)</span> is possible, which would mean that the line is parallel to the <span class="math">\(x_2\)</span> axis. So <strong>this model basically only has 2&nbsp;parameter combinations which matter, although it has 6&nbsp;values which can be adjusted. But many combinations are equivalent.</strong></p>
<p>What does normalization change?</p>
<p>For given <span class="math">\(W, x\)</span> it only divides both sides of the equation by the same constant. Hence it doesn't change the decision boundary.</p>
<p>What does standardization with softmax change?</p>
<p>Just like with normalization, softmax makes equation <span class="math">\((DB)\)</span> to be divided by a constant. This can be ignored. The exponentiation can also be ignored as we can simply take the logartihm of both sides of <span class="math">\((DB)\)</span>. Or in other words: <strong>A neural network with only one input layer and one softmax output layer also has a linear decision boundary!</strong></p>
<h2 id="-11-encoding_1">-1/+1 encoding</h2>
<p>In the 2-class case one might consider to use -1 for one class and +1 for the other class as targets. Then the classifcation is</p>
<div class="math">$$
\begin{cases}
\text{class with label 1}&amp;\text{if } f(x) \geq 0\\
\text{class with label -1}&amp;\text{if } f(x) &lt; 0
\end{cases}
$$</div>
<p>The matrix <span class="math">\(W\)</span> is now in <span class="math">\(\mathbb{R}^{1 \times 3}\)</span> and the classifier is</p>
<div class="math">$$f(x) = W x$$</div>
<p>To make sure that this is either <span class="math">\(-1\)</span> or <span class="math">\(+1\)</span>, one can modify it to</p>
<div class="math">$$f'(x) = \frac{W x}{|W x|}$$</div>
<p>Note that this ignores the case <span class="math">\(Wx = 0\)</span>.</p>
<p>Now we can easily calculate the MSE:</p>
<div class="math">$$E_{MSE}(f, D) = \sum_{i=1}^{n_t} {(t_i - f'(x))}^2$$</div>
<h2 id="differences-from-target-encoding">Differences from target encoding</h2>
<p>You might wonder if it makes a difference which type of encoding you use for your target. There are some things which come to my mind:</p>
<ul>
<li>(+) One-hot encoding can easily be expanded to more than two classes, in contrast to <span class="math">\(-1/+1\)</span> encoding.</li>
<li>(+) With one-hot encoding, you can easily get a probability distribution for the classes you are interested in. This is certainly also possible with <span class="math">\(-1/+1\)</span> encoding, but it doesn't strike my eye as clearly.</li>
<li>(-) One-hot encoding needs more storage space.</li>
<li>(?) <span class="math">\(-1/+1\)</span> encoding is used in SVMs (see <a href="https://martin-thoma.com/svm-with-sklearn/">SVM article</a>), so it might have advantages in maximum margin classification.</li>
</ul>
<h2 id="implementation">Implementation</h2>
<p>If I had to implement a linear binary classifier, I would use the delta rule
and a perceptron unit:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/usr/bin/env python</span>

<span class="sd">"""Example for a linear classifier using a perceptron and the delta rule."""</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets.samples_generator</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">class</span> <span class="nc">Perceptron</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        Single perceptron unit.</span>

<span class="sd">        Credit to Sebastian Raschka:</span>
<span class="sd">        http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html</span>
<span class="sd">        This was slightly modified.</span>
<span class="sd">        """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">eta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="n">epochs</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">errors_</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">):</span>
            <span class="n">errors</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">xi</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
                <span class="n">update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xi</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+=</span> <span class="n">update</span> <span class="o">*</span> <span class="n">xi</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">update</span>
                <span class="n">errors</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">update</span> <span class="o">!=</span> <span class="mf">0.0</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">errors_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">net_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net_input</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<span class="c1"># Generate data</span>
<span class="n">X</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Fit perceptron</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">Perceptron</span><span class="p">(</span><span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">f</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="c1"># Plot decision boundary</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="nb">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">stop</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">f</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">xi</span><span class="p">)</span> <span class="o">/</span> <span class="n">f</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">],</span> <span class="s2">"r--"</span><span class="p">)</span>

<span class="c1"># Plot data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gray</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">target</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<p>which gives:</p>
<figure class="wp-caption aligncenter img-thumbnail">
<img alt="Classification with a Perceptron." src="../images/2016/06/perceptron-classification.png"/>
<figcaption class="text-center">Classification with a Perceptron.</figcaption>
</figure>
<p>Looks about right. You can also see that this probably minimizes the MSE, but
it does not maximize the margin between the classes. This would be done by
SVMs.</p>
<h2 id="footnotes">Footnotes</h2>
<div class="footnote">
<hr/>
<ol>
<li id="fn:1">
<p>A label and a class are two different things. The class is a set which
  contains all data points which belong to this class. The label is only
  a pointer to this class.&nbsp;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
<li id="fn:2">
<p>Classification problems with only two classes.&nbsp;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">↩</a></p>
</li>
<li id="fn:3">
<p>Also called "targets" sometimes, as we want our classifier to output those values.&nbsp;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">↩</a></p>
</li>
<li id="fn:4">
<p>The objective function defines what is to be done during fitting / training / learning.&nbsp;<a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text">↩</a></p>
</li>
</ol>
</div>
            
            <div id="disqus_thread" class="no-print"></div>
<script>
    /*
    var disqus_config = function () {
        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW
        var d = document, s = d.createElement('script');

        s.src = '//martinthoma.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

            <hr/>
        </div>
        <section>
        <div class="col-sm-2 col-md-2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time pubdate="pubdate" datetime="2016-06-22T20:00:00+02:00">Jun 22, 2016</time>
            <br/>
            by <a rel="author" class="vcard author post-author" itemprop="author" href="../author/martin-thoma/"><span class="fn" itemscope="" itemtype="https://schema.org/Person"><span itemprop="name">Martin Thoma</span></span></a>
            <h4>Category</h4>
            <a class="category-link" href="../categories.html#machine-learning-ref">Machine Learning</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="../tags.html#algorithms-ref">Algorithms
                    <span>13</span>
</a></li>
                <li><a href="../tags.html#machine-learning-ref">Machine Learning
                    <span>81</span>
</a></li>
                <li><a href="../tags.html#optimization-ref">optimization
                    <span>4</span>
</a></li>
                <li><a href="../tags.html#python-ref">Python
                    <span>134</span>
</a></li>
            </ul>
<h4>Contact</h4>
    <a href="https://twitter.com/themoosemind" title="My Twitter Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-twitter sidebar-social-links"></i></a>
    <a href="mailto:info@martin-thoma.de" title="My Email Address" class="sidebar-social-links" target="_blank">
    <i class="fa fa-envelope sidebar-social-links"></i></a>
    <a href="https://github.com/MartinThoma" title="My Github Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-github sidebar-social-links"></i></a>
    <a href="http://stackoverflow.com/users/562769/martin-thoma" title="My Stackoverflow Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-stackoverflow sidebar-social-links"></i></a>
        </div>
        </section>
</div>
</article>
                </div>
                <div class="col-sm-1 col-md-1"></div>
            </div>
        </div>
        <div id="push"></div>
    </div>
<footer class="no-print">
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-subtitle"><span class="site-name">Martin Thoma</span> - A blog about Code, the Web and Cyberculture</li>
        <li><a href="https://martin-thoma.com/feeds/index.xml">RSS-Feed</a></li>
        <li><a href="http://www.martin-thoma.de/privacy.htm">Datenschutzerkl&auml;rung</a></li>
        <li><a href="http://www.martin-thoma.de/impressum.htm">Impressum</a></li>
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page" tabindex="-1">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page" tabindex="-1">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page" tabindex="-1">Talha Mansoor</a></li>
    </ul>
</div>
</footer>        <script src="//code.jquery.com/jquery.min.js"></script>
        <!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>
        <script>
(function(){
    'use strict';

    /*
    Create intra-page links
    Requires that your headings already have an `id` attribute set
    For every heading in your page, this adds a little anchor link `#` that you can click to get a permalink to the heading.
    Ignores `h1`, because you should only have one per page.
    The text content of the tag is used to generate the link, so it will fail "gracefully-ish" if you have duplicate heading text.

    Credit: https://gist.github.com/SimplGy/a229d25cdb19d7f21231
     */

    var headingNodes = [], results, link,
        tags = ['h2', 'h3', 'h4', 'h5', 'h6'];

    tags.forEach(function(tag){
        var contentTag = document.getElementById('contentAfterTitle');
      results = contentTag.getElementsByTagName(tag);
      Array.prototype.push.apply(headingNodes, results);
    });

    headingNodes.forEach(function(node){
      link = document.createElement('a');
      link.className = 'deepLink';
      link.textContent = ' ¶';
      link.href = '#' + node.getAttribute('id');
      node.appendChild(link);
    });

  })();
</script>
    </body>
    <!-- Theme: Elegant built for Pelican
    License : http://oncrashreboot.com/pelican-elegant -->
</html>