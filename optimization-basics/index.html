<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Martin Thoma" />
        <meta name="copyright" content="Martin Thoma" />
        <link title = "Martin Thoma"
              type  = "application/opensearchdescription+xml"
              rel   = "search"
              href  = "../opensearch.xml">

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="Machine Learning, optimization, gradient descent, Machine Learning, " />

<meta property="og:title" content="Optimization Basics "/>
<meta property="og:url" content="../optimization-basics/" />
<meta property="og:description" content="Optimization is a subfield of mathematics / computer science which deals with finding the best solution. Typically, problems in optimization are stated like this: $$ \begin{align} &amp;\underset{x}{\operatorname{minimize}}&amp; &amp; f(x) \\ &amp;\operatorname{subject\;to} &amp; &amp;g_i(x) \leq 0, \quad i = 1,\dots,m \\ &amp;&amp;&amp;h_i(x) = 0, \quad i = 1, \dots …" />
<meta property="og:site_name" content="Martin Thoma" />
<meta property="og:article:author" content="Martin Thoma" />
<meta property="og:article:published_time" content="2016-07-06T20:00:00+02:00" />
<meta name="twitter:title" content="Optimization Basics ">
<meta name="twitter:description" content="Optimization is a subfield of mathematics / computer science which deals with finding the best solution. Typically, problems in optimization are stated like this: $$ \begin{align} &amp;\underset{x}{\operatorname{minimize}}&amp; &amp; f(x) \\ &amp;\operatorname{subject\;to} &amp; &amp;g_i(x) \leq 0, \quad i = 1,\dots,m \\ &amp;&amp;&amp;h_i(x) = 0, \quad i = 1, \dots …">
<meta property="og:image" content="logos/ai.png" />
<meta name="twitter:image" content="logos/ai.png" >

        <title>Optimization Basics  · Martin Thoma
</title>
        <!-- Latest compiled and minified CSS -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="../theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../static/print.css" media="print">
        <link rel="stylesheet" type="text/css" href="../static/custom.css" media="screen">

        <!-- MathJax -->
<script type="text/x-mathjax-config">
<!--
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [ ['$$', '$$'], ['\\[','\\]']],
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    processEscapes: true
  }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
// -->
</script>
<script type="text/javascript" async
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

        <link href="https://martin-thoma.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Martin Thoma - Full Atom Feed" />
        <link href="https://martin-thoma.com/feeds/index.xml" type="application/rss+xml" rel="alternate" title="Martin Thoma - Full RSS Feed" />
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top navbar-default">
            <div class="container">
                <div class="container-fluid">
                    <div class="collapse navbar-collapse">
                        <ul class="nav pull-left top-menu navbar-nav">
                            <li><a href=".." style="font-family: 'Monaco', 'Inconsolata', 'Andale Mono', 'Lucida Console', 'Bitstream Vera Sans Mono', 'Courier New', Courier, Monospace;
                        font-size: 20px;" class="navbar-brand" tabindex="-1">Martin Thoma</a>
                            </li>
                        </ul>
                        <ul class="nav pull-right top-menu navbar-nav">
                            <li ><a href="..">Home</a></li>
                            <li ><a href="../categories.html">Categories</a></li>
                            <li ><a href="../tags.html">Tags</a></li>
                            <li ><a href="../archives.html">Archives</a></li>
                            <li><a href="../support-me/">Support me</a></li>
                            <li><form class="navbar-form" action="../search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="search" class="search-query form-control" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row">
                <div class="col-sm-1 col-md-1"></div>
                <div class="col-sm-10 col-md-10">
<article>
<div class="row">
    <header class="page-header col-sm-10 col-md-10 col-md-offset-2">
    <h1><a href="../optimization-basics/"> Optimization Basics  </a></h1>
    </header>
</div>

<div class="row">
    <div class="col-sm-2 col-md-2 table-of-content">
        <nav>
        <h4>Contents</h4>
        <div id="toc"><ul><li><a class="toc-href" href="#simulated-annealing" title="Simulated Annealing">Simulated Annealing</a></li><li><a class="toc-href" href="#gradient-descent" title="Gradient descent">Gradient descent</a><ul><li><a class="toc-href" href="#iterative-descent" title="Iterative Descent">Iterative Descent</a></li></ul></li><li><a class="toc-href" href="#linear-regression-with-mse_1" title="Linear Regression with MSE">Linear Regression with MSE</a></li><li><a class="toc-href" href="#lagrange-multipliers" title="Lagrange multipliers">Lagrange multipliers</a></li><li><a class="toc-href" href="#optimization-problem-characteristics" title="Optimization Problem characteristics">Optimization Problem characteristics</a></li><li><a class="toc-href" href="#resources" title="Resources">Resources</a></li><li><a class="toc-href" href="#references" title="References">References</a></li></ul></div>
        </nav>
    </div>
    <div class="col-sm-8 col-md-8 article-content" id="contentAfterTitle">

            
            <p>Optimization is a subfield of mathematics / computer science which deals with finding the best solution. Typically, problems in optimization are stated like this:</p>
<div class="math">$$
\begin{align}
&amp;\underset{x}{\operatorname{minimize}}&amp; &amp; f(x) \\
&amp;\operatorname{subject\;to}
&amp; &amp;g_i(x) \leq 0, \quad i = 1,\dots,m \\
&amp;&amp;&amp;h_i(x) = 0, \quad i = 1, \dots,p
\end{align}
$$</div>
<p>where</p>
<ul>
<li><span class="math">\(f(x): \mathbb{R}^n \to \mathbb{R}\)</span> is the <strong>loss function (objective function)</strong> to be minimized over the variable <span class="math">\(x\)</span>,</li>
<li><span class="math">\(g_i(x) \leq 0\)</span> are called <strong>inequality constraints</strong>, and</li>
<li><span class="math">\(h_i(x) = 0\)</span> are called <strong>equality constraints</strong>.</li>
</ul>
<p>By convention, the standard form defines a <strong>minimization problem</strong>. A
<strong>maximization problem</strong> can be treated by negating the objective function.</p>
<p>(That was copied from <a href="https://en.wikipedia.org/w/index.php?title=Optimization_problem&amp;oldid=715562612#Continuous_optimization_problem">en.wikipedia.org/w/Optimization_problem</a> and only slightly edited.)</p>
<p>I'm now going to explain some very basic techniques which are used for finding good solutions to optimization problems. Please note that there are also discrete optimization problems where you have to finde a solution <span class="math">\(x \in \mathbb{N}^n\)</span>. I will only focus on continuous optimization problems.</p>
<h2 id="simulated-annealing">Simulated Annealing</h2>
<p>Simulated Annealing is a heuristical optimization algorithm. It starts at a
random point <span class="math">\(x \in \mathbb{R}^n\)</span>. Then it takes a random point <span class="math">\(y\)</span> of the
environment of <span class="math">\(x\)</span>:</p>
<div class="math">$$y \in U(x)$$</div>
<p>If <span class="math">\(f(y) \leq f(x)\)</span>, then the current position <span class="math">\(x\)</span> is overwritten with <span class="math">\(y\)</span>:</p>
<div class="math">$$x \leftarrow y$$</div>
<p>Otherwise, it might be overwritten with probability <span class="math">\(\exp \left (-\frac{f(y)-f(x)}{T(t)} \right )\)</span> where <span class="math">\(T: \mathbb{N}_0 \rightarrow \mathbb{R}_{&gt; 0}\)</span> is called the temperature at time <span class="math">\(t\)</span>.</p>
<p>So the optimization algorithm is:</p>
<ol>
<li>Take a random point $x \in \mathbb{R}^n$</li>
<li>Take a random point $y \in U(x)$</li>
<li>$$x \leftarrow \begin{cases}y &amp;\text{if } f(y) \leq f(x)\\
                          y &amp;\text{if } \operatorname{rand}(0,1) &lt; \exp \left (-\frac{f(y)-f(x)}{T(t)} \right )\\
                          x &amp;\text{otherwise}\end{cases}$$</li>
<li>Go to step 2.</li>
</ol>
<p>See also my <a href="https://martin-thoma.com/neuronale-netze-vorlesung/#simulated-annealing">German description</a>.</p>
<h2 id="gradient-descent">Gradient descent</h2>
<p>The gradient descent algorithm can easily be applied when the optimization problem has no constraints and the objective function <span class="math">\(f\)</span> is differentiable.
The idea is to just take a random starting point <span class="math">\(x \in \mathbb{R}^n\)</span> and
iteratively improve it. There are many algorithms which follow this approach (Simulated annealing, L-BFGS, Newton's method, Quasi-Newtonian, Conjugate Gradient, ...).</p>
<p>Instead of randomly going in other directions, the
gradient <span class="math">\(\nabla f\)</span> is calculated at the position <span class="math">\(x\)</span>. The gradient points
in the direction of maximum increase, so we go in the opposite direction:</p>
<div class="math">$$x_{\text{new}} = x - \nabla f(x)$$</div>
<p>The problem with this approach is that the surface of the objective function
might first go down in the direction of <span class="math">\(\nabla f(x)\)</span>, but if you go a bit
further it can go up by a lot. So we want to make very small steps. To achive
this, we multiply the gradient with a factor <span class="math">\(\eta \in (0, 1]\)</span>. In machine
learining, this <span class="math">\(\eta\)</span> is called the <em>learning rate</em> and typically one
chooses <span class="math">\(\eta = 0.01\)</span>. However, there are <a href="https://martin-thoma.com/neuronale-netze-vorlesung/#learning-rate-scheduling">learning rate scheduling algorithms</a> which adapt this parameter during training.</p>
<p>The update rule is:</p>
<div class="math">$$x_{\text{new}} = x - \eta \nabla f(x)$$</div>
<h3 id="iterative-descent">Iterative Descent</h3>
<p>A more general formulation of the Gradient descent algorithm is called
iterative descent. The idea is to start at some arbitrary <span class="math">\(x_0\)</span> and iteratively
update the current guess of the minimum to</p>
<div class="math">$$x_{k+1} = x_k + \eta \cdot d_k$$</div>
<p>where <span class="math">\(\eta \in (0, 1]\)</span> is the step length (learning rate) and</p>
<div class="math">$$d_k = -D_k \nabla f(x_k)$$</div>
<p>is the direction of the descent. The direction depends on the Gradient
<span class="math">\(\nabla f(x_k)\)</span>, but also on a matrix <span class="math">\(D_k\)</span>:</p>
<ul>
<li>$D_k = I$: Gradient descent</li>
<li>$D_k = H_f^{-1}(x_k)$: Newtons method, where $H_f$ is the <a href="https://en.wikipedia.org/wiki/Hessian_matrix">Hessian matrix</a> of $f$</li>
</ul>
<h2 id="linear-regression-with-mse_1">Linear Regression with MSE</h2>
<p>In linear regression one is given a list of <span class="math">\(n\)</span> points <span class="math">\((x, y)\)</span> with <span class="math">\(x \in \mathbb{R}^m\)</span> and <span class="math">\(y \in \mathbb{R}\)</span>. The task is to find a matrix <span class="math">\(A \in \mathbb{1 \times m}\)</span> such that the predicted value <span class="math">\(\hat{y}\)</span> of the linear model</p>
<div class="math">$$\hat{y}(x) = A \cdot x$$</div>
<p>minimizes the term</p>
<div class="math">$$\text{MSE} = \sum_{i=1}^n (y_i - \hat{y}(x_i))^2$$</div>
<p>For convenience, one can write the list of points as a matrix <span class="math">\(\mathbf{X} \in \mathbb{R}^{n \times m}\)</span> and
a vector <span class="math">\(\mathbf{y} \in \mathbb{R}^n\)</span>:</p>
<div class="math">$$\text{MSE} = \|\mathbf{y} - \mathbf{X} A^T\|_2$$</div>
<p>with the Euclidean norm</p>
<div class="math">$$\| v \|_2 := \sqrt{ ( v_1 )^2 + ( v_2 )^2 + \dotsb + ( v_n )^2 } = \left( \sum_{i=1}^n ( v_i )^2 \right)^{1/2}$$</div>
<p>Every part of the sum is non-negative, so exponentiating the Euclidean norm
with a positive factor will not change the result of the minimization:</p>
<div class="math">$$\operatorname{minimize}_{A} \|\mathbf{y} - \mathbf{X} A^T\|_2^2$$</div>
<p>Now we can see that this is every element of the vector squared. So we can
get rid of the norm and then use distributivity:</p>
<div class="math">$$
\begin{align}
\operatorname{minimize}_{A}&amp;(\mathbf{y} - \mathbf{X} A^T)^T \cdot (\mathbf{y} - \mathbf{X} A^T)\\
\Leftrightarrow \operatorname{minimize}_{A}&amp;(\mathbf{y}^T - A \mathbf{X}^T) \cdot (\mathbf{y} - \mathbf{X} A^T)\\
\Leftrightarrow \operatorname{minimize}_{A}&amp;\mathbf{y}^T \mathbf{y} - A \mathbf{X}^T \mathbf{y} - \mathbf{y}^T \mathbf{X} A^T + A\mathbf{X}^T \mathbf{X} A^T\\
\end{align}
$$</div>
<p>You need to know that</p>
<div class="math">$$
\begin{align}
A \mathbf{X}^T \mathbf{y} &amp;= ((A \mathbf{X}^T \mathbf{y})^T)^T\\
&amp;= (\mathbf{y}^T \mathbf{X} A^T)^T\\
&amp;= \mathbf{y}^T \mathbf{X} A^T\\
\end{align}
$$</div>
<p>You can get rid of the last transposing operation, because
</p>
<div class="math">$$(\mathbf{y}^T \mathbf{X} A^T) \in \mathbb{R}^{1 \times 1}$$</div>
<p>This simplifies the optimization problem to</p>
<div class="math">$$\operatorname{minimize}_{A}\underbrace{\mathbf{y}^T \mathbf{y} - 2 A \mathbf{X}^T \mathbf{y} + A\mathbf{X}^T \mathbf{X} A^T}_{E_{X, y}(A)}$$</div>
<p>Now you can calculate the gradient of this term with respect to <span class="math">\(A\)</span>:</p>
<div class="math">$$\nabla E_{X, y}(A) = 2 X^T X A^T - X^T y - X^T y = 2 X^T (X A^T - y)$$</div>
<p>A necessary condition of a minimum is the gradient to be 0:</p>
<div class="math">$$
\begin{align}
\nabla E_{X, y}(A) &amp;\overset{!}{=} 0\\
\Leftrightarrow 0 &amp;\overset{!}{=} 2 X^T (X A^T - y)\\
\Leftrightarrow 0 &amp;\overset{!}{=} X^T X A^T - X^T y\\
\Leftrightarrow A &amp;\overset{!}{=} ((X^T X)^{-1} X^T y)^T\\
\end{align}
$$</div>
<p>As <span class="math">\(H_{E_{X, y}} = \nabla^2 E_{X, y}(A) = 2 X^T X\)</span> is positive definite, this is a minimum. Hence, the optimal solution to this problem is:</p>
<div class="math">$$A = ((X^T X)^{-1} X^T y)^T$$</div>
<h2 id="lagrange-multipliers">Lagrange multipliers</h2>
<p>Lagrange multipliers are a trick in optimization problems with constraints.
They can be used to get rid of the constraints.</p>
<p>The Lagrange function has the form</p>
<div class="math">$$\mathcal{L} (x, \lambda_1, \dots, \lambda_n) = f(x) + \sum_{j=1}^n \lambda_j h_j(x)$$</div>
<p>
with the <em>Lagrange multipliers</em> <span class="math">\(\lambda_j \in \mathbb{R}\)</span> and <span class="math">\(h_j\)</span> are equality constraints.<br/>
<br/>
Necessary conditions for a minimum <span class="math">\(x^*\)</span> is:</p>
<ul>
<li>$\nabla_x \mathcal{L} = \nabla_x f(x^*) + \sum_{j=1}^n \lambda_j \nabla_x h_j(x^*) \overset{!}{=} 0$</li>
<li>$\frac{\partial}{\partial \lambda_j} \mathcal{L} = h_j(x^*) \overset{!}{=} 0, \quad j=1, \dots, $</li>
</ul>
<p>See [<a href="#ref-smi04" name="ref-smi04-anchor">Smi04</a>] for many examples.</p>
<h2 id="optimization-problem-characteristics">Optimization Problem characteristics</h2>
<p>There are some properties of optimization problems which make it easier / harder
to solve:</p>
<table>
<tr>
<th>Property</th>
<th>Easy</th>
<th>Hard</th>
</tr>
<tr>
<td>Objective</td>
<td>linear</td>
<td>non-linear</td>
</tr>
<tr>
<td>Optimization Variable</td>
<td>small discrete, continuous</td>
<td>large discrete</td>
</tr>
<tr>
<td>Constraints</td>
<td>No Constraints</td>
<td>Constraints</td>
</tr>
</table>
<h2 id="resources">Resources</h2>
<ul>
<li>Reddit: <a href="https://www.reddit.com/r/MachineLearning/comments/4582s0/overview_of_optimization_algorithms/">Overview of Optimization Algorithms</a></li>
</ul>
<h2 id="references">References</h2>
<ul>
<li>[<a href="#ref-smi04-anchor" name="ref-smi04">Smi04</a>] B. T. Smith, &ldquo;Lagrange multipliers tutorial in the context of support
  vector machines,&rdquo; Memorial University of Newfoundland St. John&rsquo;s,
  Newfoundland, Canada, Jun. 2004.</li>
</ul>
            
            <div id="disqus_thread" class="no-print"></div>
<script>
    /*
    var disqus_config = function () {
        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW
        var d = document, s = d.createElement('script');

        s.src = '//martinthoma.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

            <hr/>
        </div>
        <section>
        <div class="col-sm-2 col-md-2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time pubdate="pubdate" datetime="2016-07-06T20:00:00+02:00">Jul 6, 2016</time>
            <br/>
            by <a rel="author" class="vcard author post-author" itemprop="author" href="../author/martin-thoma/"><span class="fn" itemscope="" itemtype="https://schema.org/Person"><span itemprop="name">Martin Thoma</span></span></a>
            <h4>Category</h4>
            <a class="category-link" href="../categories.html#machine-learning-ref">Machine Learning</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="../tags.html#gradient-descent-ref">gradient descent
                    <span>2</span>
</a></li>
                <li><a href="../tags.html#machine-learning-ref">Machine Learning
                    <span>80</span>
</a></li>
                <li><a href="../tags.html#optimization-ref">optimization
                    <span>4</span>
</a></li>
            </ul>
<h4>Contact</h4>
    <a href="https://twitter.com/themoosemind" title="My Twitter Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-twitter sidebar-social-links"></i></a>
    <a href="mailto:info@martin-thoma.de" title="My Email Address" class="sidebar-social-links" target="_blank">
    <i class="fa fa-envelope sidebar-social-links"></i></a>
    <a href="https://github.com/MartinThoma" title="My Github Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-github sidebar-social-links"></i></a>
    <a href="http://stackoverflow.com/users/562769/martin-thoma" title="My Stackoverflow Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-stackoverflow sidebar-social-links"></i></a>
        </div>
        </section>
</div>
</article>
                </div>
                <div class="col-sm-1 col-md-1"></div>
            </div>
        </div>
        <div id="push"></div>
    </div>
<footer class="no-print">
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-subtitle"><span class="site-name">Martin Thoma</span> - A blog about Code, the Web and Cyberculture</li>
        <li><a href="https://martin-thoma.com/feeds/index.xml">RSS-Feed</a></li>
        <li><a href="http://www.martin-thoma.de/privacy.htm">Datenschutzerkl&auml;rung</a></li>
        <li><a href="http://www.martin-thoma.de/impressum.htm">Impressum</a></li>
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page" tabindex="-1">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page" tabindex="-1">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page" tabindex="-1">Talha Mansoor</a></li>
    </ul>
</div>
</footer>        <script src="//code.jquery.com/jquery.min.js"></script>
        <!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>
        <script>
(function(){
    'use strict';

    /*
    Create intra-page links
    Requires that your headings already have an `id` attribute set
    For every heading in your page, this adds a little anchor link `#` that you can click to get a permalink to the heading.
    Ignores `h1`, because you should only have one per page.
    The text content of the tag is used to generate the link, so it will fail "gracefully-ish" if you have duplicate heading text.

    Credit: https://gist.github.com/SimplGy/a229d25cdb19d7f21231
     */

    var headingNodes = [], results, link,
        tags = ['h2', 'h3', 'h4', 'h5', 'h6'];

    tags.forEach(function(tag){
        var contentTag = document.getElementById('contentAfterTitle');
      results = contentTag.getElementsByTagName(tag);
      Array.prototype.push.apply(headingNodes, results);
    });

    headingNodes.forEach(function(node){
      link = document.createElement('a');
      link.className = 'deepLink';
      link.textContent = ' ¶';
      link.href = '#' + node.getAttribute('id');
      node.appendChild(link);
    });

  })();
</script>
    </body>
    <!-- Theme: Elegant built for Pelican
    License : http://oncrashreboot.com/pelican-elegant -->
</html>